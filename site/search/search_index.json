{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Reasoning Core","text":"<p>Welcome to the documentation for Reasoning Core, a library for procedural generation of symbolic reasoning data in RL environments.</p> <p>For full documentation visit the API Reference section.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Procedural generation of reasoning tasks</li> <li>Integration with standard RL environments</li> <li>Compatible with Gymnasium</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install reasoning_core\n</code></pre>"},{"location":"#license","title":"license","text":"<p>MIT</p>"},{"location":"template/","title":"Template Module","text":"<p>This module defines the core abstractions for the framework, including <code>Task</code>, <code>Config</code>, <code>Problem</code>, and <code>Reward</code>.</p>"},{"location":"template/#api-reference","title":"API Reference","text":""},{"location":"template/#task","title":"Task","text":""},{"location":"template/#reasoning_core.template.Task","title":"<code>reasoning_core.template.Task</code>","text":"<p>               Bases: <code>ProceduralDataset</code></p> Source code in <code>reasoning_core/template.py</code> <pre><code>class Task(ProceduralDataset):\n    def __init_subclass__(cls):\n        cls.task_name = getattr(cls, 'task_name', prepr_task_name(cls.__name__))\n        register_dataset(cls.task_name, cls)\n\n\n    def __init__(self, config=dict(), timeout=10, seed=None, _level=0, *a, **kwa):\n        self.seed = seed\n        self.config=copy.deepcopy(config)\n        self.timeout = timeout\n        self.base_timeout = timeout\n        self.cls_name = self.__class__.__name__\n        self.task_name = prepr_task_name(self.__class__.task_name)\n        for k,v in kwa.items():\n            setattr(self.config, k, v)\n        self.balancing_key_ratio = 0.5\n        self.tokenizer = tiktoken.get_encoding(\"o200k_base\")\n\n    def generate(self):\n        \"\"\"To override, return one problem\"\"\"\n        #return Problem(metadata=edict(), answer=\"\")\n        raise NotImplementedError \n\n\n    def prompt(self,metadata):\n        \"\"\"To override, turns a problem metadata into a prompt\"\"\"\n        return \"\"\n\n    def default_score_answer(self, answer, entry):\n        \"\"\"To override in most cases; entry has entry.metadata and entry.answer fields\"\"\"\n        reference = entry['answer']\n        prepr = lambda x: str(x).strip()\n        answer, reference = prepr(answer), prepr(reference)\n        if answer==reference:\n            return 1\n        return 0\n\n    def __call__(self, *args, **kwargs):\n        return self.generate_example(*args, **kwargs)\n\n    def validate(self, n_samples=10):\n        \"\"\"Sanity checks to ensure that generation and scoring are working as expected.\"\"\"\n        x=self.generate_example()\n        assert isinstance(x, Problem), f\"Generated example must be of type Problem, got {type(x)}\"\n        assert self.score_answer(x.answer, x)==1, \"The generated answer must be correct\"\n        assert x.prompt, \"Generated example must have a non-empty prompt\"\n        ys=[self.generate_example() for _ in range(n_samples)]\n        score = [self.score_answer(y.answer, x) for y in ys]\n        assert set(score)!={1}, \"The scoring function must return values other than 1 for other answers\"\n        assert {self.score_answer(y.answer,y)==1 for y in ys}=={True}, \"The generated answer must be correct\"\n\n        self.score_answer('reajrjrje9595!',x) # should not error out\n        self.score_answer('',x) # should not error out\n        self.score_answer('import fakemodule',x) # should not eval strings \n\n        c0=self.config\n        self.config.set_level(1)\n        self.config.set_level(0)\n        assert self.config==c0\n\n        self.generate_example()\n        r1=random.random()\n        self.generate_example()\n        r2=random.random()\n        assert r1!=r2\n        return ys\n\n    def postprocess_dataset(self, df):\n        \"\"\"to override, apply deduplication and filtering\"\"\"\n        return df\n\n    def balancing_key(self, problem):\n        \"\"\"\n        To override, an optional feature that must be limited in fequency.\n        This can prevent label inbalance or frequency of easy problems.\n        \"\"\"\n        return str(problem.answer)\n\n    def deduplication_key(self, problem):\n        \"\"\"\n        To override, an optional feature that must be the key to deduplicate examples.\n        This can prevent the generation of the same problem.\n        \"\"\"\n        return None\n\n\n\n\n    def generate_example(self, level=None, max_tokens=8192, **kwargs):\n        self.timeout = int(self.base_timeout * (1+level)) if level else int(self.base_timeout)\n        @timeout_retry(self.timeout)\n        def inner():\n            t0=time.time()\n            if level:\n                self.config.set_level(level)\n            for _ in range(1_000):\n                problem = self.generate(**kwargs)\n                if problem is None:\n                    continue\n                problem.prompt = self.prompt(problem.metadata)\n\n                prompt_tokens = len(self.tokenizer.encode(problem.prompt))\n                cot_tokens = len(self.tokenizer.encode(problem.metadata.get('cot','') + problem.answer))\n                if max_tokens and prompt_tokens &gt; max_tokens:\n                    continue\n                if max_tokens and cot_tokens &gt; max_tokens:\n                    continue\n                break  \n\n            problem.task = self.task_name\n\n            problem.metadata = edict(problem.metadata)\n            problem.metadata['_time']  = time.time() - t0\n            problem.metadata['_task']  = problem.task \n            problem.metadata['_level'] = self.config.level\n            problem.metadata['_config'] = self.config.to_dict()\n            problem.metadata['_prompt_tokens'] = prompt_tokens\n            problem.metadata['_cot_tokens'] = cot_tokens\n\n            problem.balancing_key = self.balancing_key(problem)\n            problem.deduplication_key = self.deduplication_key(problem)\n            return problem\n        return inner()\n\n    def generate_balanced_batch(self, batch_size=32, deduplication = False, **kwargs):\n        max_per_key = math.ceil(batch_size * self.balancing_key_ratio)\n        counts = Counter()\n        if deduplication:\n            deduplication_values = []\n        batch = []\n        while len(batch) &lt; batch_size:\n            ex = self.generate_example(**kwargs)\n            b_key = ex.balancing_key\n            d_key = ex.deduplication_key\n            if d_key is not None and deduplication:\n                if d_key in deduplication_values:\n                    continue\n            if b_key is None or counts[b_key] &lt; max_per_key:\n                batch.append(ex)\n                if d_key is not None and deduplication:\n                    deduplication_values.append(d_key)\n                if b_key is not None:\n                    counts[b_key] += 1\n        return batch\n\n\n    def __getitem__(self, idx: int) -&gt; dict:\n        if self.seed:\n            rng = random.Random(self.seed + idx)\n        example=self.generate_example()\n        example['metadata']['source_dataset'] = example.task\n\n        return {\n            \"question\": example.prompt,\n            \"answer\": example.answer,\n            \"metadata\": example.metadata\n            }\n</code></pre>"},{"location":"template/#reasoning_core.template.Task-functions","title":"Functions","text":""},{"location":"template/#reasoning_core.template.Task.balancing_key","title":"<code>balancing_key(problem)</code>","text":"<p>To override, an optional feature that must be limited in fequency. This can prevent label inbalance or frequency of easy problems.</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def balancing_key(self, problem):\n    \"\"\"\n    To override, an optional feature that must be limited in fequency.\n    This can prevent label inbalance or frequency of easy problems.\n    \"\"\"\n    return str(problem.answer)\n</code></pre>"},{"location":"template/#reasoning_core.template.Task.deduplication_key","title":"<code>deduplication_key(problem)</code>","text":"<p>To override, an optional feature that must be the key to deduplicate examples. This can prevent the generation of the same problem.</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def deduplication_key(self, problem):\n    \"\"\"\n    To override, an optional feature that must be the key to deduplicate examples.\n    This can prevent the generation of the same problem.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"template/#reasoning_core.template.Task.default_score_answer","title":"<code>default_score_answer(answer, entry)</code>","text":"<p>To override in most cases; entry has entry.metadata and entry.answer fields</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def default_score_answer(self, answer, entry):\n    \"\"\"To override in most cases; entry has entry.metadata and entry.answer fields\"\"\"\n    reference = entry['answer']\n    prepr = lambda x: str(x).strip()\n    answer, reference = prepr(answer), prepr(reference)\n    if answer==reference:\n        return 1\n    return 0\n</code></pre>"},{"location":"template/#reasoning_core.template.Task.generate","title":"<code>generate()</code>","text":"<p>To override, return one problem</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def generate(self):\n    \"\"\"To override, return one problem\"\"\"\n    #return Problem(metadata=edict(), answer=\"\")\n    raise NotImplementedError \n</code></pre>"},{"location":"template/#reasoning_core.template.Task.postprocess_dataset","title":"<code>postprocess_dataset(df)</code>","text":"<p>to override, apply deduplication and filtering</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def postprocess_dataset(self, df):\n    \"\"\"to override, apply deduplication and filtering\"\"\"\n    return df\n</code></pre>"},{"location":"template/#reasoning_core.template.Task.prompt","title":"<code>prompt(metadata)</code>","text":"<p>To override, turns a problem metadata into a prompt</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def prompt(self,metadata):\n    \"\"\"To override, turns a problem metadata into a prompt\"\"\"\n    return \"\"\n</code></pre>"},{"location":"template/#reasoning_core.template.Task.validate","title":"<code>validate(n_samples=10)</code>","text":"<p>Sanity checks to ensure that generation and scoring are working as expected.</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def validate(self, n_samples=10):\n    \"\"\"Sanity checks to ensure that generation and scoring are working as expected.\"\"\"\n    x=self.generate_example()\n    assert isinstance(x, Problem), f\"Generated example must be of type Problem, got {type(x)}\"\n    assert self.score_answer(x.answer, x)==1, \"The generated answer must be correct\"\n    assert x.prompt, \"Generated example must have a non-empty prompt\"\n    ys=[self.generate_example() for _ in range(n_samples)]\n    score = [self.score_answer(y.answer, x) for y in ys]\n    assert set(score)!={1}, \"The scoring function must return values other than 1 for other answers\"\n    assert {self.score_answer(y.answer,y)==1 for y in ys}=={True}, \"The generated answer must be correct\"\n\n    self.score_answer('reajrjrje9595!',x) # should not error out\n    self.score_answer('',x) # should not error out\n    self.score_answer('import fakemodule',x) # should not eval strings \n\n    c0=self.config\n    self.config.set_level(1)\n    self.config.set_level(0)\n    assert self.config==c0\n\n    self.generate_example()\n    r1=random.random()\n    self.generate_example()\n    r2=random.random()\n    assert r1!=r2\n    return ys\n</code></pre>"},{"location":"template/#config","title":"Config","text":""},{"location":"template/#reasoning_core.template.Config","title":"<code>reasoning_core.template.Config</code>  <code>dataclass</code>","text":"<p>Base config providing transparent stochastic rounding.</p> <p>A subclass only needs to define its attributes with <code>int</code> type hints and implement a natural <code>update()</code> method (e.g., <code>self.n_ex += self.c</code>). The base class handles all rounding logic automatically.</p> Source code in <code>reasoning_core/template.py</code> <pre><code>@dataclass\nclass Config:\n    \"\"\"\n    Base config providing transparent stochastic rounding.\n\n    A subclass only needs to define its attributes with `int` type hints\n    and implement a natural `update()` method (e.g., `self.n_ex += self.c`).\n    The base class handles all rounding logic automatically.\n    \"\"\"\n    c: float = 1.0\n    level: int = 0\n    seed: int = None\n    size: int = None\n\n    def __post_init__(self):\n        # This flag is the key to differentiating behavior during updates.\n        object.__setattr__(self, '_is_updating', False)\n\n        self._unrounded = SimpleNamespace()\n\n        self._stochastic_fields = {\n            f.name for f in fields(self) \n            if f.type is int and not f.name.startswith('_') and f.name not in ['level', 'size', 'seed']\n        }\n        for name in self._stochastic_fields:\n            if name in self.__dict__:\n                setattr(self._unrounded, name, float(self.__dict__.pop(name)))\n\n        # Save the base state before any level-based updates are applied.\n        self._base_unrounded = copy.deepcopy(self._unrounded)\n        self._base_config_dict = copy.deepcopy(self.__dict__)\n\n        # Apply updates if initialized with level &gt; 0.\n        if self.level &gt; 0:\n            # We need to capture the level passed to __init__ before calling set_level,\n            # as set_level will reset it.\n            initial_level = self.level\n            # Use the existing set_level logic to apply the updates.\n            # This is clean and avoids duplicating code.\n            self.set_level(initial_level)\n\n    def __getattribute__(self, name: str) -&gt; Any:\n        try:\n            stochastic_fields = object.__getattribute__(self, '_stochastic_fields')\n            if name in stochastic_fields:\n                is_updating = object.__getattribute__(self, '_is_updating')\n                float_val = getattr(object.__getattribute__(self, '_unrounded'), name)\n\n                # If updating, return the raw float for deterministic calculations.\n                # Otherwise, return the stochastically rounded value.\n                if is_updating:\n                    return float_val\n                else:\n                    local_rng = random.Random(object.__getattribute__(self, 'seed'))\n                    floor_val = int(float_val)\n                    return floor_val + (1 if local_rng.random() &lt; (float_val - floor_val) else 0)\n        except AttributeError:\n            pass # Object is still initializing.\n\n        return object.__getattribute__(self, name)\n\n    def get_true_value(self, name: str) -&gt; float:\n        \"\"\"Returns the unrounded float value of a stochastic field.\"\"\"\n        if name in self._stochastic_fields:\n            return getattr(self._unrounded, name)\n        return getattr(self, name)\n\n    def __setattr__(self, name: str, value: Any):\n        try:\n            if name in object.__getattribute__(self, '_stochastic_fields'):\n                setattr(object.__getattribute__(self, '_unrounded'), name, float(value))\n                return\n        except AttributeError:\n            pass # Object is still initializing.\n\n        object.__setattr__(self, name, value)\n\n    def set_level(self, i: int):\n        current_c = self.c\n        current_seed = self.seed\n        self.__dict__.update(copy.deepcopy(self._base_config_dict))\n        self._unrounded = copy.deepcopy(self._base_unrounded)\n        self.c = current_c\n        self.seed = current_seed\n        # Set the flag to enable deterministic updates.\n        object.__setattr__(self, '_is_updating', True)\n        try:\n            object.__setattr__(self, 'level', i)             \n            for _ in range(i):\n                self.update(self.c)\n        finally:\n            # Always reset the flag, even if update fails.\n            object.__setattr__(self, '_is_updating', False)\n\n        object.__setattr__(self, 'level', i) \n        return self\n\n    def update(self, c):\n        raise NotImplementedError(\"Config subclasses must implement 'update'\")\n\n    def to_dict(self):\n        return asdict(self)\n\n    def __repr__(self) -&gt; str:\n        field_strings = []\n        for f in fields(self):\n            value = getattr(self, f.name)\n            field_strings.append(f\"{f.name}={value!r}\")\n\n        return f\"{self.__class__.__name__}({', '.join(field_strings)})\"\n</code></pre>"},{"location":"template/#reasoning_core.template.Config-functions","title":"Functions","text":""},{"location":"template/#reasoning_core.template.Config.get_true_value","title":"<code>get_true_value(name)</code>","text":"<p>Returns the unrounded float value of a stochastic field.</p> Source code in <code>reasoning_core/template.py</code> <pre><code>def get_true_value(self, name: str) -&gt; float:\n    \"\"\"Returns the unrounded float value of a stochastic field.\"\"\"\n    if name in self._stochastic_fields:\n        return getattr(self._unrounded, name)\n    return getattr(self, name)\n</code></pre>"},{"location":"template/#problem","title":"Problem","text":""},{"location":"template/#reasoning_core.template.Problem","title":"<code>reasoning_core.template.Problem</code>","text":"<p>               Bases: <code>Mapping</code></p> Source code in <code>reasoning_core/template.py</code> <pre><code>class Problem(Mapping):\n    def __init__(self, metadata, answer=None, cot=None):\n        self.metadata = edict(metadata)\n        self.answer = answer\n        self.prompt = None\n        self.task = self.metadata.get('task', None)\n        if cot is not None and self.metadata.cot is None:\n            self.metadata.cot = cot\n\n    def to_dict(self):\n        return {\n            'prompt': self.prompt,\n            'answer': self.answer,\n            'metadata': self.metadata,\n            'task': self.task,\n        }\n\n    @classmethod\n    def from_dict(cls, d):\n        data = deserialize(d[\"data\"])\n        return cls(data=data, answer=d.get(\"answer\"), meta=d.get(\"meta\"), task=d.get(\"task\"))\n\n    def __repr__(self):\n        s=\"\"\n        for k,v in self.to_dict().items():\n            s+=f\"---{k.title()}:{v}\\n\"\n        return s\n\n    __str__=__repr__\n\n    def __getitem__(self,k):\n        return getattr(self,k)\n    def __iter__(self):\n        yield from self.to_dict().items()\n    def keys(self):\n        return self.to_dict().keys()\n    def __len__(self):\n        return len(self.to_dict())\n</code></pre>"},{"location":"template/#reward","title":"Reward","text":""},{"location":"template/#reasoning_core.template.Reward","title":"<code>reasoning_core.template.Reward</code>","text":"<p>               Bases: <code>ObjectProxy</code></p> Source code in <code>reasoning_core/template.py</code> <pre><code>class Reward(wrapt.ObjectProxy):\n    def __init__(self, wrapped, tag=None, **kwargs):\n        super().__init__(wrapped)\n        self._self_annotations = {'tag':tag, **kwargs}\n\n    def __getattr__(self, name):\n        if name == \"_self_annotations\":\n            return super().__getattr__(name)\n        if name in self._self_annotations:\n            return self._self_annotations[name]\n        return getattr(self.__wrapped__, name)\n\n    def __setattr__(self, name, value):\n        if name in (\"_self_annotations\", \"__wrapped__\"):\n            super().__setattr__(name, value)\n        elif name in self._self_annotations:\n            self._self_annotations[name] = value\n        else:\n            setattr(self.__wrapped__, name, value)\n</code></pre>"},{"location":"notebooks/Arithmetics/","title":"Arithmetic Reasoning with Reasoning Core","text":"In\u00a0[1]: Copied! <pre>import sys\nimport os\n\n# Ensure reasoning_core is in the path\nsys.path.append(os.path.abspath(\"..\"))\n\nimport reasoning_core as rcr\nfrom IPython.display import display, Markdown\n</pre> import sys import os  # Ensure reasoning_core is in the path sys.path.append(os.path.abspath(\"..\"))  import reasoning_core as rcr from IPython.display import display, Markdown In\u00a0[2]: Copied! <pre>task_arith = rcr.get_task(\"Arithmetics\")\nprint(\"Task initialized:\", task_arith.task_name)\n</pre> task_arith = rcr.get_task(\"Arithmetics\") print(\"Task initialized:\", task_arith.task_name) <pre>Task initialized: arithmetics\n</pre> <p>We'll also initialize the <code>SymbolicArithmetics</code> task, which performs algebraic simplifications.</p> In\u00a0[3]: Copied! <pre>task_sym = rcr.get_task(\"SymbolicArithmetics\")\nprint(\"Task initialized:\", task_sym.task_name)\n</pre> task_sym = rcr.get_task(\"SymbolicArithmetics\") print(\"Task initialized:\", task_sym.task_name) <pre>Task initialized: symbolic_arithmetics\n</pre> In\u00a0[4]: Copied! <pre># Set a seed and demonstrate Level 0\ntask_arith.config.set_level(0)\n\nprint(f\"Level 0 Config Depth: Min {task_arith.config.min_depth}, Max {task_arith.config.max_depth}\\n\")\n\nexample_l0 = task_arith.generate_example()\nprint(\"Level 0 Prompt: \\n\", example_l0.prompt)\nprint(\"\\nLevel 0 Answer: \", example_l0.answer)\n</pre> # Set a seed and demonstrate Level 0 task_arith.config.set_level(0)  print(f\"Level 0 Config Depth: Min {task_arith.config.min_depth}, Max {task_arith.config.max_depth}\\n\")  example_l0 = task_arith.generate_example() print(\"Level 0 Prompt: \\n\", example_l0.prompt) print(\"\\nLevel 0 Answer: \", example_l0.answer) <pre>Level 0 Config Depth: Min 3, Max 5\n\nLevel 0 Prompt: \n Evaluate 14 / -7.\nAnswer with only a number.\n\nLevel 0 Answer:  -2\n</pre> In\u00a0[5]: Copied! <pre>task_arith.config.set_level(3)\nprint(f\"Level 3 Config Depth: Min {task_arith.config.min_depth}, Max {task_arith.config.max_depth}\\n\")\n\nexample_l3 = task_arith.generate_example()\nprint(\"Level 3 Prompt: \\n\", example_l3.prompt)\nprint(\"\\nLevel 3 Answer: \", example_l3.answer)\n</pre> task_arith.config.set_level(3) print(f\"Level 3 Config Depth: Min {task_arith.config.min_depth}, Max {task_arith.config.max_depth}\\n\")  example_l3 = task_arith.generate_example() print(\"Level 3 Prompt: \\n\", example_l3.prompt) print(\"\\nLevel 3 Answer: \", example_l3.answer) <pre>Level 3 Config Depth: Min 6, Max 8\n\nLevel 3 Prompt: \n Evaluate -12 - (-3) / -2 / (5) + abs(-9.20) - 5.\nAnswer with only a number.\n\nLevel 3 Answer:  -8.1\n</pre> In\u00a0[6]: Copied! <pre>print(\"-&gt; Level 0 CoT:\\n\")\nprint(example_l0.metadata.cot)\n\nprint(\"\\n-&gt; Level 3 CoT:\\n\")\nprint(example_l3.metadata.cot)\n</pre> print(\"-&gt; Level 0 CoT:\\n\") print(example_l0.metadata.cot)  print(\"\\n-&gt; Level 3 CoT:\\n\") print(example_l3.metadata.cot) <pre>-&gt; Level 0 CoT:\n\n14 / -7 = -2\n\n-&gt; Level 3 CoT:\n\n-3 / -2 = 1.5\n1.5 / 5 = 0.3\n-12 - 0.3 = -12.3\nabs(-9.2) = 9.2\n-12.3 + 9.2 = -3.1\n-3.1 - 5 = -8.1\n</pre> In\u00a0[7]: Copied! <pre># Force the arithmetic task to always use floats and 4 decimal places\ntask_arith.config.float_prob = 1.0\ntask_arith.config.out_decimals = 4\ntask_arith.config.in_decimals = 4\n\nadvanced_arith_example = task_arith.generate_example()\nprint(\"Float-only Arithmetic:\\n\", advanced_arith_example.prompt)\nprint(\"\\nAnswer:\", advanced_arith_example.answer)\n</pre> # Force the arithmetic task to always use floats and 4 decimal places task_arith.config.float_prob = 1.0 task_arith.config.out_decimals = 4 task_arith.config.in_decimals = 4  advanced_arith_example = task_arith.generate_example() print(\"Float-only Arithmetic:\\n\", advanced_arith_example.prompt) print(\"\\nAnswer:\", advanced_arith_example.answer) <pre>Float-only Arithmetic:\n Evaluate 2.29 + (2.400 + abs(7.5 + -1.91) * -11.7 + -4.22 + -2.2 + 0.73000).\nAnswer with only a number.\n\nAnswer: -66.403\n</pre> In\u00a0[8]: Copied! <pre>task_sym.config.set_level(1)\n\nsym_example = task_sym.generate_example()\nprint(\"Symbolic Prompt:\\n\", sym_example.prompt)\nprint(\"\\nSymbolic Answer:\\n\", sym_example.answer)\nprint(\"\\nSymbolic CoT:\\n\", sym_example.metadata.cot)\n</pre> task_sym.config.set_level(1)  sym_example = task_sym.generate_example() print(\"Symbolic Prompt:\\n\", sym_example.prompt) print(\"\\nSymbolic Answer:\\n\", sym_example.answer) print(\"\\nSymbolic CoT:\\n\", sym_example.metadata.cot) <pre>Symbolic Prompt:\n Simplify the following algebraic expression:\n(y) - 5 + y\n\nAnswer with the simplified expression.\n\nSymbolic Answer:\n 2*y - 5\n\nSymbolic CoT:\n -1*5 = -5\ny + y - 5 = 2*y - 5\n</pre>"},{"location":"notebooks/Arithmetics/#arithmetic-reasoning-with-reasoning-core","title":"Arithmetic Reasoning with Reasoning Core\u00b6","text":"<p>This notebook provides a simple showcase of the <code>reasoning_core</code> library using one of its most fundamental tasks: Arithmetic. These tasks demonstrate evaluating mathematical expressions and algebraic equations.</p> <p>We will cover how to initialize tasks, scale their difficulty levels, examine their generated \"Chain of Thought\" (CoT), and tweak custom generation parameters.</p>"},{"location":"notebooks/Arithmetics/#1-initialize-the-task","title":"1. Initialize the Task\u00b6","text":"<p>We begin by instantiating the <code>Arithmetics</code> task. This task generates procedurally created mathematical expressions that must optionally be reduced using the provided mathematical operators.</p>"},{"location":"notebooks/Arithmetics/#2-understanding-difficulty-levels","title":"2. Understanding Difficulty Levels\u00b6","text":"<p>The library dynamically scales the difficulty of problems using <code>.config.set_level(level)</code>. As the level increases, expressions become longer and deeper, evaluating more complex mathematical trees.</p>"},{"location":"notebooks/Arithmetics/#21-basic-level-0-generation","title":"2.1 Basic Level 0 Generation\u00b6","text":""},{"location":"notebooks/Arithmetics/#22-advanced-level-3-generation","title":"2.2 Advanced Level 3 Generation\u00b6","text":"<p>When we increase the level to 3, the depth of the abstract syntax tree for the expression expands significantly.</p>"},{"location":"notebooks/Arithmetics/#23-chain-of-thought-cot","title":"2.3 Chain of Thought (CoT)\u00b6","text":"<p>The abstract syntax tree isn't just used for answer evaluation\u2014it also generates step-by-step intermediate computations to guide Large Language Models through zero-shot reasoning.</p>"},{"location":"notebooks/Arithmetics/#3-configuration-and-task-options","title":"3. Configuration and Task Options\u00b6","text":"<p>Each task in <code>reasoning_core</code> has an associated configuration object. For arithmetic, we can explicitly alter properties like trailing zero possibilities, decimal precision, or floating point occurrences.</p>"},{"location":"notebooks/Arithmetics/#4-symbolic-arithmetics","title":"4. Symbolic Arithmetics\u00b6","text":"<p>To showcase the structural flexibility of tasks, you can also use <code>SymbolicArithmetics</code>, which generates algebraic simplification problems instead of numerical evaluations.</p>"},{"location":"notebooks/Causal_reasoning/","title":"Creating a Causal Reasoning Dataset with Reasoning Core","text":"In\u00a0[30]: Copied! <pre>pip install -e /mnt/nfs_share_magnet2/share/libs/reasoning_core\n</pre> pip install -e /mnt/nfs_share_magnet2/share/libs/reasoning_core <pre>Obtaining file:///mnt/nfs_share_magnet2/share/libs/reasoning_core\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Installing backend dependencies ... done\n  Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: appdirs in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.4.4)\nRequirement already satisfied: beautifulsoup4 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (4.14.2)\nRequirement already satisfied: duckdb in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.4.2)\nRequirement already satisfied: easydict in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.13)\nRequirement already satisfied: exrex in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.12.0)\nRequirement already satisfied: faker in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (38.2.0)\nRequirement already satisfied: funcy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2.0)\nRequirement already satisfied: gramforge in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.0.0)\nRequirement already satisfied: inflection in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.5.1)\nRequirement already satisfied: lazy-object-proxy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.12.0)\nRequirement already satisfied: multiprocess in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.70.18)\nRequirement already satisfied: networkx in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (3.2.1)\nRequirement already satisfied: nltk in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (3.9.2)\nRequirement already satisfied: num2words in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.5.14)\nRequirement already satisfied: numpy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2.3.4)\nRequirement already satisfied: pandas in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2.3.3)\nRequirement already satisfied: pgmpy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.0.0)\nRequirement already satisfied: pooch in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.8.2)\nRequirement already satisfied: prime in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.4.12)\nRequirement already satisfied: pyparsing in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (3.2.5)\nRequirement already satisfied: pyyaml in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (6.0.3)\nRequirement already satisfied: rapidfuzz in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (3.14.3)\nRequirement already satisfied: reasoning-gym in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.1.24)\nRequirement already satisfied: regex in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2025.11.3)\nRequirement already satisfied: requests in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2.32.5)\nRequirement already satisfied: sympy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.14.0)\nRequirement already satisfied: tabulate in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.9.0)\nRequirement already satisfied: tarski in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.8.2)\nRequirement already satisfied: tiktoken in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.12.0)\nRequirement already satisfied: timeout-decorator in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.5.0)\nRequirement already satisfied: timeoutcontext in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (2.0.0)\nRequirement already satisfied: tqdm in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (4.67.1)\nRequirement already satisfied: udocker in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.3.17)\nRequirement already satisfied: unified-planning[pyperplan] in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (1.2.0)\nRequirement already satisfied: verifiers in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.1.8.post1)\nRequirement already satisfied: xpflow in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning_core==0.2.0) (0.8)\nRequirement already satisfied: soupsieve&gt;1.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from beautifulsoup4-&gt;reasoning_core==0.2.0) (2.8)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from beautifulsoup4-&gt;reasoning_core==0.2.0) (4.15.0)\nRequirement already satisfied: tzdata in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from faker-&gt;reasoning_core==0.2.0) (2025.2)\nRequirement already satisfied: anytree&lt;3,&gt;=2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from gramforge-&gt;reasoning_core==0.2.0) (2.13.0)\nRequirement already satisfied: dill&gt;=0.4.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from multiprocess-&gt;reasoning_core==0.2.0) (0.4.0)\nRequirement already satisfied: click in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from nltk-&gt;reasoning_core==0.2.0) (8.3.1)\nRequirement already satisfied: joblib in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from nltk-&gt;reasoning_core==0.2.0) (1.5.2)\nRequirement already satisfied: docopt&gt;=0.6.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from num2words-&gt;reasoning_core==0.2.0) (0.6.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pandas-&gt;reasoning_core==0.2.0) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pandas-&gt;reasoning_core==0.2.0) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;reasoning_core==0.2.0) (1.17.0)\nRequirement already satisfied: scipy in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (1.16.3)\nRequirement already satisfied: scikit-learn in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (1.7.2)\nRequirement already satisfied: torch in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (2.9.1)\nRequirement already satisfied: statsmodels in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (0.14.5)\nRequirement already satisfied: opt-einsum in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (3.4.0)\nRequirement already satisfied: pyro-ppl in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pgmpy-&gt;reasoning_core==0.2.0) (1.9.1)\nRequirement already satisfied: platformdirs&gt;=2.5.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pooch-&gt;reasoning_core==0.2.0) (4.5.0)\nRequirement already satisfied: packaging&gt;=20.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pooch-&gt;reasoning_core==0.2.0) (25.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from requests-&gt;reasoning_core==0.2.0) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from requests-&gt;reasoning_core==0.2.0) (3.11)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from requests-&gt;reasoning_core==0.2.0) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from requests-&gt;reasoning_core==0.2.0) (2025.11.12)\nRequirement already satisfied: build&gt;=1.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (1.3.0)\nRequirement already satisfied: cryptography&gt;=41.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (46.0.3)\nRequirement already satisfied: prime-evals&gt;=0.1.3 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (0.1.4)\nRequirement already satisfied: prime-sandboxes&gt;=0.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (0.2.4)\nRequirement already satisfied: rich&gt;=13.3.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (14.2.0)\nRequirement already satisfied: toml&gt;=0.10.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (0.10.2)\nRequirement already satisfied: typer&gt;=0.9.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-&gt;reasoning_core==0.2.0) (0.20.0)\nRequirement already satisfied: pyproject_hooks in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from build&gt;=1.0.0-&gt;prime-&gt;reasoning_core==0.2.0) (1.2.0)\nRequirement already satisfied: cffi&gt;=2.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from cryptography&gt;=41.0.0-&gt;prime-&gt;reasoning_core==0.2.0) (2.0.0)\nRequirement already satisfied: pycparser in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from cffi&gt;=2.0.0-&gt;cryptography&gt;=41.0.0-&gt;prime-&gt;reasoning_core==0.2.0) (2.23)\nRequirement already satisfied: httpx&gt;=0.25.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (0.28.1)\nRequirement already satisfied: pydantic&gt;=2.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (2.12.5)\nRequirement already satisfied: anyio in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from httpx&gt;=0.25.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from httpx&gt;=0.25.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (1.0.9)\nRequirement already satisfied: h11&gt;=0.16 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from httpcore==1.*-&gt;httpx&gt;=0.25.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (0.16.0)\nRequirement already satisfied: aiofiles&gt;=23.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from prime-sandboxes&gt;=0.1.0-&gt;prime-&gt;reasoning_core==0.2.0) (25.1.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pydantic&gt;=2.0.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pydantic&gt;=2.0.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (2.41.5)\nRequirement already satisfied: typing-inspection&gt;=0.4.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pydantic&gt;=2.0.0-&gt;prime-evals&gt;=0.1.3-&gt;prime-&gt;reasoning_core==0.2.0) (0.4.2)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from rich&gt;=13.3.1-&gt;prime-&gt;reasoning_core==0.2.0) (4.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from rich&gt;=13.3.1-&gt;prime-&gt;reasoning_core==0.2.0) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=13.3.1-&gt;prime-&gt;reasoning_core==0.2.0) (0.1.2)\nRequirement already satisfied: shellingham&gt;=1.3.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from typer&gt;=0.9.0-&gt;prime-&gt;reasoning_core==0.2.0) (1.5.4)\nRequirement already satisfied: datasets in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (4.4.1)\nRequirement already satisfied: jinja2&gt;=3.1.6 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (3.1.6)\nRequirement already satisfied: nest-asyncio&gt;=1.6.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (1.6.0)\nRequirement already satisfied: openai-agents&gt;=0.0.7 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (0.6.1)\nRequirement already satisfied: openai&gt;=1.108.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (2.8.1)\nRequirement already satisfied: textual in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (6.6.0)\nRequirement already satisfied: wget&gt;=3.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from verifiers-&gt;reasoning_core==0.2.0) (3.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from jinja2&gt;=3.1.6-&gt;verifiers-&gt;reasoning_core==0.2.0) (3.0.3)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai&gt;=1.108.1-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.9.0)\nRequirement already satisfied: jiter&lt;1,&gt;=0.10.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai&gt;=1.108.1-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.12.0)\nRequirement already satisfied: sniffio in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai&gt;=1.108.1-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.3.1)\nRequirement already satisfied: griffe&lt;2,&gt;=1.5.6 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.15.0)\nRequirement already satisfied: mcp&lt;2,&gt;=1.11.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.22.0)\nRequirement already satisfied: types-requests&lt;3,&gt;=2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (2.32.4.20250913)\nRequirement already satisfied: colorama&gt;=0.4 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from griffe&lt;2,&gt;=1.5.6-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.4.6)\nRequirement already satisfied: httpx-sse&gt;=0.4 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.4.3)\nRequirement already satisfied: jsonschema&gt;=4.20.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (4.25.1)\nRequirement already satisfied: pydantic-settings&gt;=2.5.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (2.12.0)\nRequirement already satisfied: pyjwt&gt;=2.10.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pyjwt[crypto]&gt;=2.10.1-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (2.10.1)\nRequirement already satisfied: python-multipart&gt;=0.0.9 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.0.20)\nRequirement already satisfied: sse-starlette&gt;=1.6.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (3.0.3)\nRequirement already satisfied: starlette&gt;=0.27 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.50.0)\nRequirement already satisfied: uvicorn&gt;=0.31.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.38.0)\nRequirement already satisfied: attrs&gt;=22.2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from jsonschema&gt;=4.20.0-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (25.4.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from jsonschema&gt;=4.20.0-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (2025.9.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from jsonschema&gt;=4.20.0-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.37.0)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from jsonschema&gt;=4.20.0-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.29.0)\nRequirement already satisfied: python-dotenv&gt;=0.21.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pydantic-settings&gt;=2.5.2-&gt;mcp&lt;2,&gt;=1.11.0-&gt;openai-agents&gt;=0.0.7-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.2.1)\nRequirement already satisfied: filelock in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (3.20.0)\nRequirement already satisfied: pyarrow&gt;=21.0.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (22.0.0)\nRequirement already satisfied: xxhash in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (3.6.0)\nRequirement already satisfied: fsspec&lt;=2025.10.0,&gt;=2023.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (2025.10.0)\nRequirement already satisfied: huggingface-hub&lt;2.0,&gt;=0.25.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.36.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (3.13.2)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from huggingface-hub&lt;2.0,&gt;=0.25.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.2.0)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.4.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.4.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.8.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (6.7.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.4.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.10.0,&gt;=2023.1.0-&gt;datasets-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.22.0)\nRequirement already satisfied: pyro-api&gt;=0.1.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pyro-ppl-&gt;pgmpy-&gt;reasoning_core==0.2.0) (0.1.2)\nRequirement already satisfied: setuptools in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (80.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (1.13.1.3)\nRequirement already satisfied: triton==3.5.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from torch-&gt;pgmpy-&gt;reasoning_core==0.2.0) (3.5.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from sympy-&gt;reasoning_core==0.2.0) (1.3.0)\nRequirement already satisfied: arckit==0.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (0.1.0)\nRequirement already satisfied: bfi==1.0.4 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (1.0.4)\nRequirement already satisfied: cellpylib==2.4.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (2.4.0)\nRequirement already satisfied: magiccube==0.3.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (0.3.0)\nRequirement already satisfied: pycosat==0.6.6 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (0.6.6)\nRequirement already satisfied: pyfiglet==1.0.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (1.0.2)\nRequirement already satisfied: zss&gt;=1.2.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from reasoning-gym-&gt;reasoning_core==0.2.0) (1.2.0)\nRequirement already satisfied: drawsvg in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from arckit==0.1.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (2.4.0)\nRequirement already satisfied: matplotlib&gt;=3.0.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (3.10.7)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from matplotlib&gt;=3.0.2-&gt;cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (1.3.3)\nRequirement already satisfied: cycler&gt;=0.10 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from matplotlib&gt;=3.0.2-&gt;cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from matplotlib&gt;=3.0.2-&gt;cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (4.60.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from matplotlib&gt;=3.0.2-&gt;cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (1.4.9)\nRequirement already satisfied: pillow&gt;=8 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from matplotlib&gt;=3.0.2-&gt;cellpylib==2.4.0-&gt;reasoning-gym-&gt;reasoning_core==0.2.0) (12.0.0)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from scikit-learn-&gt;pgmpy-&gt;reasoning_core==0.2.0) (3.6.0)\nRequirement already satisfied: patsy&gt;=0.5.6 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from statsmodels-&gt;pgmpy-&gt;reasoning_core==0.2.0) (1.0.2)\nRequirement already satisfied: multipledispatch in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from tarski-&gt;reasoning_core==0.2.0) (1.0.0)\nRequirement already satisfied: antlr4-python3-runtime==4.7.2 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from tarski-&gt;reasoning_core==0.2.0) (4.7.2)\nRequirement already satisfied: psutil in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from tarski-&gt;reasoning_core==0.2.0) (7.1.3)\nRequirement already satisfied: mdit-py-plugins in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from textual-&gt;verifiers-&gt;reasoning_core==0.2.0) (0.5.0)\nRequirement already satisfied: linkify-it-py&lt;3,&gt;=1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from markdown-it-py[linkify]&gt;=2.1.0-&gt;textual-&gt;verifiers-&gt;reasoning_core==0.2.0) (2.0.3)\nRequirement already satisfied: uc-micro-py in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from linkify-it-py&lt;3,&gt;=1-&gt;markdown-it-py[linkify]&gt;=2.1.0-&gt;textual-&gt;verifiers-&gt;reasoning_core==0.2.0) (1.0.3)\nRequirement already satisfied: ConfigSpace in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from unified-planning[pyperplan]-&gt;reasoning_core==0.2.0) (1.2.1)\nRequirement already satisfied: up-pyperplan~=1.1.0 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from unified-planning[pyperplan]-&gt;reasoning_core==0.2.0) (1.1.0)\nRequirement already satisfied: pyperplan==2.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from up-pyperplan~=1.1.0-&gt;unified-planning[pyperplan]-&gt;reasoning_core==0.2.0) (2.1)\nRequirement already satisfied: wheel in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from pyperplan==2.1-&gt;up-pyperplan~=1.1.0-&gt;unified-planning[pyperplan]-&gt;reasoning_core==0.2.0) (0.45.1)\nRequirement already satisfied: more_itertools in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from ConfigSpace-&gt;unified-planning[pyperplan]-&gt;reasoning_core==0.2.0) (10.8.0)\nRequirement already satisfied: sorcery in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from xpflow-&gt;reasoning_core==0.2.0) (0.2.2)\nRequirement already satisfied: executing in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from sorcery-&gt;xpflow-&gt;reasoning_core==0.2.0) (2.2.1)\nRequirement already satisfied: littleutils&gt;=0.2.1 in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from sorcery-&gt;xpflow-&gt;reasoning_core==0.2.0) (0.2.4)\nRequirement already satisfied: asttokens in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from sorcery-&gt;xpflow-&gt;reasoning_core==0.2.0) (3.0.1)\nRequirement already satisfied: wrapt in /home/vlacombe/micromamba/envs/py312/lib/python3.12/site-packages (from sorcery-&gt;xpflow-&gt;reasoning_core==0.2.0) (2.0.1)\nBuilding wheels for collected packages: reasoning_core\n  Building editable for reasoning_core (pyproject.toml) ... done\n  Created wheel for reasoning_core: filename=reasoning_core-0.2.0-py2.py3-none-any.whl size=3248 sha256=2bc65b6108f8d280b7d9d894c0ef028c188271fede3ff97260325152c1666842\n  Stored in directory: /tmp/pip-ephem-wheel-cache-p6ug0a7l/wheels/b0/64/10/477cb8d84e41ef54d000432461f19a9984d02a9f1d29d9130e\nSuccessfully built reasoning_core\nInstalling collected packages: reasoning_core\n  Attempting uninstall: reasoning_core\n    Found existing installation: reasoning_core 0.2.0\n    Uninstalling reasoning_core-0.2.0:\n      Successfully uninstalled reasoning_core-0.2.0\nSuccessfully installed reasoning_core-0.2.0\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[6]: Copied! <pre>import sys\nimport os\n\n# Ensure reasoning_core is in the path\nsys.path.append(os.path.abspath(\"..\"))\n\nimport pandas as pd\nimport reasoning_core as rcr\n</pre> import sys import os  # Ensure reasoning_core is in the path sys.path.append(os.path.abspath(\"..\"))  import pandas as pd import reasoning_core as rcr In\u00a0[7]: Copied! <pre>task_r1 = rcr.get_task(\"BayesianAssociation\")\nprint(\"Task initialized:\", task_r1.task_name)\n</pre> task_r1 = rcr.get_task(\"BayesianAssociation\") print(\"Task initialized:\", task_r1.task_name) <pre>Task initialized: bayesian_association\n</pre> <p>We can also use the <code>Rung 2</code>  version of the Pearl's ladder of causation.</p> In\u00a0[8]: Copied! <pre>task_r2 = rcr.get_task(\"BayesianIntervention\")\n</pre> task_r2 = rcr.get_task(\"BayesianIntervention\") In\u00a0[9]: Copied! <pre># Demonstrate Level 0\ntask_r1.config.set_level(0)\nprint(f\"Level 0 Config:\\n {task_r1.config}\\n\")\nexample_l0 = task_r1.generate_example()\nprint(\"Level 0 Question excerpt: \\n\", example_l0)\n</pre> # Demonstrate Level 0 task_r1.config.set_level(0) print(f\"Level 0 Config:\\n {task_r1.config}\\n\") example_l0 = task_r1.generate_example() print(\"Level 0 Question excerpt: \\n\", example_l0) <pre>Level 0 Config:\n Rung12Config(c=1.0, level=0, seed=None, size=None, n_nodes=3, max_domain_size=2, edge_prob=0.5, graph_generation_mode='erdos', n_round=1, cpt_relative_threshold=0, cot_scientific_notation=False, is_verbose=False, concise_cot=True)\n\nLevel 0 Question excerpt: \n ---Prompt:System:\nP(X_0) = {'0': 0.8, '1': 0.2} \nP(X_1|X_0=0) = {'0': 0.6, '1': 0.4} \nP(X_1|X_0=1) = {'0': 0.4, '1': 0.6} \nP(X_2) = {'0': 0.4, '1': 0.6}\nObserved conditions:\nObserving/Knowing that the state X_2 is equal to 1\nTask: Compute probability distribution for X_0 (possible values: [0, 1]).\n\nOutput: Python dict mapping each value to its probability, rounded to 1 decimals.\nExample: {0: 0.1, 1: 0.9}\n---Answer:{0: 0.8, 1: 0.2}\n---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.8, 0.2 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.6, 0.4;\\n    ( 1 ) 0.4, 0.6;\\n\\n}\\nprobability ( X_2 ) {\\n    table 0.4, 0.6 ;\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_2 is equal to 1', 'target': 'X_0', 'variables': ['X_0', 'X_1', 'X_2'], 'n_round': 1, 'cot': 'Result: P(X_0) = {0: 0.8, 1: 0.2}\\nResult: P(X_0) = {0: 0.8, 1: 0.2}', '_time': 1.6101374626159668, '_task': 'bayesian_association', '_level': 0, '_config': {'c': 1.0, 'level': 0, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 179, '_cot_tokens': 63}\n---Task:bayesian_association\n\n</pre> In\u00a0[10]: Copied! <pre># Demonstrate Level 3\ntask_r1.config.set_level(3)\nprint(f\"\\nLevel 3 Config:\\n {task_r1.config}\\n\")\nexample_l3 = task_r1.generate_example()\nprint(\"Level 3 Question excerpt: \\n\", example_l3)\n</pre> # Demonstrate Level 3 task_r1.config.set_level(3) print(f\"\\nLevel 3 Config:\\n {task_r1.config}\\n\") example_l3 = task_r1.generate_example() print(\"Level 3 Question excerpt: \\n\", example_l3) <pre>\nLevel 3 Config:\n Rung12Config(c=1.0, level=3, seed=None, size=None, n_nodes=4, max_domain_size=4, edge_prob=0.5, graph_generation_mode='erdos', n_round=3, cpt_relative_threshold=1.5, cot_scientific_notation=False, is_verbose=False, concise_cot=True)\n\nLevel 3 Question excerpt: \n ---Prompt:System:\nP(X_0) = {'0': 0.546, '1': 0.454} \nP(X_1|X_0=0) = {'0': 0.465, '1': 0.535} \nP(X_1|X_0=1) = {'0': 0.398, '1': 0.602} \nX_3 ~ Noisy-MAX(leak=None, influences={'X_0': {'1': [0.566, 0.24, 0.194]}, 'X_1': {'1': [0.571, 0.386, 0.043]}, 'X_2': {'1': [0.394, 0.465, 0.141]}}) \nX_4 ~ Noisy-MIN(leak=None, influences={'X_0': {'1': [0.0, 0.0, 1.0]}, 'X_1': {'1': [0.0, 0.0, 1.0]}, 'X_2': {'1': [0.0, 0.0, 1.0]}}) \nP(X_2) = {'0': 0.845, '1': 0.155}\nObserved conditions:\nObserving/Knowing that the state X_2 is equal to 0, and the state X_1 is equal to 1, and the state X_4 is equal to 0, and the state X_3 is equal to 1\nTask: Compute probability distribution for X_0 (possible values: [0, 1]).\n\nOutput: Python dict mapping each value to its probability, rounded to 3 decimals.\nExample: {0: 0.123, 1: 0.877}\n---Answer:{0: 0.579, 1: 0.421}\n---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1, 2], 'X_0': [0, 1], 'X_1': [0, 1], 'X_2': [0, 1]}\\n// type: MultilevelInfluenceModel\\n// mode: MAX\\n// leak: None\\n// influence_tables: {'X_0': {0: [1.0, 0.0, 0.0], 1: [0.566, 0.24, 0.194]}, 'X_1': {0: [1.0, 0.0, 0.0], 1: [0.571, 0.386, 0.043]}, 'X_2': {0: [1.0, 0.0, 0.0], 1: [0.394, 0.465, 0.141]}}\\n// parents: ['X_0', 'X_1', 'X_2']\\n// CANONICAL\\n// variable: X_4\\n// state_names: {'X_4': [0, 1, 2], 'X_0': [0, 1], 'X_1': [0, 1], 'X_2': [0, 1]}\\n// type: MultilevelInfluenceModel\\n// mode: MIN\\n// leak: None\\n// influence_tables: {'X_0': {0: [0.155, 0.209, 0.636], 1: [0.0, 0.0, 1.0]}, 'X_1': {0: [0.071, 0.295, 0.634], 1: [0.0, 0.0, 1.0]}, 'X_2': {0: [0.239, 0.301, 0.46], 1: [0.0, 0.0, 1.0]}}\\n// parents: ['X_0', 'X_1', 'X_2']\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_4 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.546, 0.454 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.465, 0.535;\\n    ( 1 ) 0.398, 0.602;\\n\\n}\\nprobability ( X_2 ) {\\n    table 0.845, 0.155 ;\\n}\\nprobability ( X_3 | X_0, X_1, X_2 ) {\\n    ( 0, 0, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 0, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 1, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 1, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 0, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 0, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 1, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 1, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n\\n}\\nprobability ( X_4 | X_0, X_1, X_2 ) {\\n    ( 0, 0, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 0, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 1, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 0, 1, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 0, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 0, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 1, 0 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n    ( 1, 1, 1 ) 0.3333333333333333, 0.3333333333333333, 0.3333333333333333;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_2 is equal to 0, and the state X_1 is equal to 1, and the state X_4 is equal to 0, and the state X_3 is equal to 1', 'target': 'X_0', 'variables': ['X_0', 'X_1', 'X_3', 'X_4', 'X_2'], 'n_round': 3, 'cot': 'Normalize (sum=0.07) -&gt; P(X_0 | X_1=1, X_2=0, X_3=1, X_4=0) = {0: 0.579, 1: 0.421}\\nNormalize (sum=0.07) -&gt; P(X_0 | X_1=1, X_2=0, X_3=1, X_4=0) = {0: 0.579, 1: 0.421}', '_time': 1.351759672164917, '_task': 'bayesian_association', '_level': 3, '_config': {'c': 1.0, 'level': 3, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 1.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 385, '_cot_tokens': 125}\n---Task:bayesian_association\n\n</pre> In\u00a0[11]: Copied! <pre>print(f\"Level 0 Cot example: \\n{example_l0.metadata.cot}\\n\")\n\nprint(f\"Level 3 Cot example: \\n{example_l3.metadata.cot}\\n\")\n</pre> print(f\"Level 0 Cot example: \\n{example_l0.metadata.cot}\\n\")  print(f\"Level 3 Cot example: \\n{example_l3.metadata.cot}\\n\") <pre>Level 0 Cot example: \nResult: P(X_0) = {0: 0.8, 1: 0.2}\nResult: P(X_0) = {0: 0.8, 1: 0.2}\n\nLevel 3 Cot example: \nNormalize (sum=0.07) -&gt; P(X_0 | X_1=1, X_2=0, X_3=1, X_4=0) = {0: 0.579, 1: 0.421}\nNormalize (sum=0.07) -&gt; P(X_0 | X_1=1, X_2=0, X_3=1, X_4=0) = {0: 0.579, 1: 0.421}\n\n</pre> In\u00a0[12]: Copied! <pre>#set tasks to level 2\ntask_r1.config.set_level(2)\ntask_r2.config.set_level(2)\n\ntask_r1.config.set_seed(graph_seed=42, conditionning_seed=24)\ntask_r2.config.set_seed(graph_seed=42, conditionning_seed=24)\n\ntwin_r1_example = task_r1.generate_example()\nprint(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)\n\nprint(\"\\n ------------------------------ \\n\")\n\ntwin_r2_example = task_r2.generate_example()\nprint(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer)\n</pre> #set tasks to level 2 task_r1.config.set_level(2) task_r2.config.set_level(2)  task_r1.config.set_seed(graph_seed=42, conditionning_seed=24) task_r2.config.set_seed(graph_seed=42, conditionning_seed=24)  twin_r1_example = task_r1.generate_example() print(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)  print(\"\\n ------------------------------ \\n\")  twin_r2_example = task_r2.generate_example() print(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer) <pre>Rung1 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nX_2 ~ Noisy-MAX(leak=None, influences={'X_0': {'1': [0.61, 0.38, 0.01]}, 'X_1': {'1': [0.79, 0.17, 0.04], '2': [0.56, 0.34, 0.1]}}) \nX_3 ~ Noisy-MIN(leak=None, influences={'X_0': {'1': [0.0, 1.0]}, 'X_1': {'1': [0.2, 0.8], '2': [0.0, 1.0]}, 'X_2': {'1': [0.23, 0.77], '2': [0.0, 1.0]}}) \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42}\nObserved conditions:\nObserving/Knowing that the state X_0 is equal to 0, and the state X_3 is equal to 0, and the state X_2 is equal to 2\nTask: Compute probability distribution for X_1 (possible values: [0, 1, 2]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.0, 1: 0.2, 2: 0.8}\n\n ------------------------------ \n\nRung2 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nX_2 ~ Noisy-MAX(leak=None, influences={'X_0': {'1': [0.61, 0.38, 0.01]}, 'X_1': {'1': [0.79, 0.17, 0.04], '2': [0.56, 0.34, 0.1]}}) \nX_3 ~ Noisy-MIN(leak=None, influences={'X_0': {'1': [0.0, 1.0]}, 'X_1': {'1': [0.2, 0.8], '2': [0.0, 1.0]}, 'X_2': {'1': [0.23, 0.77], '2': [0.0, 1.0]}}) \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42}\nObserved conditions:\nDoing/Imposing that the state X_2 is equal to 2. Observing/Knowing that the state X_0 is equal to 0, and the state X_3 is equal to 0\nTask: Compute probability distribution for X_1 (possible values: [0, 1, 2]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.42424242424242425, 1: 0.22222222222222224, 2: 0.35353535353535354}\n</pre> <p>Noisy-OR/AND for binary interaction, and there extension Noisy-MAX/MIN are aiming to reduce the size of the exponentially growing parameter's size (w.r.t to their \"parents' size\")</p> In\u00a0[13]: Copied! <pre>task_r1.config.Noisy_mode, task_r2.config.Noisy_mode = False , False #disable Noisy interaction\n\ntwin_r1_example = task_r1.generate_example()\nprint(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)\n\nprint(\"\\n ------------------------------ \\n\")\n\ntwin_r2_example = task_r2.generate_example()\nprint(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer)\n</pre> task_r1.config.Noisy_mode, task_r2.config.Noisy_mode = False , False #disable Noisy interaction  twin_r1_example = task_r1.generate_example() print(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)  print(\"\\n ------------------------------ \\n\")  twin_r2_example = task_r2.generate_example() print(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer) <pre>Rung1 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nP(X_2|X_0=0, X_1=0) = {'0': 0.35, '1': 0.35, '2': 0.3} \nP(X_2|X_0=0, X_1=1) = {'0': 0.22, '1': 0.38, '2': 0.4} \nP(X_2|X_0=0, X_1=2) = {'0': 0.6, '1': 0.09, '2': 0.31} \nP(X_2|X_0=1, X_1=0) = {'0': 0.51, '1': 0.33, '2': 0.16} \nP(X_2|X_0=1, X_1=1) = {'0': 0.09, '1': 0.36, '2': 0.55} \nP(X_2|X_0=1, X_1=2) = {'0': 0.5, '1': 0.47, '2': 0.03} \nP(X_3|X_0=0, X_1=0, X_2=0) = {'0': 0.48, '1': 0.52} \nP(X_3|X_0=0, X_1=0, X_2=1) = {'0': 0.41, '1': 0.59} \nP(X_3|X_0=0, X_1=0, X_2=2) = {'0': 0.53, '1': 0.47} \nP(X_3|X_0=0, X_1=1, X_2=0) = {'0': 0.66, '1': 0.34} \nP(X_3|X_0=0, X_1=1, X_2=1) = {'0': 0.09, '1': 0.91} \nP(X_3|X_0=0, X_1=1, X_2=2) = {'0': 0.52, '1': 0.48} \nP(X_3|X_0=0, X_1=2, X_2=0) = {'0': 0.49, '1': 0.51} \nP(X_3|X_0=0, X_1=2, X_2=1) = {'0': 0.8, '1': 0.2} \nP(X_3|X_0=0, X_1=2, X_2=2) = {'0': 0.22, '1': 0.78} \nP(X_3|X_0=1, X_1=0, X_2=0) = {'0': 0.91, '1': 0.09} \nP(X_3|X_0=1, X_1=0, X_2=1) = {'0': 0.71, '1': 0.29} \nP(X_3|X_0=1, X_1=0, X_2=2) = {'0': 0.58, '1': 0.42} \nP(X_3|X_0=1, X_1=1, X_2=0) = {'0': 0.46, '1': 0.54} \nP(X_3|X_0=1, X_1=1, X_2=1) = {'0': 0.46, '1': 0.54} \nP(X_3|X_0=1, X_1=1, X_2=2) = {'0': 0.58, '1': 0.42} \nP(X_3|X_0=1, X_1=2, X_2=0) = {'0': 0.38, '1': 0.62} \nP(X_3|X_0=1, X_1=2, X_2=1) = {'0': 0.54, '1': 0.46} \nP(X_3|X_0=1, X_1=2, X_2=2) = {'0': 0.25, '1': 0.75} \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42}\nObserved conditions:\nObserving/Knowing that the state X_0 is equal to 0, and the state X_3 is equal to 0, and the state X_2 is equal to 2\nTask: Compute probability distribution for X_1 (possible values: [0, 1, 2]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.45, 1: 0.33, 2: 0.22}\n\n ------------------------------ \n\nRung2 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nP(X_2|X_0=0, X_1=0) = {'0': 0.35, '1': 0.35, '2': 0.3} \nP(X_2|X_0=0, X_1=1) = {'0': 0.22, '1': 0.38, '2': 0.4} \nP(X_2|X_0=0, X_1=2) = {'0': 0.6, '1': 0.09, '2': 0.31} \nP(X_2|X_0=1, X_1=0) = {'0': 0.51, '1': 0.33, '2': 0.16} \nP(X_2|X_0=1, X_1=1) = {'0': 0.09, '1': 0.36, '2': 0.55} \nP(X_2|X_0=1, X_1=2) = {'0': 0.5, '1': 0.47, '2': 0.03} \nP(X_3|X_0=0, X_1=0, X_2=0) = {'0': 0.48, '1': 0.52} \nP(X_3|X_0=0, X_1=0, X_2=1) = {'0': 0.41, '1': 0.59} \nP(X_3|X_0=0, X_1=0, X_2=2) = {'0': 0.53, '1': 0.47} \nP(X_3|X_0=0, X_1=1, X_2=0) = {'0': 0.66, '1': 0.34} \nP(X_3|X_0=0, X_1=1, X_2=1) = {'0': 0.09, '1': 0.91} \nP(X_3|X_0=0, X_1=1, X_2=2) = {'0': 0.52, '1': 0.48} \nP(X_3|X_0=0, X_1=2, X_2=0) = {'0': 0.49, '1': 0.51} \nP(X_3|X_0=0, X_1=2, X_2=1) = {'0': 0.8, '1': 0.2} \nP(X_3|X_0=0, X_1=2, X_2=2) = {'0': 0.22, '1': 0.78} \nP(X_3|X_0=1, X_1=0, X_2=0) = {'0': 0.91, '1': 0.09} \nP(X_3|X_0=1, X_1=0, X_2=1) = {'0': 0.71, '1': 0.29} \nP(X_3|X_0=1, X_1=0, X_2=2) = {'0': 0.58, '1': 0.42} \nP(X_3|X_0=1, X_1=1, X_2=0) = {'0': 0.46, '1': 0.54} \nP(X_3|X_0=1, X_1=1, X_2=1) = {'0': 0.46, '1': 0.54} \nP(X_3|X_0=1, X_1=1, X_2=2) = {'0': 0.58, '1': 0.42} \nP(X_3|X_0=1, X_1=2, X_2=0) = {'0': 0.38, '1': 0.62} \nP(X_3|X_0=1, X_1=2, X_2=1) = {'0': 0.54, '1': 0.46} \nP(X_3|X_0=1, X_1=2, X_2=2) = {'0': 0.25, '1': 0.75} \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42}\nObserved conditions:\nDoing/Imposing that the state X_2 is equal to 2. Observing/Knowing that the state X_0 is equal to 0, and the state X_3 is equal to 0\nTask: Compute probability distribution for X_1 (possible values: [0, 1, 2]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.494949494949495, 1: 0.27272727272727276, 2: 0.23232323232323235}\n</pre> <p>Without the Noisy nodes, the number of parameters increase a lot. We might want to reduce the connectivity of the graph.</p> In\u00a0[14]: Copied! <pre>task_r1.config.edge_prob, task_r2.config.edge_prob = 0.25 , 0.25 #reduce the connectivity of the graph\n\ntwin_r1_example = task_r1.generate_example()\nprint(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)\n\nprint(\"\\n ------------------------------ \\n\")\n\ntwin_r2_example = task_r2.generate_example()\nprint(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer)\n</pre> task_r1.config.edge_prob, task_r2.config.edge_prob = 0.25 , 0.25 #reduce the connectivity of the graph  twin_r1_example = task_r1.generate_example() print(\"Rung1 Question excerpt: \\n\", twin_r1_example.prompt,\"\\nAnswer: \", twin_r1_example.answer)  print(\"\\n ------------------------------ \\n\")  twin_r2_example = task_r2.generate_example() print(\"Rung2 Question excerpt: \\n\", twin_r2_example.prompt,\"\\nAnswer: \", twin_r2_example.answer) <pre>Rung1 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nP(X_2|X_0=0, X_1=0) = {'0': 0.35, '1': 0.35, '2': 0.3} \nP(X_2|X_0=0, X_1=1) = {'0': 0.22, '1': 0.38, '2': 0.4} \nP(X_2|X_0=0, X_1=2) = {'0': 0.6, '1': 0.09, '2': 0.31} \nP(X_2|X_0=1, X_1=0) = {'0': 0.51, '1': 0.33, '2': 0.16} \nP(X_2|X_0=1, X_1=1) = {'0': 0.09, '1': 0.36, '2': 0.55} \nP(X_2|X_0=1, X_1=2) = {'0': 0.5, '1': 0.47, '2': 0.03} \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42} \nP(X_3|X_1=0, X_2=0) = {'0': 0.63, '1': 0.37} \nP(X_3|X_1=0, X_2=1) = {'0': 0.54, '1': 0.46} \nP(X_3|X_1=0, X_2=2) = {'0': 0.48, '1': 0.52} \nP(X_3|X_1=1, X_2=0) = {'0': 0.52, '1': 0.48} \nP(X_3|X_1=1, X_2=1) = {'0': 0.1, '1': 0.9} \nP(X_3|X_1=1, X_2=2) = {'0': 0.69, '1': 0.31} \nP(X_3|X_1=2, X_2=0) = {'0': 0.77, '1': 0.23} \nP(X_3|X_1=2, X_2=1) = {'0': 0.59, '1': 0.41} \nP(X_3|X_1=2, X_2=2) = {'0': 0.67, '1': 0.33}\nObserved conditions:\nObserving/Knowing that the state X_0 is equal to 0, and the state X_1 is equal to 0, and the state X_2 is equal to 2\nTask: Compute probability distribution for X_3 (possible values: [0, 1]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.48, 1: 0.52}\n\n ------------------------------ \n\nRung2 Question excerpt: \n System:\nP(X_0) = {'0': 0.64, '1': 0.36} \nP(X_2|X_0=0, X_1=0) = {'0': 0.35, '1': 0.35, '2': 0.3} \nP(X_2|X_0=0, X_1=1) = {'0': 0.22, '1': 0.38, '2': 0.4} \nP(X_2|X_0=0, X_1=2) = {'0': 0.6, '1': 0.09, '2': 0.31} \nP(X_2|X_0=1, X_1=0) = {'0': 0.51, '1': 0.33, '2': 0.16} \nP(X_2|X_0=1, X_1=1) = {'0': 0.09, '1': 0.36, '2': 0.55} \nP(X_2|X_0=1, X_1=2) = {'0': 0.5, '1': 0.47, '2': 0.03} \nP(X_1) = {'0': 0.37, '1': 0.21, '2': 0.42} \nP(X_3|X_1=0, X_2=0) = {'0': 0.63, '1': 0.37} \nP(X_3|X_1=0, X_2=1) = {'0': 0.54, '1': 0.46} \nP(X_3|X_1=0, X_2=2) = {'0': 0.48, '1': 0.52} \nP(X_3|X_1=1, X_2=0) = {'0': 0.52, '1': 0.48} \nP(X_3|X_1=1, X_2=1) = {'0': 0.1, '1': 0.9} \nP(X_3|X_1=1, X_2=2) = {'0': 0.69, '1': 0.31} \nP(X_3|X_1=2, X_2=0) = {'0': 0.77, '1': 0.23} \nP(X_3|X_1=2, X_2=1) = {'0': 0.59, '1': 0.41} \nP(X_3|X_1=2, X_2=2) = {'0': 0.67, '1': 0.33}\nObserved conditions:\nDoing/Imposing that the state X_2 is equal to 2. Observing/Knowing that the state X_0 is equal to 0, and the state X_1 is equal to 0\nTask: Compute probability distribution for X_3 (possible values: [0, 1]).\n\nOutput: Python dict mapping each value to its probability, rounded to 2 decimals.\nExample: {0: 0.12, 1: 0.88} \nAnswer:  {0: 0.48, 1: 0.52}\n</pre> <p>Unseed the generators, and set them to level 1</p> In\u00a0[15]: Copied! <pre>task_r1.config.set_seed(), task_r2.config.set_seed()\ntask_r1.config.set_level(1), task_r2.config.set_level(1)\n</pre> task_r1.config.set_seed(), task_r2.config.set_seed() task_r1.config.set_level(1), task_r2.config.set_level(1)  Out[15]: <pre>(Rung12Config(c=1.0, level=1, seed=None, size=None, n_nodes=4, max_domain_size=2, edge_prob=0.5, graph_generation_mode='erdos', n_round=1, cpt_relative_threshold=0.5, cot_scientific_notation=False, is_verbose=False, concise_cot=True),\n Rung12Config(c=1.0, level=1, seed=None, size=None, n_nodes=4, max_domain_size=2, edge_prob=0.5, graph_generation_mode='erdos', n_round=1, cpt_relative_threshold=0.5, cot_scientific_notation=False, is_verbose=False, concise_cot=True))</pre> <p>set there mode to verbose</p> In\u00a0[16]: Copied! <pre>task_r1.config.is_verbose, task_r2.config.is_verbose = True,True\ntask_r1.config.concise_cot, task_r2.config.concise_cot = False, False\n</pre> task_r1.config.is_verbose, task_r2.config.is_verbose = True,True task_r1.config.concise_cot, task_r2.config.concise_cot = False, False In\u00a0[17]: Copied! <pre>verbose_example_1 = task_r1.generate_example()\nprint(f\"Verbose rung1 example: \\n{verbose_example_1} \\n\")\n\nverbose_example_2 = task_r2.generate_example()\nprint(f\"Verbose rung2 example: \\n{verbose_example_2} \\n\")\n</pre> verbose_example_1 = task_r1.generate_example() print(f\"Verbose rung1 example: \\n{verbose_example_1} \\n\")  verbose_example_2 = task_r2.generate_example() print(f\"Verbose rung2 example: \\n{verbose_example_2} \\n\") <pre>Verbose rung1 example: \n---Prompt:System:\nThe probability of X_0 = 0 is 0.6 and The probability of X_0 = 1 is 0.4. \nIf X_0 = 0 and X_1 = 0, then The probability of X_2 = 0 is 0.8 and The probability of X_2 = 1 is 0.2. \nIf X_0 = 0 and X_1 = 1, then The probability of X_2 = 0 is 0.3 and The probability of X_2 = 1 is 0.7. \nIf X_0 = 1 and X_1 = 0, then The probability of X_2 = 0 is 0.1 and The probability of X_2 = 1 is 0.9. \nIf X_0 = 1 and X_1 = 1, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.6. \nIf X_0 = 0 and X_2 = 0, then The probability of X_3 = 0 is 0.5 and The probability of X_3 = 1 is 0.5. \nIf X_0 = 0 and X_2 = 1, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.6. \nIf X_0 = 1 and X_2 = 0, then The probability of X_3 = 0 is 0.5 and The probability of X_3 = 1 is 0.5. \nIf X_0 = 1 and X_2 = 1, then The probability of X_3 = 0 is 0.6 and The probability of X_3 = 1 is 0.4. \nThe probability of X_1 = 0 is 0.5 and The probability of X_1 = 1 is 0.5.\nObserved conditions:\nObserving/Knowing that the state X_0 is equal to 1, and the state X_3 is equal to 0, and the state X_1 is equal to 0\nTask: Compute probability distribution for X_2 (possible values: [0, 1]).\n\nOutput: Python dict mapping each value to its probability, rounded to 1 decimals.\nExample: {0: 0.1, 1: 0.9}\n---Answer:{0: 0.1, 1: 0.9}\n---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_0': [0, 1], 'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_0': [0, 1], 'X_2': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.6, 0.4 ;\\n}\\nprobability ( X_1 ) {\\n    table 0.5, 0.5 ;\\n}\\nprobability ( X_2 | X_0, X_1 ) {\\n    ( 0, 0 ) 0.8, 0.2;\\n    ( 0, 1 ) 0.3, 0.7;\\n    ( 1, 0 ) 0.1, 0.9;\\n    ( 1, 1 ) 0.4, 0.6;\\n\\n}\\nprobability ( X_3 | X_0, X_2 ) {\\n    ( 0, 0 ) 0.5, 0.5;\\n    ( 0, 1 ) 0.4, 0.6;\\n    ( 1, 0 ) 0.5, 0.5;\\n    ( 1, 1 ) 0.6, 0.4;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_0 is equal to 1, and the state X_3 is equal to 0, and the state X_1 is equal to 0', 'target': 'X_2', 'variables': ['X_0', 'X_2', 'X_3', 'X_1'], 'n_round': 1, 'cot': 'Initialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_3=0 | X_0=1, X_2):\\n     [X_2=0] = 0.5\\n     [X_2=1] = 0.6\\n   Table for P(X_2 | X_0=1, X_1=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.9\\n\\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=1, X_1=0, X_3=0)):\\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.5\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.9\\nInitialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_3=0 | X_0=1, X_2):\\n     [X_2=0] = 0.5\\n     [X_2=1] = 0.6\\n   Table for P(X_2 | X_0=1, X_1=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.9\\n\\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=1, X_1=0, X_3=0)):\\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.5\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\\n     [X_2=0] = 0.1\\n     [X_2=1] = 0.9', '_time': 1.287592887878418, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 537, '_cot_tokens': 557}\n---Task:bayesian_association\n \n\nVerbose rung2 example: \n---Prompt:System:\nThe probability of X_0 = 0 is 0.2 and The probability of X_0 = 1 is 0.2 and The probability of X_0 = 2 is 0.6. \nIf X_0 = 0 and X_1 = 0 and X_2 = 0, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.3 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 0 and X_1 = 0 and X_2 = 1, then The probability of X_3 = 0 is 0.3 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.3. \nIf X_0 = 0 and X_1 = 0 and X_2 = 2, then The probability of X_3 = 0 is 0.0 and The probability of X_3 = 1 is 0.5 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 0 and X_1 = 1 and X_2 = 0, then The probability of X_3 = 0 is 0.5 and The probability of X_3 = 1 is 0.3 and The probability of X_3 = 2 is 0.2. \nIf X_0 = 0 and X_1 = 1 and X_2 = 1, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 0 and X_1 = 1 and X_2 = 2, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.1 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 1 and X_1 = 0 and X_2 = 0, then The probability of X_3 = 0 is 0.3 and The probability of X_3 = 1 is 0.2 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 1 and X_1 = 0 and X_2 = 1, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 1 and X_1 = 0 and X_2 = 2, then The probability of X_3 = 0 is 0.3 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.3. \nIf X_0 = 1 and X_1 = 1 and X_2 = 0, then The probability of X_3 = 0 is 0.0 and The probability of X_3 = 1 is 0.5 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 1 and X_1 = 1 and X_2 = 1, then The probability of X_3 = 0 is 0.5 and The probability of X_3 = 1 is 0.2 and The probability of X_3 = 2 is 0.3. \nIf X_0 = 1 and X_1 = 1 and X_2 = 2, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.1 and The probability of X_3 = 2 is 0.5. \nIf X_0 = 2 and X_1 = 0 and X_2 = 0, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 2 and X_1 = 0 and X_2 = 1, then The probability of X_3 = 0 is 0.1 and The probability of X_3 = 1 is 0.3 and The probability of X_3 = 2 is 0.6. \nIf X_0 = 2 and X_1 = 0 and X_2 = 2, then The probability of X_3 = 0 is 0.1 and The probability of X_3 = 1 is 0.5 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 2 and X_1 = 1 and X_2 = 0, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.4 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 2 and X_1 = 1 and X_2 = 1, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.2 and The probability of X_3 = 2 is 0.4. \nIf X_0 = 2 and X_1 = 1 and X_2 = 2, then The probability of X_3 = 0 is 0.1 and The probability of X_3 = 1 is 0.1 and The probability of X_3 = 2 is 0.8. \nThe probability of X_1 = 0 is 0.2 and The probability of X_1 = 1 is 0.8. \nIf X_1 = 0, then The probability of X_2 = 0 is 0.2 and The probability of X_2 = 1 is 0.6 and The probability of X_2 = 2 is 0.2. \nIf X_1 = 1, then The probability of X_2 = 0 is 0.5 and The probability of X_2 = 1 is 0.1 and The probability of X_2 = 2 is 0.4.\nObserved conditions:\nDoing/Imposing that the state X_3 is equal to 0. Observing/Knowing that the state X_1 is equal to 0, and the state X_0 is equal to 1\nTask: Compute probability distribution for X_2 (possible values: [0, 1, 2]).\n\nOutput: Python dict mapping each value to its probability, rounded to 1 decimals.\nExample: {0: 0.1, 1: 0.9}\n---Answer:{0: 0.2, 1: 0.6, 2: 0.2}\n---Metadata:{'target_var_values': [0, 1, 2], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1, 2], 'X_0': [0, 1, 2], 'X_1': [0, 1], 'X_2': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1, 2], 'X_1': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.2, 0.2, 0.6 ;\\n}\\nprobability ( X_1 ) {\\n    table 0.2, 0.8 ;\\n}\\nprobability ( X_2 | X_1 ) {\\n    ( 0 ) 0.2, 0.6, 0.2;\\n    ( 1 ) 0.5, 0.1, 0.4;\\n\\n}\\nprobability ( X_3 | X_0, X_1, X_2 ) {\\n    ( 0, 0, 0 ) 0.2, 0.3, 0.5;\\n    ( 0, 0, 1 ) 0.3, 0.4, 0.3;\\n    ( 0, 0, 2 ) 0.0, 0.5, 0.5;\\n    ( 0, 1, 0 ) 0.5, 0.3, 0.2;\\n    ( 0, 1, 1 ) 0.2, 0.4, 0.4;\\n    ( 0, 1, 2 ) 0.4, 0.1, 0.5;\\n    ( 1, 0, 0 ) 0.3, 0.2, 0.5;\\n    ( 1, 0, 1 ) 0.2, 0.4, 0.4;\\n    ( 1, 0, 2 ) 0.3, 0.4, 0.3;\\n    ( 1, 1, 0 ) 0.0, 0.5, 0.5;\\n    ( 1, 1, 1 ) 0.5, 0.2, 0.3;\\n    ( 1, 1, 2 ) 0.4, 0.1, 0.5;\\n    ( 2, 0, 0 ) 0.2, 0.4, 0.4;\\n    ( 2, 0, 1 ) 0.1, 0.3, 0.6;\\n    ( 2, 0, 2 ) 0.1, 0.5, 0.4;\\n    ( 2, 1, 0 ) 0.2, 0.4, 0.4;\\n    ( 2, 1, 1 ) 0.4, 0.2, 0.4;\\n    ( 2, 1, 2 ) 0.1, 0.1, 0.8;\\n\\n}\\n\", 'scenario': 'Doing/Imposing that the state X_3 is equal to 0. Observing/Knowing that the state X_1 is equal to 0, and the state X_0 is equal to 1', 'target': 'X_2', 'variables': ['X_0', 'X_3', 'X_1', 'X_2'], 'n_round': 1, 'cot': \"Goal: Compute Causal Effect: P(X_2 | do(X_3=0), X_1=0, X_0=1)\\n\\n--- Causal Graph Surgery ---\\n  - Cut incoming edges to intervened node 'X_3': ['X_0', 'X_1', 'X_2'] -&gt; X_3\\n  - Set P(X_3) := Point Mass at X_3=0.\\n\\nInitialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_2 | X_1=0):\\n     [X_2=0] = 0.2\\n     [X_2=1] = 0.6\\n     [X_2=2] = 0.2\\n\\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_1=0)):\\n   Table for P(X_2 | X_1=0):\\n     [X_2=0] = 0.2\\n     [X_2=1] = 0.6\\n     [X_2=2] = 0.2\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_2 | X_1=0):\\n     [X_2=0] = 0.2\\n     [X_2=1] = 0.6\\n     [X_2=2] = 0.2\", '_time': 1.2845921516418457, '_task': 'bayesian_intervention', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 1535, '_cot_tokens': 335}\n---Task:bayesian_intervention\n \n\n</pre> In\u00a0[18]: Copied! <pre>print(f\"Verbose rung1 example: \\n{verbose_example_1.metadata.cot} \\n\")\n\nprint(f\"Verbose rung2 example: \\n{verbose_example_2.metadata.cot} \\n\")\n</pre> print(f\"Verbose rung1 example: \\n{verbose_example_1.metadata.cot} \\n\")  print(f\"Verbose rung2 example: \\n{verbose_example_2.metadata.cot} \\n\") <pre>Verbose rung1 example: \nInitialization: Selected Elimination Order = []\n\n--- Final Step: Normalization ---\n1. Gather all remaining factors (Query Variables + Priors):\n   Table for P(X_3=0 | X_0=1, X_2):\n     [X_2=0] = 0.5\n     [X_2=1] = 0.6\n   Table for P(X_2 | X_0=1, X_1=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.9\n\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=1, X_1=0, X_3=0)):\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.5\n\n3. Normalize to obtain Probability Distribution:\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.9\nInitialization: Selected Elimination Order = []\n\n--- Final Step: Normalization ---\n1. Gather all remaining factors (Query Variables + Priors):\n   Table for P(X_3=0 | X_0=1, X_2):\n     [X_2=0] = 0.5\n     [X_2=1] = 0.6\n   Table for P(X_2 | X_0=1, X_1=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.9\n\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=1, X_1=0, X_3=0)):\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.5\n\n3. Normalize to obtain Probability Distribution:\n   Table for P(X_2 | X_0=1, X_1=0, X_3=0):\n     [X_2=0] = 0.1\n     [X_2=1] = 0.9 \n\nVerbose rung2 example: \nGoal: Compute Causal Effect: P(X_2 | do(X_3=0), X_1=0, X_0=1)\n\n--- Causal Graph Surgery ---\n  - Cut incoming edges to intervened node 'X_3': ['X_0', 'X_1', 'X_2'] -&gt; X_3\n  - Set P(X_3) := Point Mass at X_3=0.\n\nInitialization: Selected Elimination Order = []\n\n--- Final Step: Normalization ---\n1. Gather all remaining factors (Query Variables + Priors):\n   Table for P(X_2 | X_1=0):\n     [X_2=0] = 0.2\n     [X_2=1] = 0.6\n     [X_2=2] = 0.2\n\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_1=0)):\n   Table for P(X_2 | X_1=0):\n     [X_2=0] = 0.2\n     [X_2=1] = 0.6\n     [X_2=2] = 0.2\n\n3. Normalize to obtain Probability Distribution:\n   Table for P(X_2 | X_1=0):\n     [X_2=0] = 0.2\n     [X_2=1] = 0.6\n     [X_2=2] = 0.2 \n\n</pre> In\u00a0[19]: Copied! <pre>task_r1.generate_balanced_batch(batch_size=5)\n</pre> task_r1.generate_balanced_batch(batch_size=5) Out[19]: <pre>[---Prompt:System:\n The probability of X_0 = 0 is 0.3 and The probability of X_0 = 1 is 0.7. \n If X_0 = 0 and X_1 = 0, then The probability of X_2 = 0 is 0.3 and The probability of X_2 = 1 is 0.7. \n If X_0 = 0 and X_1 = 1, then The probability of X_2 = 0 is 0.5 and The probability of X_2 = 1 is 0.5. \n If X_0 = 1 and X_1 = 0, then The probability of X_2 = 0 is 0.3 and The probability of X_2 = 1 is 0.7. \n If X_0 = 1 and X_1 = 1, then The probability of X_2 = 0 is 0.1 and The probability of X_2 = 1 is 0.9. \n If X_0 = 0, then The probability of X_3 = 0 is 0.5 and The probability of X_3 = 1 is 0.5. \n If X_0 = 1, then The probability of X_3 = 0 is 0.2 and The probability of X_3 = 1 is 0.8. \n The probability of X_1 = 0 is 0.6 and The probability of X_1 = 1 is 0.4.\n Observed conditions:\n Observing/Knowing that the state X_3 is equal to 0, and the state X_0 is equal to 0\n Task: Compute probability distribution for X_2 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.4, 1: 0.6}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_0': [0, 1], 'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.3, 0.7 ;\\n}\\nprobability ( X_1 ) {\\n    table 0.6, 0.4 ;\\n}\\nprobability ( X_2 | X_0, X_1 ) {\\n    ( 0, 0 ) 0.3, 0.7;\\n    ( 0, 1 ) 0.5, 0.5;\\n    ( 1, 0 ) 0.3, 0.7;\\n    ( 1, 1 ) 0.1, 0.9;\\n\\n}\\nprobability ( X_3 | X_0 ) {\\n    ( 0 ) 0.5, 0.5;\\n    ( 1 ) 0.2, 0.8;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_3 is equal to 0, and the state X_0 is equal to 0', 'target': 'X_2', 'variables': ['X_0', 'X_2', 'X_3', 'X_1'], 'n_round': 1, 'cot': \"Initialization: Selected Elimination Order = ['X_1']\\n\\n--- Step: Eliminate Variable 'X_1' ---\\n1. Retrieve relevant factors containing 'X_1':\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.4\\n   Table for P(X_2 | X_0=0, X_1):\\n     [X_2=0, X_1=0] = 0.3\\n     [X_2=0, X_1=1] = 0.5\\n     [X_2=1, X_1=0] = 0.7\\n     [X_2=1, X_1=1] = 0.5\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_1, X_2 | X_0=0) = P(X_1) * P(X_2 | X_0=0, X_1)\\n   Table for P(X_1, X_2 | X_0=0):\\n     [X_1=0, X_2=0] = 0.2\\n     [X_1=0, X_2=1] = 0.4\\n     [X_1=1, X_2=0] = 0.2\\n     [X_1=1, X_2=1] = 0.2\\n\\n3. Marginalize (Sum) out variable 'X_1':\\n   Formula: P(X_2 | X_0=0) = \u2211_{X_1} P(X_1, X_2 | X_0=0)\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=0)):\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\nInitialization: Selected Elimination Order = ['X_1']\\n\\n--- Step: Eliminate Variable 'X_1' ---\\n1. Retrieve relevant factors containing 'X_1':\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.4\\n   Table for P(X_2 | X_0=0, X_1):\\n     [X_2=0, X_1=0] = 0.3\\n     [X_2=0, X_1=1] = 0.5\\n     [X_2=1, X_1=0] = 0.7\\n     [X_2=1, X_1=1] = 0.5\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_1, X_2 | X_0=0) = P(X_1) * P(X_2 | X_0=0, X_1)\\n   Table for P(X_1, X_2 | X_0=0):\\n     [X_1=0, X_2=0] = 0.2\\n     [X_1=0, X_2=1] = 0.4\\n     [X_1=1, X_2=0] = 0.2\\n     [X_1=1, X_2=1] = 0.2\\n\\n3. Marginalize (Sum) out variable 'X_1':\\n   Formula: P(X_2 | X_0=0) = \u2211_{X_1} P(X_1, X_2 | X_0=0)\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n2. Compute Unnormalized Joint Distribution (P(X_2 | X_0=0)):\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_2 | X_0=0):\\n     [X_2=0] = 0.4\\n     [X_2=1] = 0.6\", '_time': 1.3111770153045654, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 417, '_cot_tokens': 1181}\n ---Task:bayesian_association,\n ---Prompt:System:\n The probability of X_0 = 0 is 0.15 and The probability of X_0 = 1 is 0.49 and The probability of X_0 = 2 is 0.36. \n If X_0 = 0 and X_1 = 0, then The probability of X_2 = 0 is 0.45 and The probability of X_2 = 1 is 0.55. \n If X_0 = 0 and X_1 = 1, then The probability of X_2 = 0 is 0.71 and The probability of X_2 = 1 is 0.29. \n If X_0 = 0 and X_1 = 2, then The probability of X_2 = 0 is 0.44 and The probability of X_2 = 1 is 0.56. \n If X_0 = 1 and X_1 = 0, then The probability of X_2 = 0 is 0.27 and The probability of X_2 = 1 is 0.73. \n If X_0 = 1 and X_1 = 1, then The probability of X_2 = 0 is 0.45 and The probability of X_2 = 1 is 0.55. \n If X_0 = 1 and X_1 = 2, then The probability of X_2 = 0 is 0.42 and The probability of X_2 = 1 is 0.58. \n If X_0 = 2 and X_1 = 0, then The probability of X_2 = 0 is 0.31 and The probability of X_2 = 1 is 0.69. \n If X_0 = 2 and X_1 = 1, then The probability of X_2 = 0 is 0.38 and The probability of X_2 = 1 is 0.62. \n If X_0 = 2 and X_1 = 2, then The probability of X_2 = 0 is 0.63 and The probability of X_2 = 1 is 0.37. \n The probability of X_1 = 0 is 0.33 and The probability of X_1 = 1 is 0.3 and The probability of X_1 = 2 is 0.37.\n Observed conditions:\n Observing/Knowing that the state X_2 is equal to 1\n Task: Compute probability distribution for X_0 (possible values: [0, 1, 2]).\n \n Output: Python dict mapping each value to its probability, rounded to 2 decimals.\n Example: {0: 0.12, 1: 0.88}\n ---Answer:{0: 0.12, 1: 0.53, 2: 0.35}\n ---Metadata:{'target_var_values': [0, 1, 2], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_0': [0, 1, 2], 'X_1': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1, 2]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.15, 0.49, 0.36 ;\\n}\\nprobability ( X_1 ) {\\n    table 0.33, 0.3, 0.37 ;\\n}\\nprobability ( X_2 | X_0, X_1 ) {\\n    ( 0, 0 ) 0.45, 0.55;\\n    ( 0, 1 ) 0.71, 0.29;\\n    ( 0, 2 ) 0.44, 0.56;\\n    ( 1, 0 ) 0.27, 0.73;\\n    ( 1, 1 ) 0.45, 0.55;\\n    ( 1, 2 ) 0.42, 0.58;\\n    ( 2, 0 ) 0.31, 0.69;\\n    ( 2, 1 ) 0.38, 0.62;\\n    ( 2, 2 ) 0.63, 0.37;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_2 is equal to 1', 'target': 'X_0', 'variables': ['X_0', 'X_2', 'X_1'], 'n_round': 2, 'cot': \"Initialization: Selected Elimination Order = ['X_1']\\n\\n--- Step: Eliminate Variable 'X_1' ---\\n1. Retrieve relevant factors containing 'X_1':\\n   Table for P(X_2=1 | X_0, X_1):\\n     [X_0=0, X_1=0] = 0.55\\n     [X_0=0, X_1=1] = 0.29\\n     [X_0=0, X_1=2] = 0.56\\n     [X_0=1, X_1=0] = 0.73\\n     [X_0=1, X_1=1] = 0.55\\n     [X_0=1, X_1=2] = 0.58\\n     [X_0=2, X_1=0] = 0.69\\n     [X_0=2, X_1=1] = 0.62\\n     [X_0=2, X_1=2] = 0.37\\n   Table for P(X_1):\\n     [X_1=0] = 0.33\\n     [X_1=1] = 0.3\\n     [X_1=2] = 0.37\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_1, X_2=1 | X_0) = P(X_2=1 | X_0, X_1) * P(X_1)\\n   Table for P(X_1, X_2=1 | X_0):\\n     [X_1=0, X_0=0] = 0.18\\n     [X_1=0, X_0=1] = 0.24\\n     [X_1=0, X_0=2] = 0.23\\n     [X_1=1, X_0=0] = 0.09\\n     [X_1=1, X_0=1] = 0.17\\n     [X_1=1, X_0=2] = 0.19\\n     [X_1=2, X_0=0] = 0.21\\n     [X_1=2, X_0=1] = 0.21\\n     [X_1=2, X_0=2] = 0.14\\n\\n3. Marginalize (Sum) out variable 'X_1':\\n   Formula: P(X_2=1 | X_0) = \u2211_{X_1} P(X_1, X_2=1 | X_0)\\n   Table for P(X_2=1 | X_0):\\n     [X_0=0] = 0.48\\n     [X_0=1] = 0.62\\n     [X_0=2] = 0.55\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_2=1 | X_0):\\n     [X_0=0] = 0.48\\n     [X_0=1] = 0.62\\n     [X_0=2] = 0.55\\n   Table for P(X_0):\\n     [X_0=0] = 0.15\\n     [X_0=1] = 0.49\\n     [X_0=2] = 0.36\\n\\n2. Compute Unnormalized Joint Distribution (P(X_0 | X_2=1)):\\n   Table for P(X_0 | X_2=1):\\n     [X_0=0] = 0.07\\n     [X_0=1] = 0.3\\n     [X_0=2] = 0.2\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_0 | X_2=1):\\n     [X_0=0] = 0.12\\n     [X_0=1] = 0.53\\n     [X_0=2] = 0.35\\nInitialization: Selected Elimination Order = ['X_1']\\n\\n--- Step: Eliminate Variable 'X_1' ---\\n1. Retrieve relevant factors containing 'X_1':\\n   Table for P(X_2=1 | X_0, X_1):\\n     [X_0=0, X_1=0] = 0.55\\n     [X_0=0, X_1=1] = 0.29\\n     [X_0=0, X_1=2] = 0.56\\n     [X_0=1, X_1=0] = 0.73\\n     [X_0=1, X_1=1] = 0.55\\n     [X_0=1, X_1=2] = 0.58\\n     [X_0=2, X_1=0] = 0.69\\n     [X_0=2, X_1=1] = 0.62\\n     [X_0=2, X_1=2] = 0.37\\n   Table for P(X_1):\\n     [X_1=0] = 0.33\\n     [X_1=1] = 0.3\\n     [X_1=2] = 0.37\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_1, X_2=1 | X_0) = P(X_2=1 | X_0, X_1) * P(X_1)\\n   Table for P(X_1, X_2=1 | X_0):\\n     [X_1=0, X_0=0] = 0.18\\n     [X_1=0, X_0=1] = 0.24\\n     [X_1=0, X_0=2] = 0.23\\n     [X_1=1, X_0=0] = 0.09\\n     [X_1=1, X_0=1] = 0.17\\n     [X_1=1, X_0=2] = 0.19\\n     [X_1=2, X_0=0] = 0.21\\n     [X_1=2, X_0=1] = 0.21\\n     [X_1=2, X_0=2] = 0.14\\n\\n3. Marginalize (Sum) out variable 'X_1':\\n   Formula: P(X_2=1 | X_0) = \u2211_{X_1} P(X_1, X_2=1 | X_0)\\n   Table for P(X_2=1 | X_0):\\n     [X_0=0] = 0.48\\n     [X_0=1] = 0.62\\n     [X_0=2] = 0.55\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_2=1 | X_0):\\n     [X_0=0] = 0.48\\n     [X_0=1] = 0.62\\n     [X_0=2] = 0.55\\n   Table for P(X_0):\\n     [X_0=0] = 0.15\\n     [X_0=1] = 0.49\\n     [X_0=2] = 0.36\\n\\n2. Compute Unnormalized Joint Distribution (P(X_0 | X_2=1)):\\n   Table for P(X_0 | X_2=1):\\n     [X_0=0] = 0.07\\n     [X_0=1] = 0.3\\n     [X_0=2] = 0.2\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_0 | X_2=1):\\n     [X_0=0] = 0.12\\n     [X_0=1] = 0.53\\n     [X_0=2] = 0.35\", '_time': 1.2288808822631836, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 593, '_cot_tokens': 1829}\n ---Task:bayesian_association,\n ---Prompt:System:\n The probability of X_0 = 0 is 0.6 and The probability of X_0 = 1 is 0.3 and The probability of X_0 = 2 is 0.1. \n If X_0 = 0, then The probability of X_1 = 0 is 0.2 and The probability of X_1 = 1 is 0.1 and The probability of X_1 = 2 is 0.7. \n If X_0 = 1, then The probability of X_1 = 0 is 0.4 and The probability of X_1 = 1 is 0.3 and The probability of X_1 = 2 is 0.3. \n If X_0 = 2, then The probability of X_1 = 0 is 0.4 and The probability of X_1 = 1 is 0.3 and The probability of X_1 = 2 is 0.3. \n If X_0 = 0 and X_1 = 0, then The probability of X_2 = 0 is 0.2 and The probability of X_2 = 1 is 0.5 and The probability of X_2 = 2 is 0.3. \n If X_0 = 0 and X_1 = 1, then The probability of X_2 = 0 is 0.7 and The probability of X_2 = 1 is 0.2 and The probability of X_2 = 2 is 0.1. \n If X_0 = 0 and X_1 = 2, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.5 and The probability of X_2 = 2 is 0.1. \n If X_0 = 1 and X_1 = 0, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.1 and The probability of X_2 = 2 is 0.5. \n If X_0 = 1 and X_1 = 1, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.1 and The probability of X_2 = 2 is 0.5. \n If X_0 = 1 and X_1 = 2, then The probability of X_2 = 0 is 0.2 and The probability of X_2 = 1 is 0.2 and The probability of X_2 = 2 is 0.6. \n If X_0 = 2 and X_1 = 0, then The probability of X_2 = 0 is 0.5 and The probability of X_2 = 1 is 0.4 and The probability of X_2 = 2 is 0.1. \n If X_0 = 2 and X_1 = 1, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.1 and The probability of X_2 = 2 is 0.5. \n If X_0 = 2 and X_1 = 2, then The probability of X_2 = 0 is 0.5 and The probability of X_2 = 1 is 0.2 and The probability of X_2 = 2 is 0.3. \n The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.6.\n Observed conditions:\n Observing/Knowing that the state X_1 is equal to 1\n Task: Compute probability distribution for X_3 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.4, 1: 0.6}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1, 2], 'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1, 2], 'X_0': [0, 1, 2], 'X_1': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.6, 0.3, 0.1 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.2, 0.1, 0.7;\\n    ( 1 ) 0.4, 0.3, 0.3;\\n    ( 2 ) 0.4, 0.3, 0.3;\\n\\n}\\nprobability ( X_2 | X_0, X_1 ) {\\n    ( 0, 0 ) 0.2, 0.5, 0.3;\\n    ( 0, 1 ) 0.7, 0.2, 0.1;\\n    ( 0, 2 ) 0.4, 0.5, 0.1;\\n    ( 1, 0 ) 0.4, 0.1, 0.5;\\n    ( 1, 1 ) 0.4, 0.1, 0.5;\\n    ( 1, 2 ) 0.2, 0.2, 0.6;\\n    ( 2, 0 ) 0.5, 0.4, 0.1;\\n    ( 2, 1 ) 0.4, 0.1, 0.5;\\n    ( 2, 2 ) 0.5, 0.2, 0.3;\\n\\n}\\nprobability ( X_3 ) {\\n    table 0.4, 0.6 ;\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_1 is equal to 1', 'target': 'X_3', 'variables': ['X_0', 'X_1', 'X_2', 'X_3'], 'n_round': 1, 'cot': 'Initialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6\\n\\n2. Compute Unnormalized Joint Distribution (P(X_3)):\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6\\nInitialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6\\n\\n2. Compute Unnormalized Joint Distribution (P(X_3)):\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_3):\\n     [X_3=0] = 0.4\\n     [X_3=1] = 0.6', '_time': 1.30381178855896, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 875, '_cot_tokens': 329}\n ---Task:bayesian_association,\n ---Prompt:System:\n The probability of X_0 = 0 is 0.5 and The probability of X_0 = 1 is 0.4 and The probability of X_0 = 2 is 0.1. \n If X_0 = 0, then The probability of X_1 = 0 is 0.4 and The probability of X_1 = 1 is 0.4 and The probability of X_1 = 2 is 0.2. \n If X_0 = 1, then The probability of X_1 = 0 is 0.8 and The probability of X_1 = 1 is 0.1 and The probability of X_1 = 2 is 0.1. \n If X_0 = 2, then The probability of X_1 = 0 is 0.7 and The probability of X_1 = 1 is 0.1 and The probability of X_1 = 2 is 0.2. \n If X_0 = 0, then The probability of X_2 = 0 is 0.8 and The probability of X_2 = 1 is 0.2. \n If X_0 = 1, then The probability of X_2 = 0 is 0.4 and The probability of X_2 = 1 is 0.6. \n If X_0 = 2, then The probability of X_2 = 0 is 0.7 and The probability of X_2 = 1 is 0.3. \n If X_2 = 0, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.6. \n If X_2 = 1, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.6.\n Observed conditions:\n Without further Observation/Knowledge of other variable.\n Task: Compute probability distribution for X_1 (possible values: [0, 1, 2]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.6, 1: 0.2, 2: 0.2}\n ---Metadata:{'target_var_values': [0, 1, 2], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1, 2], 'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_0': [0, 1, 2]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.5, 0.4, 0.1 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.4, 0.4, 0.2;\\n    ( 1 ) 0.8, 0.1, 0.1;\\n    ( 2 ) 0.7, 0.1, 0.2;\\n\\n}\\nprobability ( X_2 | X_0 ) {\\n    ( 0 ) 0.8, 0.2;\\n    ( 1 ) 0.4, 0.6;\\n    ( 2 ) 0.7, 0.3;\\n\\n}\\nprobability ( X_3 | X_2 ) {\\n    ( 0 ) 0.4, 0.6;\\n    ( 1 ) 0.4, 0.6;\\n\\n}\\n\", 'scenario': 'Without further Observation/Knowledge of other variable.', 'target': 'X_1', 'variables': ['X_0', 'X_1', 'X_2', 'X_3'], 'n_round': 1, 'cot': \"Initialization: Selected Elimination Order = ['X_0']\\n\\n--- Step: Eliminate Variable 'X_0' ---\\n1. Retrieve relevant factors containing 'X_0':\\n   Table for P(X_0):\\n     [X_0=0] = 0.5\\n     [X_0=1] = 0.4\\n     [X_0=2] = 0.1\\n   Table for P(X_1 | X_0):\\n     [X_1=0, X_0=0] = 0.4\\n     [X_1=0, X_0=1] = 0.8\\n     [X_1=0, X_0=2] = 0.7\\n     [X_1=1, X_0=0] = 0.4\\n     [X_1=1, X_0=1] = 0.1\\n     [X_1=1, X_0=2] = 0.1\\n     [X_1=2, X_0=0] = 0.2\\n     [X_1=2, X_0=1] = 0.1\\n     [X_1=2, X_0=2] = 0.2\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_0, X_1) = P(X_0) * P(X_1 | X_0)\\n   Table for P(X_0, X_1):\\n     [X_1=0, X_0=0] = 0.2\\n     [X_1=0, X_0=1] = 0.3\\n     [X_1=0, X_0=2] = 0.1\\n     [X_1=1, X_0=0] = 0.2\\n     [X_1=1, X_0=1] = 0\\n     [X_1=1, X_0=2] = 0\\n     [X_1=2, X_0=0] = 0.1\\n     [X_1=2, X_0=1] = 0\\n     [X_1=2, X_0=2] = 0\\n\\n3. Marginalize (Sum) out variable 'X_0':\\n   Formula: P(X_1) = \u2211_{X_0} P(X_0, X_1)\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n2. Compute Unnormalized Joint Distribution (P(X_1)):\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\nInitialization: Selected Elimination Order = ['X_0']\\n\\n--- Step: Eliminate Variable 'X_0' ---\\n1. Retrieve relevant factors containing 'X_0':\\n   Table for P(X_0):\\n     [X_0=0] = 0.5\\n     [X_0=1] = 0.4\\n     [X_0=2] = 0.1\\n   Table for P(X_1 | X_0):\\n     [X_1=0, X_0=0] = 0.4\\n     [X_1=0, X_0=1] = 0.8\\n     [X_1=0, X_0=2] = 0.7\\n     [X_1=1, X_0=0] = 0.4\\n     [X_1=1, X_0=1] = 0.1\\n     [X_1=1, X_0=2] = 0.1\\n     [X_1=2, X_0=0] = 0.2\\n     [X_1=2, X_0=1] = 0.1\\n     [X_1=2, X_0=2] = 0.2\\n\\n2. Compute the Intermediate Joint (Product):\\n   Formula: P(X_0, X_1) = P(X_0) * P(X_1 | X_0)\\n   Table for P(X_0, X_1):\\n     [X_1=0, X_0=0] = 0.2\\n     [X_1=0, X_0=1] = 0.3\\n     [X_1=0, X_0=2] = 0.1\\n     [X_1=1, X_0=0] = 0.2\\n     [X_1=1, X_0=1] = 0\\n     [X_1=1, X_0=2] = 0\\n     [X_1=2, X_0=0] = 0.1\\n     [X_1=2, X_0=1] = 0\\n     [X_1=2, X_0=2] = 0\\n\\n3. Marginalize (Sum) out variable 'X_0':\\n   Formula: P(X_1) = \u2211_{X_0} P(X_0, X_1)\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n2. Compute Unnormalized Joint Distribution (P(X_1)):\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_1):\\n     [X_1=0] = 0.6\\n     [X_1=1] = 0.2\\n     [X_1=2] = 0.2\", '_time': 1.2433500289916992, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 482, '_cot_tokens': 1581}\n ---Task:bayesian_association,\n ---Prompt:System:\n The probability of X_0 = 0 is 0.2 and The probability of X_0 = 1 is 0.8. \n If X_0 = 0, then The probability of X_1 = 0 is 0.5 and The probability of X_1 = 1 is 0.5. \n If X_0 = 1, then The probability of X_1 = 0 is 0.6 and The probability of X_1 = 1 is 0.4. \n The probability of X_2 = 0 is 0.7 and The probability of X_2 = 1 is 0.3. \n If X_2 = 0, then The probability of X_3 = 0 is 0.6 and The probability of X_3 = 1 is 0.4. \n If X_2 = 1, then The probability of X_3 = 0 is 0.4 and The probability of X_3 = 1 is 0.6.\n Observed conditions:\n Observing/Knowing that the state X_3 is equal to 0, and the state X_2 is equal to 1, and the state X_1 is equal to 0\n Task: Compute probability distribution for X_0 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.2, 1: 0.8}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.2, 0.8 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.5, 0.5;\\n    ( 1 ) 0.6, 0.4;\\n\\n}\\nprobability ( X_2 ) {\\n    table 0.7, 0.3 ;\\n}\\nprobability ( X_3 | X_2 ) {\\n    ( 0 ) 0.6, 0.4;\\n    ( 1 ) 0.4, 0.6;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_3 is equal to 0, and the state X_2 is equal to 1, and the state X_1 is equal to 0', 'target': 'X_0', 'variables': ['X_0', 'X_1', 'X_2', 'X_3'], 'n_round': 1, 'cot': 'Initialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_1=0 | X_0):\\n     [X_0=0] = 0.5\\n     [X_0=1] = 0.6\\n   Table for P(X_0):\\n     [X_0=0] = 0.2\\n     [X_0=1] = 0.8\\n\\n2. Compute Unnormalized Joint Distribution (P(X_0 | X_1=0)):\\n   Table for P(X_0 | X_1=0):\\n     [X_0=0] = 0.1\\n     [X_0=1] = 0.5\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_0 | X_1=0):\\n     [X_0=0] = 0.2\\n     [X_0=1] = 0.8\\nInitialization: Selected Elimination Order = []\\n\\n--- Final Step: Normalization ---\\n1. Gather all remaining factors (Query Variables + Priors):\\n   Table for P(X_1=0 | X_0):\\n     [X_0=0] = 0.5\\n     [X_0=1] = 0.6\\n   Table for P(X_0):\\n     [X_0=0] = 0.2\\n     [X_0=1] = 0.8\\n\\n2. Compute Unnormalized Joint Distribution (P(X_0 | X_1=0)):\\n   Table for P(X_0 | X_1=0):\\n     [X_0=0] = 0.1\\n     [X_0=1] = 0.5\\n\\n3. Normalize to obtain Probability Distribution:\\n   Table for P(X_0 | X_1=0):\\n     [X_0=0] = 0.2\\n     [X_0=1] = 0.8', '_time': 1.2841696739196777, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': True, 'concise_cot': False}, '_prompt_tokens': 321, '_cot_tokens': 449}\n ---Task:bayesian_association]</pre> <p>Generate with a precise level, whatever the current config of the generator</p> In\u00a0[20]: Copied! <pre>task_r1.generate_balanced_batch(batch_size=5, level=1)\n</pre> task_r1.generate_balanced_batch(batch_size=5, level=1) Out[20]: <pre>[---Prompt:System:\n P(X_0) = {'0': 0.52, '1': 0.48} \n P(X_1|X_0=0) = {'0': 0.8, '1': 0.2} \n P(X_1|X_0=1) = {'0': 0.57, '1': 0.43} \n P(X_2|X_1=0) = {'0': 0.59, '1': 0.41} \n P(X_2|X_1=1) = {'0': 0.36, '1': 0.64}\n Observed conditions:\n Observing/Knowing that the state X_0 is equal to 1\n Task: Compute probability distribution for X_2 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 2 decimals.\n Example: {0: 0.12, 1: 0.88}\n ---Answer:{0: 0.49, 1: 0.51}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_1': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.52, 0.48 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.8, 0.2;\\n    ( 1 ) 0.57, 0.43;\\n\\n}\\nprobability ( X_2 | X_1 ) {\\n    ( 0 ) 0.59, 0.41;\\n    ( 1 ) 0.36, 0.64;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_0 is equal to 1', 'target': 'X_2', 'variables': ['X_0', 'X_1', 'X_2'], 'n_round': 2, 'cot': \"Elim order: ['X_1']\\nSum out X_1 -&gt; P(X_2 | X_0=1) = {0: 0.49, 1: 0.51}\\nResult: P(X_2 | X_0=1) = {0: 0.49, 1: 0.51}\\nElim order: ['X_1']\\nSum out X_1 -&gt; P(X_2 | X_0=1) = {0: 0.49, 1: 0.51}\\nResult: P(X_2 | X_0=1) = {0: 0.49, 1: 0.51}\", '_time': 1.262965202331543, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 214, '_cot_tokens': 161}\n ---Task:bayesian_association,\n ---Prompt:System:\n P(X_0) = {'0': 0.45, '1': 0.55} \n P(X_1|X_0=0) = {'0': 0.39, '1': 0.35, '2': 0.26} \n P(X_1|X_0=1) = {'0': 0.23, '1': 0.14, '2': 0.63} \n P(X_3|X_0=0) = {'0': 0.02, '1': 0.65, '2': 0.33} \n P(X_3|X_0=1) = {'0': 0.33, '1': 0.4, '2': 0.27} \n P(X_2) = {'0': 0.53, '1': 0.47}\n Observed conditions:\n Observing/Knowing that the state X_0 is equal to 1\n Task: Compute probability distribution for X_2 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 2 decimals.\n Example: {0: 0.12, 1: 0.88}\n ---Answer:{0: 0.53, 1: 0.47}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1, 2], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1, 2], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 3 ] { 0, 1, 2 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.45, 0.55 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.39, 0.35, 0.26;\\n    ( 1 ) 0.23, 0.14, 0.63;\\n\\n}\\nprobability ( X_2 ) {\\n    table 0.53, 0.47 ;\\n}\\nprobability ( X_3 | X_0 ) {\\n    ( 0 ) 0.02, 0.65, 0.33;\\n    ( 1 ) 0.33, 0.4, 0.27;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_0 is equal to 1', 'target': 'X_2', 'variables': ['X_0', 'X_1', 'X_3', 'X_2'], 'n_round': 2, 'cot': 'Result: P(X_2) = {0: 0.53, 1: 0.47}\\nResult: P(X_2) = {0: 0.53, 1: 0.47}', '_time': 1.3029215335845947, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 269, '_cot_tokens': 63}\n ---Task:bayesian_association,\n ---Prompt:System:\n P(X_0) = {'0': 0.6, '1': 0.4} \n P(X_1|X_0=0) = {'0': 0.8, '1': 0.2} \n P(X_1|X_0=1) = {'0': 0.5, '1': 0.5} \n P(X_2) = {'0': 0.4, '1': 0.6} \n P(X_3|X_2=0) = {'0': 0.7, '1': 0.3} \n P(X_3|X_2=1) = {'0': 0.6, '1': 0.4}\n Observed conditions:\n Observing/Knowing that the state X_3 is equal to 0, and the state X_1 is equal to 0, and the state X_2 is equal to 1\n Task: Compute probability distribution for X_0 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.7, 1: 0.3}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.6, 0.4 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.8, 0.2;\\n    ( 1 ) 0.5, 0.5;\\n\\n}\\nprobability ( X_2 ) {\\n    table 0.4, 0.6 ;\\n}\\nprobability ( X_3 | X_2 ) {\\n    ( 0 ) 0.7, 0.3;\\n    ( 1 ) 0.6, 0.4;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_3 is equal to 0, and the state X_1 is equal to 0, and the state X_2 is equal to 1', 'target': 'X_0', 'variables': ['X_0', 'X_1', 'X_2', 'X_3'], 'n_round': 1, 'cot': 'Normalize (sum=0.7) -&gt; P(X_0 | X_1=0) = {0: 0.7, 1: 0.3}\\nNormalize (sum=0.7) -&gt; P(X_0 | X_1=0) = {0: 0.7, 1: 0.3}', '_time': 1.26723051071167, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 3, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 261, '_cot_tokens': 89}\n ---Task:bayesian_association,\n ---Prompt:System:\n P(X_1) = {'0': 0.31, '1': 0.69} \n P(X_2|X_1=0) = {'0': 0.5, '1': 0.5} \n P(X_2|X_1=1) = {'0': 0.55, '1': 0.45} \n P(X_0) = {'0': 0.32, '1': 0.68}\n Observed conditions:\n Observing/Knowing that the state X_1 is equal to 0, and the state X_0 is equal to 1\n Task: Compute probability distribution for X_2 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 2 decimals.\n Example: {0: 0.12, 1: 0.88}\n ---Answer:{0: 0.5, 1: 0.5}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.32, 0.68 ;\\n}\\nprobability ( X_1 ) {\\n    table 0.31, 0.69 ;\\n}\\nprobability ( X_2 | X_1 ) {\\n    ( 0 ) 0.5, 0.5;\\n    ( 1 ) 0.55, 0.45;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_1 is equal to 0, and the state X_0 is equal to 1', 'target': 'X_2', 'variables': ['X_1', 'X_2', 'X_0'], 'n_round': 2, 'cot': 'Result: P(X_2 | X_1=0) = {0: 0.5, 1: 0.5}\\nResult: P(X_2 | X_1=0) = {0: 0.5, 1: 0.5}', '_time': 1.2834177017211914, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 2, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 1, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 191, '_cot_tokens': 75}\n ---Task:bayesian_association,\n ---Prompt:System:\n P(X_0) = {'0': 0.8, '1': 0.2} \n P(X_1|X_0=0) = {'0': 0.2, '1': 0.8} \n P(X_1|X_0=1) = {'0': 0.6, '1': 0.4} \n P(X_2|X_0=0, X_1=0) = {'0': 0.3, '1': 0.7} \n P(X_2|X_0=0, X_1=1) = {'0': 0.5, '1': 0.5} \n P(X_2|X_0=1, X_1=0) = {'0': 0.3, '1': 0.7} \n P(X_2|X_0=1, X_1=1) = {'0': 0.5, '1': 0.5} \n P(X_3|X_0=0, X_2=0) = {'0': 0.7, '1': 0.3} \n P(X_3|X_0=0, X_2=1) = {'0': 0.4, '1': 0.6} \n P(X_3|X_0=1, X_2=0) = {'0': 0.2, '1': 0.8} \n P(X_3|X_0=1, X_2=1) = {'0': 0.8, '1': 0.2}\n Observed conditions:\n Observing/Knowing that the state X_2 is equal to 1, and the state X_3 is equal to 1\n Task: Compute probability distribution for X_1 (possible values: [0, 1]).\n \n Output: Python dict mapping each value to its probability, rounded to 1 decimals.\n Example: {0: 0.1, 1: 0.9}\n ---Answer:{0: 0.3, 1: 0.7}\n ---Metadata:{'target_var_values': [0, 1], 'bif_description': \"// CANONICAL\\n// variable: X_0\\n// state_names: {'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_1\\n// state_names: {'X_1': [0, 1], 'X_0': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_2\\n// state_names: {'X_2': [0, 1], 'X_0': [0, 1], 'X_1': [0, 1]}\\n// type: TabularCPD\\n// CANONICAL\\n// variable: X_3\\n// state_names: {'X_3': [0, 1], 'X_0': [0, 1], 'X_2': [0, 1]}\\n// type: TabularCPD\\n\\nnetwork unknown {\\n}\\nvariable X_0 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_1 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_2 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nvariable X_3 {\\n    type discrete [ 2 ] { 0, 1 };\\n    property weight = None ;\\n}\\nprobability ( X_0 ) {\\n    table 0.8, 0.2 ;\\n}\\nprobability ( X_1 | X_0 ) {\\n    ( 0 ) 0.2, 0.8;\\n    ( 1 ) 0.6, 0.4;\\n\\n}\\nprobability ( X_2 | X_0, X_1 ) {\\n    ( 0, 0 ) 0.3, 0.7;\\n    ( 0, 1 ) 0.5, 0.5;\\n    ( 1, 0 ) 0.3, 0.7;\\n    ( 1, 1 ) 0.5, 0.5;\\n\\n}\\nprobability ( X_3 | X_0, X_2 ) {\\n    ( 0, 0 ) 0.7, 0.3;\\n    ( 0, 1 ) 0.4, 0.6;\\n    ( 1, 0 ) 0.2, 0.8;\\n    ( 1, 1 ) 0.8, 0.2;\\n\\n}\\n\", 'scenario': 'Observing/Knowing that the state X_2 is equal to 1, and the state X_3 is equal to 1', 'target': 'X_1', 'variables': ['X_0', 'X_1', 'X_2', 'X_3'], 'n_round': 1, 'cot': \"Elim order: ['X_0']\\nSum out X_0 -&gt; P(X_1, X_2=1, X_3=1) = {0: 0.1, 1: 0.2}\\nNormalize (sum=0.3) -&gt; P(X_1 | X_2=1, X_3=1) = {0: 0.3, 1: 0.7}\\nElim order: ['X_0']\\nSum out X_0 -&gt; P(X_1, X_2=1, X_3=1) = {0: 0.1, 1: 0.2}\\nNormalize (sum=0.3) -&gt; P(X_1 | X_2=1, X_3=1) = {0: 0.3, 1: 0.7}\", '_time': 1.2358498573303223, '_task': 'bayesian_association', '_level': 1, '_config': {'c': 1.0, 'level': 1, 'seed': None, 'size': None, 'n_nodes': 4, 'max_domain_size': 3, 'edge_prob': 0.5, 'graph_generation_mode': 'erdos', 'n_round': 2, 'cpt_relative_threshold': 0.5, 'cot_scientific_notation': False, 'is_verbose': False, 'concise_cot': True}, '_prompt_tokens': 448, '_cot_tokens': 199}\n ---Task:bayesian_association]</pre> In\u00a0[\u00a0]: Copied! <pre>import datasets\nimport pandas as pd\n\n# 1. Generate data, as a list of Problem\ndata = task_r1.generate_balanced_batch(10, level=1)\n\n# 2. Create DataFrame\ndf = pd.DataFrame(data)\ndisplay(df.head())\n\n# 3. Convert to Hugging Face Dataset (Corrected Syntax)\nhf_dataset = datasets.Dataset.from_pandas(df)\n\n# 4. Split into train and test\ndataset_dict = hf_dataset.train_test_split(test_size=0.1)\nprint(\"\\nDataset Structure:\", dataset_dict)\n</pre> import datasets import pandas as pd  # 1. Generate data, as a list of Problem data = task_r1.generate_balanced_batch(10, level=1)  # 2. Create DataFrame df = pd.DataFrame(data) display(df.head())  # 3. Convert to Hugging Face Dataset (Corrected Syntax) hf_dataset = datasets.Dataset.from_pandas(df)  # 4. Split into train and test dataset_dict = hf_dataset.train_test_split(test_size=0.1) print(\"\\nDataset Structure:\", dataset_dict) answer metadata prompt task 0 {0: 0.4, 1: 0.6} {'target_var_values': [0, 1], 'bif_description... System:\\nP(X_0) = {'0': 0.1, '1': 0.9} \\nP(X_1... bayesian_association 1 {0: 0.09, 1: 0.88, 2: 0.03} {'target_var_values': [0, 1, 2], 'bif_descript... System:\\nP(X_0) = {'0': 0.12, '1': 0.85, '2': ... bayesian_association 2 {0: 0.57, 1: 0.43} {'target_var_values': [0, 1], 'bif_description... System:\\nP(X_0) = {'0': 0.51, '1': 0.49} \\nP(X... bayesian_association 3 {0: 0.8, 1: 0.2} {'target_var_values': [0, 1], 'bif_description... System:\\nP(X_0) = {'0': 0.4, '1': 0.2, '2': 0.... bayesian_association 4 {0: 0.5, 1: 0.5} {'target_var_values': [0, 1], 'bif_description... System:\\nP(X_0) = {'0': 0.7, '1': 0.3} \\nP(X_2... bayesian_association <pre>\nDataset Structure: DatasetDict({\n    train: Dataset({\n        features: ['answer', 'metadata', 'prompt', 'task'],\n        num_rows: 9\n    })\n    test: Dataset({\n        features: ['answer', 'metadata', 'prompt', 'task'],\n        num_rows: 1\n    })\n})\n</pre> In\u00a0[\u00a0]: Copied! <pre># DATASET_NAME = \"your-username/causal-reasoning-curriculum\"\n# dataset_dict.push_to_hub(DATASET_NAME)\n# print(f\"Dataset uploaded to https://huggingface.co/datasets/{DATASET_NAME}\")\n</pre> # DATASET_NAME = \"your-username/causal-reasoning-curriculum\" # dataset_dict.push_to_hub(DATASET_NAME) # print(f\"Dataset uploaded to https://huggingface.co/datasets/{DATASET_NAME}\")"},{"location":"notebooks/Causal_reasoning/#creating-a-causal-reasoning-dataset-with-reasoning-core","title":"Creating a Causal Reasoning Dataset with Reasoning Core\u00b6","text":"<p>This notebook demonstrates how to use the <code>reasoning_core</code> library to generate a synthetic dataset for Causal Reasoning tasks (specifically <code>BayesianAssociation</code>). We will show how to use the <code>.set_level()</code> method to generate problems of increasing difficulty and how to prepare the final dataset for uploading to the Hugging Face Hub.</p>"},{"location":"notebooks/Causal_reasoning/#1-initialize-the-task","title":"1. Initialize the Task\u00b6","text":"<p>We start by instantiating the <code>BayesianAssociation</code> task. This task generates problems where the goal is to compute the probability of a target variable given some evidence in a Bayesian Network, that correspond to the <code>Rung 1</code> of Pearl's ladder of causation.</p>"},{"location":"notebooks/Causal_reasoning/#2-understanding-difficulty-levels","title":"2. Understanding Difficulty Levels\u00b6","text":"<p>The <code>reasoning_core</code> library allows you to control the difficulty of the generated problems using the <code>.config.set_level(level)</code> method. Increasing the level typically increases the complexity of the underlying graph (e.g., more nodes, larger domains, more complex dependencies).</p>"},{"location":"notebooks/Causal_reasoning/#21-generation","title":"2.1 Generation\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#22-cot","title":"2.2 Cot\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#3-other-features","title":"3. Other features\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#31-you-can-as-well-seed-the-generation-that-are-twined-across-both-rung","title":"3.1 You can as well seed the generation, that are twined across both rung.\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#32-you-can-as-well-choose-to-configure-the-configuration-in-a-precise-way","title":"3.2 You can as well choose to configure the configuration in a precise way\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#321-noisy-mode","title":"3.2.1 Noisy mode\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#322-verbose-mode","title":"3.2.2 Verbose mode\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#3220-noisy-setting","title":"3.2.2.0 Noisy setting\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#3221-verbose-prompts","title":"3.2.2.1 Verbose prompts\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#3222-verbose-cot","title":"3.2.2.2 Verbose CoT\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#4-generating-the-dataset","title":"4. Generating the Dataset\u00b6","text":"<p>We will now generate a dataset.</p>"},{"location":"notebooks/Causal_reasoning/#41-generate-a-dataset-with-the-previous-config-settings-verbose-in-that-case","title":"4.1 Generate a dataset with the previous config settings (verbose in that case)\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#42-generating-with-specific-level","title":"4.2 Generating with specific level\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#43-converting-to-hugging-face-dataset","title":"4.3 Converting to Hugging Face Dataset\u00b6","text":""},{"location":"notebooks/Causal_reasoning/#44-uploading-to-hugging-face-hub","title":"4.4 Uploading to Hugging Face Hub\u00b6","text":""},{"location":"tasks/arithmetics/","title":"Arithmetics","text":""},{"location":"tasks/arithmetics/#reasoning_core.tasks.arithmetics","title":"<code>reasoning_core.tasks.arithmetics</code>","text":""},{"location":"tasks/arithmetics/#reasoning_core.tasks.arithmetics-classes","title":"Classes","text":""},{"location":"tasks/causal_reasoning/","title":"Causal Reasoning","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning","title":"<code>reasoning_core.tasks.causal_reasoning</code>","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning-classes","title":"Classes","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.BinaryInfluenceModel","title":"<code>BinaryInfluenceModel</code>","text":"<p>               Bases: <code>TabularCPD</code></p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>class BinaryInfluenceModel(TabularCPD):\n    def __init__(\n        self,\n        variable,\n        evidence,\n        activation_magnitude,\n        mode=\"OR\",\n        leak=None,\n        isboolean_style=False,\n        state_names=None,\n    ):\n        \"\"\"\n        A mechanistically defined CPD for Noisy-OR / Noisy-AND logic.\n        \"\"\"\n        self.mode = mode.upper()\n        if self.mode not in {\"OR\", \"AND\"}:\n            raise ValueError(\"mode must be either 'OR' or 'AND'\")\n\n        self.isboolean_style = isboolean_style\n        self.activation_magnitude = np.asarray(activation_magnitude, dtype=float)\n        self.leak = np.array([leak]) if leak is not None else None\n        self.isleaky = leak is not None\n\n        # --- FIX: Renamed back to self.evidence for compatibility with to_nl ---\n        self.evidence = list(evidence) \n\n        # Prepare State Names (Robustly)\n        if self.isboolean_style:\n            default_states = [False, True]\n        else:\n            default_states = [0, 1]\n\n        all_vars = [variable] + self.evidence\n        self.full_state_names = _fill_missing_state_names(all_vars, state_names, default_states)\n\n        # Calculate Probability Table\n        parent_states = [self.full_state_names[e] for e in self.evidence]\n        cols = []\n\n        for combo in product(*parent_states):\n            evidence_inst = dict(zip(self.evidence, combo))\n            probs = self._evaluate(evidence_inst)\n            cols.append(probs)\n\n        cpd_values = np.array(cols).T\n\n        # Canonical Initialization\n        super().__init__(\n            variable=variable,\n            variable_card=2,\n            values=cpd_values,\n            evidence=self.evidence,\n            evidence_card=[2] * len(self.evidence),\n            state_names=self.full_state_names,\n        )\n\n    def _evaluate(self, evidence_instantiate: dict) -&gt; np.ndarray:\n        active_mask = []\n        for e in self.evidence:\n            val = evidence_instantiate[e]\n            states = self.full_state_names[e]\n            active_mask.append(val == states[-1])\n\n        active_mask = np.array(active_mask)\n        probs = self.activation_magnitude[active_mask]\n\n        if self.mode == \"OR\":\n            p_active = 1 - np.prod(1 - probs)\n            if self.isleaky:\n                p_active = 1 - (1 - p_active) * (1 - self.leak[0])\n        else:  # AND\n            if np.any(~active_mask):\n                p_active = self.leak[0] if self.isleaky else 0.0\n            else:\n                p_active = np.prod(probs)\n                if self.isleaky:\n                    p_active = 1 - (1 - p_active) * (1 - self.leak[0])\n\n        return np.array([1 - p_active, p_active])\n\n    def copy(self):\n        new_cpd = BinaryInfluenceModel(\n            variable=self.variable,\n            evidence=self.evidence, # Use stored evidence\n            activation_magnitude=self.activation_magnitude.copy(),\n            mode=self.mode,\n            leak=self.leak[0] if self.leak is not None else None,\n            isboolean_style=self.isboolean_style,\n            state_names=self.state_names.copy()\n        )\n        return new_cpd\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.BinaryInfluenceModel-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.BinaryInfluenceModel.__init__","title":"<code>__init__(variable, evidence, activation_magnitude, mode='OR', leak=None, isboolean_style=False, state_names=None)</code>","text":"<p>A mechanistically defined CPD for Noisy-OR / Noisy-AND logic.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def __init__(\n    self,\n    variable,\n    evidence,\n    activation_magnitude,\n    mode=\"OR\",\n    leak=None,\n    isboolean_style=False,\n    state_names=None,\n):\n    \"\"\"\n    A mechanistically defined CPD for Noisy-OR / Noisy-AND logic.\n    \"\"\"\n    self.mode = mode.upper()\n    if self.mode not in {\"OR\", \"AND\"}:\n        raise ValueError(\"mode must be either 'OR' or 'AND'\")\n\n    self.isboolean_style = isboolean_style\n    self.activation_magnitude = np.asarray(activation_magnitude, dtype=float)\n    self.leak = np.array([leak]) if leak is not None else None\n    self.isleaky = leak is not None\n\n    # --- FIX: Renamed back to self.evidence for compatibility with to_nl ---\n    self.evidence = list(evidence) \n\n    # Prepare State Names (Robustly)\n    if self.isboolean_style:\n        default_states = [False, True]\n    else:\n        default_states = [0, 1]\n\n    all_vars = [variable] + self.evidence\n    self.full_state_names = _fill_missing_state_names(all_vars, state_names, default_states)\n\n    # Calculate Probability Table\n    parent_states = [self.full_state_names[e] for e in self.evidence]\n    cols = []\n\n    for combo in product(*parent_states):\n        evidence_inst = dict(zip(self.evidence, combo))\n        probs = self._evaluate(evidence_inst)\n        cols.append(probs)\n\n    cpd_values = np.array(cols).T\n\n    # Canonical Initialization\n    super().__init__(\n        variable=variable,\n        variable_card=2,\n        values=cpd_values,\n        evidence=self.evidence,\n        evidence_card=[2] * len(self.evidence),\n        state_names=self.full_state_names,\n    )\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.MultilevelInfluenceModel","title":"<code>MultilevelInfluenceModel</code>","text":"<p>               Bases: <code>TabularCPD</code></p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>class MultilevelInfluenceModel(TabularCPD):\n    def __init__(self, variable, evidence, influence_tables, levels, leak=None, mode=\"MAX\", state_names=None):\n        \"\"\"\n        A mechanistically defined CPD for Min/Max logic on multi-state variables.\n        \"\"\"\n        self.mode = mode.upper()\n        if self.mode not in {\"MAX\", \"MIN\"}:\n            raise ValueError(\"mode must be 'MAX' or 'MIN'\")\n\n        self.levels = levels\n        self.influence_tables = influence_tables\n        self.leak = np.array(leak) if leak is not None else None\n        self.isleaky = leak is not None\n\n        # --- FIX: Renamed back to self.evidence for compatibility with to_nl ---\n        self.evidence = list(evidence)\n\n        # Prepare State Names (Robustly)\n        temp_state_names = state_names.copy() if state_names else {}\n\n        if variable not in temp_state_names:\n            temp_state_names[variable] = list(range(levels))\n\n        for parent in self.evidence:\n            if parent not in temp_state_names:\n                if parent in influence_tables:\n                    temp_state_names[parent] = list(influence_tables[parent].keys())\n                else:\n                    temp_state_names[parent] = list(range(levels))\n\n        self.full_state_names = temp_state_names\n\n        # Pre-calculate Cumulative Tables\n        self.cumulative_tables = {\n            p: {v: np.cumsum(probs) for v, probs in table.items()}\n            for p, table in influence_tables.items()\n        }\n        self.cumulative_leak = np.cumsum(leak) if leak is not None else np.ones(levels)\n\n        # Calculate Probability Table\n        parent_states = [self.full_state_names[p] for p in self.evidence]\n        cols = []\n\n        for combo in product(*parent_states):\n            e = dict(zip(self.evidence, combo))\n            probs = self._evaluate(e)\n            cols.append(probs)\n\n        values = np.vstack(cols).T\n\n        # Canonical Initialization\n        super().__init__(\n            variable=variable,\n            variable_card=self.levels,\n            values=values,\n            evidence=self.evidence,\n            evidence_card=[len(st) for st in parent_states],\n            state_names=self.full_state_names,\n        )\n\n    def _validate_probs(self, arr, name=\"probabilities\"):\n        arr = np.asarray(arr)\n        if np.any(arr &lt; 0) or np.any(arr &gt; 1):\n             raise ValueError(f\"{name} must be between 0 and 1.\")\n        if not np.isclose(arr.sum(), 1.0, atol=1e-5):\n             raise ValueError(f\"{name} must sum to 1. Got {arr.sum()}\")\n        return arr\n\n    def _evaluate(self, evidence_instantiate: dict) -&gt; np.ndarray:\n        if self.mode == \"MAX\":\n            cum_prob = np.ones(self.levels)\n            for parent, val in evidence_instantiate.items():\n                theta = self.cumulative_tables[parent][val]\n                cum_prob *= theta\n            if self.isleaky:\n                cum_prob *= self.cumulative_leak\n        elif self.mode == \"MIN\":\n            complement_prod = np.ones(self.levels)\n            for parent, val in evidence_instantiate.items():\n                theta = self.cumulative_tables[parent][val]\n                complement_prod *= (1 - theta)\n            if self.isleaky:\n                complement_prod *= (1 - self.cumulative_leak)\n            cum_prob = 1 - complement_prod\n\n        cum_prob = np.maximum.accumulate(np.clip(cum_prob, 0, 1))\n        probs = np.diff(np.concatenate(([0.0], cum_prob)))\n\n        total = probs.sum()\n        if total &gt; 0:\n            probs = probs / total\n        return probs\n\n    def copy(self):\n        new_cpd = MultilevelInfluenceModel(\n            variable=self.variable,\n            evidence=self.evidence, # Use stored evidence\n            influence_tables=self.influence_tables, \n            levels=self.levels,\n            leak=self.leak.tolist() if self.leak is not None else None,\n            mode=self.mode,\n            state_names=self.state_names.copy()\n        )\n        return new_cpd\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.MultilevelInfluenceModel-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.MultilevelInfluenceModel.__init__","title":"<code>__init__(variable, evidence, influence_tables, levels, leak=None, mode='MAX', state_names=None)</code>","text":"<p>A mechanistically defined CPD for Min/Max logic on multi-state variables.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def __init__(self, variable, evidence, influence_tables, levels, leak=None, mode=\"MAX\", state_names=None):\n    \"\"\"\n    A mechanistically defined CPD for Min/Max logic on multi-state variables.\n    \"\"\"\n    self.mode = mode.upper()\n    if self.mode not in {\"MAX\", \"MIN\"}:\n        raise ValueError(\"mode must be 'MAX' or 'MIN'\")\n\n    self.levels = levels\n    self.influence_tables = influence_tables\n    self.leak = np.array(leak) if leak is not None else None\n    self.isleaky = leak is not None\n\n    # --- FIX: Renamed back to self.evidence for compatibility with to_nl ---\n    self.evidence = list(evidence)\n\n    # Prepare State Names (Robustly)\n    temp_state_names = state_names.copy() if state_names else {}\n\n    if variable not in temp_state_names:\n        temp_state_names[variable] = list(range(levels))\n\n    for parent in self.evidence:\n        if parent not in temp_state_names:\n            if parent in influence_tables:\n                temp_state_names[parent] = list(influence_tables[parent].keys())\n            else:\n                temp_state_names[parent] = list(range(levels))\n\n    self.full_state_names = temp_state_names\n\n    # Pre-calculate Cumulative Tables\n    self.cumulative_tables = {\n        p: {v: np.cumsum(probs) for v, probs in table.items()}\n        for p, table in influence_tables.items()\n    }\n    self.cumulative_leak = np.cumsum(leak) if leak is not None else np.ones(levels)\n\n    # Calculate Probability Table\n    parent_states = [self.full_state_names[p] for p in self.evidence]\n    cols = []\n\n    for combo in product(*parent_states):\n        e = dict(zip(self.evidence, combo))\n        probs = self._evaluate(e)\n        cols.append(probs)\n\n    values = np.vstack(cols).T\n\n    # Canonical Initialization\n    super().__init__(\n        variable=variable,\n        variable_card=self.levels,\n        values=values,\n        evidence=self.evidence,\n        evidence_card=[len(st) for st in parent_states],\n        state_names=self.full_state_names,\n    )\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph","title":"<code>ReasoningGraph</code>","text":"Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>class ReasoningGraph:\n    def __init__(self, bn: DiscreteBayesianNetwork = None):\n        if bn:\n            self.bn = bn.copy()\n        else:\n            self.bn = DiscreteBayesianNetwork()\n        self.reset_inference()\n\n    def generate_new_graph(self, n=4, max_domain_size = 3,**kwargs):\n        method = kwargs.pop('method', 'erdos')\n        seed = kwargs.pop('seed', None)\n        conditionning_seed = kwargs.pop('conditionning_seed', None)\n        edge_prob = kwargs.pop('edge_prob', 0.5)\n        self.bn = DiscreteBayesianNetwork.get_random(\n        n_nodes = n,\n        edge_prob = edge_prob,\n        n_states = [ k+1 for k in range(1,max_domain_size)],\n        method = method,\n        seed = seed,\n        conditionning_seed = conditionning_seed,\n        **kwargs)\n\n        self.ie = CausalVE(self.bn) #Causal Variable Elimination Home Made\n\n    def reset_inference(self):\n        self.target = None\n        self.do_var = None\n        self.evidence_values = {}\n        self.do_values = {}\n\n    def generate_rung1(self, seed=None):\n        \"\"\"Sets up an observational query (Rung 1).\"\"\"\n\n        self.reset_inference()\n\n        # Local RNG \u2192 isolated seed, no global contamination\n        rng = random.Random(seed)\n\n        variables = list(self.bn.nodes())\n        self.target = rng.choice(variables)\n\n        variables.remove(self.target)\n        n_evidence = rng.randint(0, len(variables))\n        evidence_variables = rng.sample(variables, n_evidence)\n\n        for state in evidence_variables:\n            possible_values = self.bn.states[state]\n            self.evidence_values[state] = rng.choice(possible_values)\n\n    def generate_rung2(self, seed=None):\n        \"\"\"Sets up an interventional query (Rung 2).\"\"\"\n\n        self.reset_inference()\n\n        rng = random.Random(seed)\n\n\n        variables = list(self.bn.nodes())\n        self.target = rng.choice(variables)\n\n        variables.remove(self.target)\n\n        do_var = rng.choice(variables)\n        possible_values = self.bn.states[do_var]\n        self.do_values = {do_var: rng.choice(possible_values)}\n        self.do_var = do_var\n\n        variables.remove(do_var)\n\n        n_evidence = rng.randint(0, len(variables))\n        evidence_variables = rng.sample(variables, n_evidence)\n\n        for state in evidence_variables:\n            possible_values = self.bn.states[state]\n            self.evidence_values[state] = rng.choice(possible_values)\n\n#### Bounded Generation ####\n\n    def generate_bounded_rung1_and_rung2(self, seed=None):\n        \"\"\"Generate matched Rung1 and Rung2 queries from the same seed.\"\"\"\n        rng = random.Random(seed)\n        variables = list(self.bn.nodes())\n\n        target = rng.choice(variables)\n\n        remaining_vars = variables.copy()\n        remaining_vars.remove(target)\n\n        n_evidence = rng.randint(1, len(remaining_vars)) # up to len(remaining_var) - 1\n        evidence_vars = rng.sample(remaining_vars, n_evidence)\n\n        evidence_values = {}\n        for v in evidence_vars:\n            evidence_values[v] = rng.choice(self.bn.states[v])\n\n        do_var = rng.choice(evidence_vars)\n        do_value = evidence_values[do_var]\n\n        # Rung2 evidence is Rung1 evidence minus the promoted do-var\n        evidence_vars_r2 = [v for v in evidence_vars if v != do_var]\n        evidence_values_r2 = {v: evidence_values[v] for v in evidence_vars_r2}\n\n        self._r1_target = target\n        self._r1_evidence_values = evidence_values\n\n        self._r2_target = target\n        self._r2_do_var = do_var\n        self._r2_do_values = {do_var: do_value}\n        self._r2_evidence_values = evidence_values_r2\n\n    def generate_bonded_rung1(self, seed=None):\n        \"\"\"Builds Rung1 using precomputed aligned data.\"\"\"\n        self.generate_bounded_rung1_and_rung2(seed)\n\n        self.reset_inference()\n        self.target = self._r1_target\n        self.evidence_values = self._r1_evidence_values.copy()\n\n\n    def generate_bonded_rung2(self, seed=None):\n        \"\"\"Builds Rung2 using precomputed aligned data.\"\"\"\n        self.generate_bounded_rung1_and_rung2(seed)\n\n        self.reset_inference()\n        self.target = self._r2_target\n        self.do_values = self._r2_do_values.copy()\n        self.do_var = self._r2_do_var\n        self.evidence_values = self._r2_evidence_values.copy()\n\n#### End bounded generation ####\n\n    def predict(self) -&gt; DiscreteFactor:\n        \"\"\"Make observational predictions.\"\"\"\n        if self.ie is None:\n            raise Exception(\"Inference engine not initialized. Generate a graph first.\")\n        return self.ie.query( variables = [self.target], evidence = self.evidence_values, do = self.do_values )\n\n    def do_to_NL(self):\n        \"\"\"Convert interventional evidence to NL.\"\"\"\n        ret = \"\"\n        if self.do_values:\n            ret += \"Doing/Imposing that \"\n            ret += \", and \".join(f\"the state {state} is equal to {repr(val)}\" \n                                 for state, val in self.do_values.items())\n        return ret\n\n    def evidences_to_NL(self):\n        \"\"\"Convert observational evidence to NL.\"\"\"\n        ret = \"\"\n        if self.evidence_values:\n            ret += \"Observing/Knowing that \"\n            ret += \", and \".join(f\"the state {state} is equal to {repr(val)}\" \n                                 for state, val in self.evidence_values.items())\n        else:\n            ret = \"Without further Observation/Knowledge of other variable.\"\n        return ret\n\n    def target_to_NL(self):\n        return f\"\"\"Provide the probability over the state named {self.target} \"\"\"\n\n    def to_NL(self, n_round: int = 4, verbose = False) -&gt; str:     \n        return self.bn.to_nl(n_round, verbose)\n\n    def convert_complex_nodes_to_ci(\n        self, \n        cpt_relative_threshold: float, \n        seed: int = None,\n        binari_ci_modes: List[str] = ['or', 'and'],\n        multi_ci_modes: List[str] = ['max', 'min'],\n        n_round: Optional[int] = None,\n    ) -&gt; list:\n        \"\"\"\n        Post-processing step to convert large TabularCPDs to CI models.\n        \"\"\"\n        if self.bn is None or not self.bn.nodes():\n            return []\n\n        rng = np.random.default_rng(seed)\n        converted_nodes = []\n\n        for node in self.bn.nodes():\n            old_cpd = self.bn.get_cpds(node)\n\n            if not isinstance(old_cpd, TabularCPD):\n                continue\n\n            cpt_relative_diff = (old_cpd.size_full() - old_cpd.size_CI_model())/old_cpd.size_CI_model()\n\n            if cpt_relative_diff &gt; cpt_relative_threshold:\n                parents = list(self.bn.predecessors(node))\n                if not parents:\n                    continue\n\n                all_vars = [node] + parents\n                cardinalities = {v: self.bn.get_cardinality(v) for v in all_vars}\n                child_card = cardinalities[node]\n                parent_cards = [cardinalities[p] for p in parents]\n\n                new_cpd = None\n\n                if child_card == 2 and all(c == 2 for c in parent_cards):\n                    chosen_mode = rng.choice(binari_ci_modes)\n                else:\n                    chosen_mode = rng.choice(multi_ci_modes)\n                new_cpd = get_random_CI(\n                        variable = node,\n                        evidence = parents,\n                        cardinality = cardinalities,\n                        mode = chosen_mode,\n                        seed = seed)\n                if n_round != None:\n                    new_cpd.round(n_round)\n                if new_cpd:\n                    self.bn.remove_cpds(old_cpd)\n                    self.bn.add_cpds(new_cpd)\n                    converted_nodes.append(node)\n\n\n        self.ie = CausalVE(self.bn)\n\n        return converted_nodes\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.convert_complex_nodes_to_ci","title":"<code>convert_complex_nodes_to_ci(cpt_relative_threshold, seed=None, binari_ci_modes=['or', 'and'], multi_ci_modes=['max', 'min'], n_round=None)</code>","text":"<p>Post-processing step to convert large TabularCPDs to CI models.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def convert_complex_nodes_to_ci(\n    self, \n    cpt_relative_threshold: float, \n    seed: int = None,\n    binari_ci_modes: List[str] = ['or', 'and'],\n    multi_ci_modes: List[str] = ['max', 'min'],\n    n_round: Optional[int] = None,\n) -&gt; list:\n    \"\"\"\n    Post-processing step to convert large TabularCPDs to CI models.\n    \"\"\"\n    if self.bn is None or not self.bn.nodes():\n        return []\n\n    rng = np.random.default_rng(seed)\n    converted_nodes = []\n\n    for node in self.bn.nodes():\n        old_cpd = self.bn.get_cpds(node)\n\n        if not isinstance(old_cpd, TabularCPD):\n            continue\n\n        cpt_relative_diff = (old_cpd.size_full() - old_cpd.size_CI_model())/old_cpd.size_CI_model()\n\n        if cpt_relative_diff &gt; cpt_relative_threshold:\n            parents = list(self.bn.predecessors(node))\n            if not parents:\n                continue\n\n            all_vars = [node] + parents\n            cardinalities = {v: self.bn.get_cardinality(v) for v in all_vars}\n            child_card = cardinalities[node]\n            parent_cards = [cardinalities[p] for p in parents]\n\n            new_cpd = None\n\n            if child_card == 2 and all(c == 2 for c in parent_cards):\n                chosen_mode = rng.choice(binari_ci_modes)\n            else:\n                chosen_mode = rng.choice(multi_ci_modes)\n            new_cpd = get_random_CI(\n                    variable = node,\n                    evidence = parents,\n                    cardinality = cardinalities,\n                    mode = chosen_mode,\n                    seed = seed)\n            if n_round != None:\n                new_cpd.round(n_round)\n            if new_cpd:\n                self.bn.remove_cpds(old_cpd)\n                self.bn.add_cpds(new_cpd)\n                converted_nodes.append(node)\n\n\n    self.ie = CausalVE(self.bn)\n\n    return converted_nodes\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.do_to_NL","title":"<code>do_to_NL()</code>","text":"<p>Convert interventional evidence to NL.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def do_to_NL(self):\n    \"\"\"Convert interventional evidence to NL.\"\"\"\n    ret = \"\"\n    if self.do_values:\n        ret += \"Doing/Imposing that \"\n        ret += \", and \".join(f\"the state {state} is equal to {repr(val)}\" \n                             for state, val in self.do_values.items())\n    return ret\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.evidences_to_NL","title":"<code>evidences_to_NL()</code>","text":"<p>Convert observational evidence to NL.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def evidences_to_NL(self):\n    \"\"\"Convert observational evidence to NL.\"\"\"\n    ret = \"\"\n    if self.evidence_values:\n        ret += \"Observing/Knowing that \"\n        ret += \", and \".join(f\"the state {state} is equal to {repr(val)}\" \n                             for state, val in self.evidence_values.items())\n    else:\n        ret = \"Without further Observation/Knowledge of other variable.\"\n    return ret\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.generate_bonded_rung1","title":"<code>generate_bonded_rung1(seed=None)</code>","text":"<p>Builds Rung1 using precomputed aligned data.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def generate_bonded_rung1(self, seed=None):\n    \"\"\"Builds Rung1 using precomputed aligned data.\"\"\"\n    self.generate_bounded_rung1_and_rung2(seed)\n\n    self.reset_inference()\n    self.target = self._r1_target\n    self.evidence_values = self._r1_evidence_values.copy()\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.generate_bonded_rung2","title":"<code>generate_bonded_rung2(seed=None)</code>","text":"<p>Builds Rung2 using precomputed aligned data.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def generate_bonded_rung2(self, seed=None):\n    \"\"\"Builds Rung2 using precomputed aligned data.\"\"\"\n    self.generate_bounded_rung1_and_rung2(seed)\n\n    self.reset_inference()\n    self.target = self._r2_target\n    self.do_values = self._r2_do_values.copy()\n    self.do_var = self._r2_do_var\n    self.evidence_values = self._r2_evidence_values.copy()\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.generate_bounded_rung1_and_rung2","title":"<code>generate_bounded_rung1_and_rung2(seed=None)</code>","text":"<p>Generate matched Rung1 and Rung2 queries from the same seed.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def generate_bounded_rung1_and_rung2(self, seed=None):\n    \"\"\"Generate matched Rung1 and Rung2 queries from the same seed.\"\"\"\n    rng = random.Random(seed)\n    variables = list(self.bn.nodes())\n\n    target = rng.choice(variables)\n\n    remaining_vars = variables.copy()\n    remaining_vars.remove(target)\n\n    n_evidence = rng.randint(1, len(remaining_vars)) # up to len(remaining_var) - 1\n    evidence_vars = rng.sample(remaining_vars, n_evidence)\n\n    evidence_values = {}\n    for v in evidence_vars:\n        evidence_values[v] = rng.choice(self.bn.states[v])\n\n    do_var = rng.choice(evidence_vars)\n    do_value = evidence_values[do_var]\n\n    # Rung2 evidence is Rung1 evidence minus the promoted do-var\n    evidence_vars_r2 = [v for v in evidence_vars if v != do_var]\n    evidence_values_r2 = {v: evidence_values[v] for v in evidence_vars_r2}\n\n    self._r1_target = target\n    self._r1_evidence_values = evidence_values\n\n    self._r2_target = target\n    self._r2_do_var = do_var\n    self._r2_do_values = {do_var: do_value}\n    self._r2_evidence_values = evidence_values_r2\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.generate_rung1","title":"<code>generate_rung1(seed=None)</code>","text":"<p>Sets up an observational query (Rung 1).</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def generate_rung1(self, seed=None):\n    \"\"\"Sets up an observational query (Rung 1).\"\"\"\n\n    self.reset_inference()\n\n    # Local RNG \u2192 isolated seed, no global contamination\n    rng = random.Random(seed)\n\n    variables = list(self.bn.nodes())\n    self.target = rng.choice(variables)\n\n    variables.remove(self.target)\n    n_evidence = rng.randint(0, len(variables))\n    evidence_variables = rng.sample(variables, n_evidence)\n\n    for state in evidence_variables:\n        possible_values = self.bn.states[state]\n        self.evidence_values[state] = rng.choice(possible_values)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.generate_rung2","title":"<code>generate_rung2(seed=None)</code>","text":"<p>Sets up an interventional query (Rung 2).</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def generate_rung2(self, seed=None):\n    \"\"\"Sets up an interventional query (Rung 2).\"\"\"\n\n    self.reset_inference()\n\n    rng = random.Random(seed)\n\n\n    variables = list(self.bn.nodes())\n    self.target = rng.choice(variables)\n\n    variables.remove(self.target)\n\n    do_var = rng.choice(variables)\n    possible_values = self.bn.states[do_var]\n    self.do_values = {do_var: rng.choice(possible_values)}\n    self.do_var = do_var\n\n    variables.remove(do_var)\n\n    n_evidence = rng.randint(0, len(variables))\n    evidence_variables = rng.sample(variables, n_evidence)\n\n    for state in evidence_variables:\n        possible_values = self.bn.states[state]\n        self.evidence_values[state] = rng.choice(possible_values)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.ReasoningGraph.predict","title":"<code>predict()</code>","text":"<p>Make observational predictions.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def predict(self) -&gt; DiscreteFactor:\n    \"\"\"Make observational predictions.\"\"\"\n    if self.ie is None:\n        raise Exception(\"Inference engine not initialized. Generate a graph first.\")\n    return self.ie.query( variables = [self.target], evidence = self.evidence_values, do = self.do_values )\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung","title":"<code>Rung</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for Rung tasks of any degree.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>class Rung(ABC):\n    \"\"\"An abstract base class for Rung tasks of any degree.\"\"\"\n    def __init__(self, config=Rung12Config(), bn: DiscreteBayesianNetwork = None):\n        super().__init__()\n        self.config = config\n        self.reason_graph = ReasoningGraph(bn=bn)\n\n    @abstractmethod\n    def _generate_specific_problem(self):\n        pass\n\n    @abstractmethod\n    def _generate_network(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def _calculate_answer_and_metadata(self):\n        pass\n\n    @abstractmethod\n    def _construct_scenario(self):\n        pass\n\n    def generate(self):\n        n_round = self.config.n_round #stochastic rounding value, that we will use for all subfunction to be coherent within the example    \n        self._generate_network(n=self.config.n_nodes,\n            method=self.config.graph_generation_mode,\n            edge_prob=self.config.edge_prob,\n            max_domain_size = self.config.max_domain_size,\n            n_round = n_round,\n           )\n\n        self._generate_specific_problem(n_round)\n\n        answer, specific_metadata = self._calculate_answer_and_metadata(n_round)\n        cot = self.reason_graph.ie.generate_natural_language_proof(scientific_notation=self.config.cot_scientific_notation, precision=n_round, concise=self.config.concise_cot)\n        while nan in set(eval(answer).values()): #Create another scenario if this one is probabilistically impossible.\n            if self.config.graph_seed != None:\n                self.config.graph_seed += 1\n            self._generate_specific_problem(n_round)\n            answer, specific_metadata = self._calculate_answer_and_metadata(n_round)\n\n        scenario = self._construct_scenario()\n        target_vals = self.reason_graph.bn.states[self.reason_graph.target]\n\n        writer = CanonicalBIFWriter(self.reason_graph.bn)\n        bif_data = writer.write_string()\n\n        if 'nan' in cot: #case where the problem is tricky for concise Cot solving\n            cot = None\n\n        metadata = {\n            \"target_var_values\": target_vals,\n            \"bif_description\":bif_data,\n            \"scenario\": scenario,\n            \"target\": self.reason_graph.target,\n            \"variables\": list(self.reason_graph.bn.nodes()),\n            \"n_round\": n_round,\n            \"cot\": cot\n        }\n        metadata.update(specific_metadata)\n\n        return Problem(metadata=metadata, answer=answer)\n\n    def prompt(self, metadata):\n        bif_data = metadata[\"bif_description\"]\n        model = ReasoningGraph(CanonicalBIFReader(string=bif_data).get_model())\n        n_round = metadata['n_round']\n        system_description = model.to_NL(n_round, self.config.is_verbose) \n\n        target = metadata[\"target\"]\n        values = metadata[\"target_var_values\"]\n\n        return (\n            f\"System:\\n{system_description}\\n\"\n            f\"Observed conditions:\\n{metadata['scenario']}\\n\"\n            f\"Task: Compute probability distribution for {target} (possible values: {values}).\\n\\n\"\n            f\"Output: Python dict mapping each value to its probability, rounded to {n_round} decimals.\\n\"\n            f\"Example: {{0: {round(0.123456789,n_round)}, 1: {round(0.876543211,n_round)}}}\"\n        )\n\n    def get_cot(self, expr):\n        return expr.cot\n\n    def score_answer(self, answer, entry):\n        \"\"\"Shared scoring function.\"\"\"\n        dict_truth = _to_dict(entry.answer)\n        try:\n            dict_pred = _to_dict(answer)\n        except:\n            return 0\n        return js_reward(dict_truth, dict_pred)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>Shared scoring function.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"Shared scoring function.\"\"\"\n    dict_truth = _to_dict(entry.answer)\n    try:\n        dict_pred = _to_dict(answer)\n    except:\n        return 0\n    return js_reward(dict_truth, dict_pred)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung12Config","title":"<code>Rung12Config</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Config</code></p> <p>Configuration for Rung 1 and Rung 2 tasks.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung12Config--parameters","title":"Parameters","text":"<p>n_nodes : int     Number of nodes in the graph. max_domain_size : int     Maximum domain size for the variables. edge_prob : float     Probability of an edge between two nodes. graph_generation_mode : str     Method for generating the graph (e.g., \"erdos\"). n_round : int     Number of decimal places to round probabilities to. Noisy_mode : bool     Whether to use the sparser Noisy interaction within the network. cpt_relative_threshold : float     If Noisy_mode is True, set the conversion relative threshold in gain of parameter size (classical/Noisy), to converte a classical CDP interaction into a random Noisy one. cot_scientific_notation : bool     Whether to use scientific notation for chain of thought. generate_trivial : bool     Whether to accept problem where no computationn only retriavial skills are necessary (mainly usefull for law level problems). is_verbose : bool     Whether to use a more humanlike description of the system, or a less verbose one that describe the Bayesian Network by listing all the conditional probabilities.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>@dataclass\nclass Rung12Config(Config):\n    \"\"\"\n    Configuration for Rung 1 and Rung 2 tasks.\n\n    Parameters\n    ----------\n    n_nodes : int\n        Number of nodes in the graph.\n    max_domain_size : int\n        Maximum domain size for the variables.\n    edge_prob : float\n        Probability of an edge between two nodes.\n    graph_generation_mode : str\n        Method for generating the graph (e.g., \"erdos\").\n    n_round : int\n        Number of decimal places to round probabilities to.\n    Noisy_mode : bool\n        Whether to use the sparser Noisy interaction within the network.\n    cpt_relative_threshold : float\n        If Noisy_mode is True, set the conversion relative threshold in gain of parameter size (classical/Noisy), to converte a classical CDP interaction into a random Noisy one.\n    cot_scientific_notation : bool\n        Whether to use scientific notation for chain of thought.\n    generate_trivial : bool\n        Whether to accept problem where no computationn only retriavial skills are necessary (mainly usefull for law level problems).\n    is_verbose : bool\n        Whether to use a more humanlike description of the system, or a less verbose one that describe the Bayesian Network by listing all the conditional probabilities.\n    \"\"\"\n    n_nodes: int = 3\n    max_domain_size: int = 2\n    edge_prob: float = 0.5\n    graph_generation_mode: str = \"erdos\"\n    n_round: int = 1\n    Noisy_mode = True\n    cpt_relative_threshold: float = 0\n    cot_scientific_notation: bool = False\n    graph_seed = None\n    conditionning_seed = None\n    seed = None\n    is_verbose: bool = False\n    concise_cot: bool  = True\n\n    def set_level(self, i: int):\n        # 1. Call the parent to handle the standard progression logic\n        super().set_level(i)\n\n        # 2. Handle specific levels\n        if i == 0:\n            self.generate_trivial = False\n        else:\n            self.generate_trivial = True\n        return self\n\n    def update(self, c):\n            self.n_round += .5 * c\n            self.n_nodes += .5 * c\n            self.max_domain_size += .5 * c\n            self.cpt_relative_threshold += .5 * c \n\n\n    def set_seed(self, graph_seed = None, conditionning_seed = None):\n        \"\"\"\n        Sets the random seeds for reproducibility of the graph structure and the specific query.\n\n        Parameters\n        ----------\n        Graph_seed : int, optional\n            The seed used to control the generation of the Bayesian Network structure \n            (topology, edges) and the parameters (CPDs). If provided, it ensures \n            the \"laws of the world\" are consistent across runs.\n        conditionning_seed : int, optional\n            The seed used to control the selection of the target variable, \n            evidence variables, and intervention values (the \"scenario\"). \n            Allows generating different questions/conditions on the exact same graph.\n            Morever, the conditionning accross rungs (association/intervention) are twined.\n        \"\"\"\n        self.graph_seed = graph_seed\n        self.seed = graph_seed\n\n        self.conditionning_seed = conditionning_seed\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung12Config-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung12Config.set_seed","title":"<code>set_seed(graph_seed=None, conditionning_seed=None)</code>","text":"<p>Sets the random seeds for reproducibility of the graph structure and the specific query.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.Rung12Config.set_seed--parameters","title":"Parameters","text":"<p>Graph_seed : int, optional     The seed used to control the generation of the Bayesian Network structure      (topology, edges) and the parameters (CPDs). If provided, it ensures      the \"laws of the world\" are consistent across runs. conditionning_seed : int, optional     The seed used to control the selection of the target variable,      evidence variables, and intervention values (the \"scenario\").      Allows generating different questions/conditions on the exact same graph.     Morever, the conditionning accross rungs (association/intervention) are twined.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def set_seed(self, graph_seed = None, conditionning_seed = None):\n    \"\"\"\n    Sets the random seeds for reproducibility of the graph structure and the specific query.\n\n    Parameters\n    ----------\n    Graph_seed : int, optional\n        The seed used to control the generation of the Bayesian Network structure \n        (topology, edges) and the parameters (CPDs). If provided, it ensures \n        the \"laws of the world\" are consistent across runs.\n    conditionning_seed : int, optional\n        The seed used to control the selection of the target variable, \n        evidence variables, and intervention values (the \"scenario\"). \n        Allows generating different questions/conditions on the exact same graph.\n        Morever, the conditionning accross rungs (association/intervention) are twined.\n    \"\"\"\n    self.graph_seed = graph_seed\n    self.seed = graph_seed\n\n    self.conditionning_seed = conditionning_seed\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.SemanticTraceVE","title":"<code>SemanticTraceVE</code>","text":"<p>               Bases: <code>VariableElimination</code></p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>class SemanticTraceVE(VariableElimination):\n    def __init__(self, model):\n        super().__init__(model)\n        self.trace_log = []\n        self.factor_semantics = {}\n        self.global_do = {} \n\n    # --- Core Logic (STABLE OLD VERSION) ---\n\n    def _get_working_factors(self, evidence):\n        \"\"\"Uses the stable dictionary-of-sets approach.\"\"\"\n        working_factors = {\n            node: set() for node in self.model.nodes()\n        }\n        for factor in self.factors.values():\n            factor = factor.reduce([(var, evidence[var]) for var in evidence if var in factor.variables], inplace=False)\n            for var in factor.variables:\n                working_factors[var].add((factor, var))\n        return working_factors\n\n    def _map_factors_to_cpds(self, working_factors, evidence):\n        unique_factors = {}\n        for bucket in working_factors.values():\n            for factor, _ in bucket:\n                unique_factors[id(factor)] = factor\n\n        if not hasattr(self.model, 'cpds'): return \n\n        for cpd in self.model.cpds:\n            temp_factor = cpd.to_factor()\n            reduce_list = [(var, state) for var, state in evidence.items() if var in temp_factor.variables]\n            if reduce_list:\n                try: reduced_factor = temp_factor.reduce(reduce_list, inplace=False)\n                except: continue\n            else:\n                reduced_factor = temp_factor\n\n            for factor in unique_factors.values():\n                if set(factor.variables) != set(reduced_factor.variables): continue\n                vals_to_compare = reduced_factor.values\n                if factor.variables != reduced_factor.variables:\n                    try:\n                        axes_order = [reduced_factor.variables.index(v) for v in factor.variables]\n                        vals_to_compare = np.transpose(reduced_factor.values, axes_order)\n                    except ValueError: continue\n\n                if factor.values.shape == vals_to_compare.shape:\n                    if np.allclose(factor.values, vals_to_compare):\n                        tracker = SemanticTracker.from_cpd(cpd, evidence, self.global_do)\n                        self.factor_semantics[id(factor)] = tracker\n\n    def base_query(self, variables, evidence=None, elimination_order=\"MinFill\", joint=True, show_progress=True):\n        evidence = evidence if evidence is not None else dict()\n        # Pruning logic from stable version\n        if isinstance(self.model, DiscreteBayesianNetwork):\n            try:\n                model_reduced, evidence = self._prune_bayesian_model(variables, evidence)\n            except AttributeError:\n                 model_reduced = self.model\n        else:\n            model_reduced = self.model\n\n        reduced_ve = self.__class__(model_reduced)\n        reduced_ve.trace_log = self.trace_log\n        reduced_ve.factor_semantics = self.factor_semantics\n        reduced_ve.global_do = self.global_do \n\n        # Manually initialize factors if parent doesn't (pgmpy quirk)\n        if not hasattr(reduced_ve, 'factors'):\n            reduced_ve.factors = {node: cpd.to_factor() for node, cpd in enumerate(model_reduced.cpds)}\n\n        return reduced_ve._variable_elimination(variables, \"marginalize\", evidence, elimination_order, joint, show_progress)\n\n    def query_with_trace(self, variables, evidence=None, elimination_order=\"MinFill\", joint=True):\n        self.trace_log = []\n        self.factor_semantics = {}\n        result = self.base_query(variables, evidence, elimination_order=elimination_order, joint=joint, show_progress=False)\n        return result, self.trace_log\n\n    def _variable_elimination(self, variables, operation, evidence=None, elimination_order=\"MinFill\", joint=True, show_progress=True):\n        # Stable loop implementation\n        if not variables:\n            return factor_product(*self.factors.values()) if joint else set(self.factors.values())\n\n        eliminated_variables = set()\n        working_factors = self._get_working_factors(evidence)\n        elimination_order = self._get_elimination_order(variables, evidence, elimination_order, show_progress)\n\n        self._map_factors_to_cpds(working_factors, evidence)\n        self.trace_log.append({\"step\": \"INIT\", \"elimination_order\": elimination_order})\n\n        for var in elimination_order:\n            factors = [f for f, _ in working_factors[var] if not set(f.variables).intersection(eliminated_variables)]\n\n            if not factors:\n                del working_factors[var]\n                eliminated_variables.add(var)\n                continue\n\n            input_data = []\n            for f in factors:\n                tracker = self.factor_semantics.get(id(f))\n                input_data.append((f, tracker))\n\n            phi = factor_product(*factors)\n            phi_tracker = input_data[0][1] if input_data and input_data[0][1] else SemanticTracker(heads={var}) # Fallback\n            for _, next_tracker in input_data[1:]:\n                if next_tracker: phi_tracker = phi_tracker.multiply(next_tracker)\n\n            self.factor_semantics[id(phi)] = phi_tracker\n\n            product_expr = \" * \".join([str(t) for _, t in input_data if t])\n            product_formula = str(phi_tracker)\n\n            tau = getattr(phi, operation)([var], inplace=False)\n            tau_tracker = phi_tracker.marginalize(var)\n            self.factor_semantics[id(tau)] = tau_tracker\n            sum_formula = str(tau_tracker)\n\n            self.trace_log.append({\n                \"step\": \"ELIMINATE\",\n                \"variable\": var,\n                \"input_data\": input_data,\n                \"product_data\": (phi, phi_tracker),\n                \"sum_data\": (tau, tau_tracker),\n                \"product_expr\": product_expr,\n                \"product_formula\": product_formula,\n                \"sum_formula\": sum_formula\n            })\n\n            del working_factors[var]\n            for variable in phi.variables:\n                 if variable in working_factors:\n                    working_factors[variable].add((tau, var))\n            for variable in tau.variables:\n                if variable not in working_factors:\n                    working_factors[variable] = set()\n                working_factors[variable].add((tau, var))\n            eliminated_variables.add(var)\n\n        final_factors_map = {}\n        for bucket in working_factors.values():\n            for factor, _ in bucket:\n                if not set(factor.variables).intersection(eliminated_variables):\n                    final_factors_map[id(factor)] = factor\n\n        unique_factors = list(final_factors_map.values())\n        final_input_data = []\n        for f in unique_factors:\n            tracker = self.factor_semantics.get(id(f))\n            final_input_data.append((f, tracker))\n\n        if unique_factors:\n            final_tracker = self.factor_semantics.get(id(unique_factors[0]))\n            # Handle edge case if tracker is None\n            if not final_tracker: final_tracker = SemanticTracker(heads=set(unique_factors[0].variables))\n\n            for f in unique_factors[1:]:\n                ft = self.factor_semantics.get(id(f))\n                if ft: final_tracker = final_tracker.multiply(ft)\n\n            unnormalized = factor_product(*unique_factors)\n            clean_tracker = final_tracker.project(unnormalized.variables)\n\n            if joint:\n                normalized = unnormalized.normalize(inplace=False)\n                self.trace_log.append({\n                    \"step\": \"NORMALIZE\",\n                    \"final_input_data\": final_input_data,\n                    \"unnormalized_data\": (unnormalized, clean_tracker),\n                    \"normalized_data\": (normalized, clean_tracker),\n                    \"final_formula\": str(clean_tracker)\n                })\n                return normalized\n            else:\n                return unnormalized\n        else:\n             return DiscreteFactor(variables=[], cardinality=[], values=[1.0])\n\n    # --- New Reporting Features (GRAFTED FROM NEW VERSION) ---\n\n    def generate_natural_language_proof(self, scientific_notation=False, precision=2, concise=True):\n        \"\"\"\n        Generates the proof.\n        :param concise: If True, returns the short unverbose CoT. If False, returns the detailed verbose trace.\n        \"\"\"\n        if concise: \n            return self._generate_concise_proof(scientific_notation, precision)\n        return self._generate_verbose_proof(scientific_notation, precision)\n\n    def _format_value(self, val, scientific=False, precision=4):\n        if scientific:\n            fmt = f\"{{:.{precision}e}}\"\n            s = fmt.format(val)\n            base, exponent = s.split('e')\n            if '.' in base: base = base.rstrip('0').rstrip('.')\n            if exponent.startswith('+') or exponent.startswith('-'):\n                sign = exponent[0]\n                num = exponent[1:].lstrip('0')\n                exponent = sign + (num if num else '0')\n            return f\"{base}e{exponent}\"\n        else:\n            if float(val).is_integer(): return str(int(val))\n            s = f\"{{:.{precision}f}}\".format(val)\n            if '.' in s: s = s.rstrip('0').rstrip('.')\n            return s\n\n    def _render_inline_dist(self, factor, precision=2):\n        \"\"\"Helper to render small distributions inline like {0: 0.2, 1: 0.8}.\"\"\"\n        vars = factor.variables\n        values = factor.values.flatten()\n        if len(vars) == 1:\n            card = factor.cardinality[0]\n            return \"{\" + \", \".join(f\"{i}: {round(values[i], precision)}\" for i in range(card)) + \"}\"\n        elif len(vars) == 0:\n            return f\"{round(values[0], precision)}\"\n\n        # For multi-var, just show first few or shape? \n        # Keeping it simple for concise mode:\n        return f\"[Distribution over {vars}]\"\n\n    def _render_factor_table(self, factor, semantics=None, scientific=False, precision=4):\n        name = str(semantics) if semantics else \"Unknown Factor\"\n        vars = factor.variables\n        values = factor.values.flatten()\n        card = factor.cardinality\n\n        if not vars:\n            val_str = self._format_value(values[0], scientific, precision)\n            return f\"Table for {name}:\\n  [] = {val_str}\"\n\n        states = [range(c) for c in card]\n        combinations = list(itertools.product(*states))\n        lines = [f\"Table for {name}:\"]\n        for i, comb in enumerate(combinations):\n            assignments = \", \".join([f\"{v}={s}\" for v, s in zip(vars, comb)])\n            val_str = self._format_value(values[i], scientific, precision)\n            lines.append(f\"  [{assignments}] = {val_str}\")\n        return \"\\n\".join(lines)\n\n    def _generate_concise_proof(self, scientific, precision):\n        lines = []\n        for entry in self.trace_log:\n            step = entry['step']\n            if step == 'GOAL':\n                lines.append(f\"Goal: {entry['description']}\")\n            elif step == 'SURGERY':\n                surgery_parts = []\n                for d in entry['details']:\n                    if \"Point Mass\" in d:\n                        surgery_parts.append(d.replace(\"Set P(\", \"P(\").replace(\") :=\", \")=\"))\n                    elif \"no parents\" not in d:\n                        surgery_parts.append(d)\n                if surgery_parts:\n                    lines.append(\"Surgery: \" + \"; \".join(surgery_parts))\n            elif step == 'INIT':\n                order = entry.get('elimination_order', [])\n                if order: lines.append(f\"Elim order: {order}\")\n            elif step == 'ELIMINATE':\n                var = entry['variable']\n                tau, _ = entry['sum_data']\n                result_str = self._render_inline_dist(tau, precision)\n                lines.append(f\"Sum out {var} -&gt; {entry['sum_formula']} = {result_str}\")\n            elif step == 'TRIVIAL_DO':\n                # Old version doesn't generate this, but keeping for compatibility\n                do_var, do_val, target = entry['do_var'], entry['do_val'], entry['target']\n                result = entry['result']\n                result_str = self._render_inline_dist(result, precision)\n                lines.append(f\"do({do_var}={do_val}) trivial -&gt; P({target}) = {result_str}\")\n            elif step == 'NORMALIZE':\n                n_factor, _ = entry['normalized_data']\n                u_factor, _ = entry['unnormalized_data']\n                u_sum = u_factor.values.sum()\n                result_str = self._render_inline_dist(n_factor, precision)\n                if abs(u_sum - 1.0) &lt; 1e-6:\n                    lines.append(f\"Result: {entry['final_formula']} = {result_str}\")\n                else:\n                    lines.append(f\"Normalize (sum={round(u_sum, precision)}) -&gt; {entry['final_formula']} = {result_str}\")\n        return \"\\n\".join(lines)\n\n    def _generate_verbose_proof(self, scientific, precision):\n        proof = []\n        for entry in self.trace_log:\n            text = self._render_trace_entry(entry, scientific, precision)\n            if text: proof.append(text)\n        return \"\\n\".join(proof)\n\n    def _render_trace_entry(self, entry, scientific, precision):\n        step = entry['step']\n        if step == 'GOAL':\n            return f\"Goal: {entry['description']}\\n\"\n        elif step == 'INIT':\n            return f\"Initialization: Selected Elimination Order = {entry['elimination_order']}\\n\"\n        elif step == 'ELIMINATE':\n            var = entry['variable']\n            lines = [f\"--- Step: Eliminate Variable '{var}' ---\"]\n            lines.append(f\"1. Retrieve relevant factors containing '{var}':\")\n            for (factor, tracker) in entry['input_data']:\n                table_str = self._render_factor_table(factor, tracker, scientific, precision)\n                lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            lines.append(f\"\\n2. Compute the Intermediate Joint (Product):\")\n            lines.append(f\"   Formula: {entry['product_formula']} = {entry['product_expr']}\")\n            phi, phi_track = entry['product_data']\n            table_str = self._render_factor_table(phi, phi_track, scientific, precision)\n            lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            lines.append(f\"\\n3. Marginalize (Sum) out variable '{var}':\")\n            lines.append(f\"   Formula: {entry['sum_formula']} = \\u2211_{{{var}}} {entry['product_formula']}\")\n            tau, tau_track = entry['sum_data']\n            table_str = self._render_factor_table(tau, tau_track, scientific, precision)\n            lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            lines.append(\"\\n\")\n            return \"\\n\".join(lines)\n        elif step == 'NORMALIZE':\n            lines = [f\"--- Final Step: Normalization ---\"]\n            lines.append(f\"1. Gather all remaining factors (Query Variables + Priors):\")\n            for (factor, tracker) in entry['final_input_data']:\n                table_str = self._render_factor_table(factor, tracker, scientific, precision)\n                lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            lines.append(f\"\\n2. Compute Unnormalized Joint Distribution ({entry['final_formula']}):\")\n            u_factor, u_track = entry['unnormalized_data']\n            table_str = self._render_factor_table(u_factor, u_track, scientific, precision)\n            lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            lines.append(f\"\\n3. Normalize to obtain Probability Distribution:\")\n            n_factor, n_track = entry['normalized_data']\n            table_str = self._render_factor_table(n_factor, n_track, scientific, precision)\n            lines.append(\"\\n\".join([\"   \" + line for line in table_str.split('\\n')]))\n            return \"\\n\".join(lines)\n        elif step == 'SURGERY':\n            lines = [\"--- Causal Graph Surgery ---\"]\n            for detail in entry.get('details', []):\n                lines.append(f\"  - {detail}\")\n            lines.append(\"\")\n            return \"\\n\".join(lines)\n        return None\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.SemanticTraceVE-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.SemanticTraceVE.generate_natural_language_proof","title":"<code>generate_natural_language_proof(scientific_notation=False, precision=2, concise=True)</code>","text":"<p>Generates the proof. :param concise: If True, returns the short unverbose CoT. If False, returns the detailed verbose trace.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def generate_natural_language_proof(self, scientific_notation=False, precision=2, concise=True):\n    \"\"\"\n    Generates the proof.\n    :param concise: If True, returns the short unverbose CoT. If False, returns the detailed verbose trace.\n    \"\"\"\n    if concise: \n        return self._generate_concise_proof(scientific_notation, precision)\n    return self._generate_verbose_proof(scientific_notation, precision)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning-functions","title":"Functions","text":""},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.bim_round","title":"<code>bim_round(self, n_round)</code>","text":"<p>In-place rounding for Binary Influence Models with shape correction.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def bim_round(self, n_round: int):\n    \"\"\"In-place rounding for Binary Influence Models with shape correction.\"\"\"\n    self.activation_magnitude = np.round(self.activation_magnitude, n_round)\n    if self.leak is not None:\n        self.leak = np.round(self.leak, n_round)\n\n    parent_states = [self.full_state_names[e] for e in self.evidence]\n    cols = [self._evaluate(dict(zip(self.evidence, c))) for c in product(*parent_states)]\n\n    # Create 2D array first\n    val_2d = np.array(cols).T\n    # Reshape to pgmpy's expected N-dimensional shape: (var_card, parent1_card, parent2_card...)\n    self.values = val_2d.reshape([self.variable_card] + self.cardinality[1:].tolist())\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.bn_round","title":"<code>bn_round(self, n_round)</code>","text":"<p>In-place rounding of all internal CPDs.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def bn_round(self, n_round: int):\n    \"\"\"In-place rounding of all internal CPDs.\"\"\"\n    for cpd in self.cpds:\n        cpd.round(n_round)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.cpd_round","title":"<code>cpd_round(self, n_round)</code>","text":"<p>In-place rounding of TabularCPD values.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def cpd_round(self, n_round: int):\n    \"\"\"In-place rounding of TabularCPD values.\"\"\"\n    original_shape = self.values.shape\n    if len(original_shape) &gt; 1:\n        flat_values = self.values.reshape(original_shape[0], -1)\n        for i in range(flat_values.shape[1]):\n            flat_values[:, i] = _precise_round_series(flat_values[:, i], n_round)\n        self.values = flat_values.reshape(original_shape)\n    else:\n        self.values = _precise_round_series(self.values, n_round)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.factor_to_dict","title":"<code>factor_to_dict(factor, n_round=2)</code>","text":"<p>Converts a 1D pgmpy posterior factor into a result dict.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def factor_to_dict(factor: TabularCPD, n_round: int = 2) -&gt; dict:\n    \"\"\"Converts a 1D pgmpy posterior factor into a result dict.\"\"\"\n    if len(factor.variables) != 1:\n        raise ValueError(\"Factor must be a 1D posterior distribution.\")\n\n    var = factor.variables[0]\n    states = factor.state_names[var]\n    values_rounded = [round(val, n_round) for val in  factor.values]\n    return _to_dict({state: float(val) for state, val in zip(states, values_rounded)})\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_CI","title":"<code>get_random_CI(variable, evidence, cardinality, mode='or', isLeaky=False, seed=None, mass_shift_rate=0.5, max_rejection_tries=100)</code>  <code>staticmethod</code>","text":"<p>Creates a random CI model using Rejection Sampling to guarantee stochastic dominance for MIMs.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>@staticmethod\ndef get_random_CI(\n    variable: Hashable,\n    evidence: List[Hashable],\n    cardinality: Dict[Hashable, int],\n    mode: str = 'or',\n    isLeaky: bool = False,\n    seed: Optional[int] = None,\n    mass_shift_rate: float = 0.5,\n    max_rejection_tries: int = 100,\n) -&gt; Union[\"BinaryInfluenceModel\", \"MultilevelInfluenceModel\"]:\n    \"\"\"\n    Creates a random CI model using Rejection Sampling to\n    guarantee stochastic dominance for MIMs.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    mode_upper = mode.upper()\n\n    child_card = cardinality[variable]\n    parent_cards = [cardinality[p] for p in evidence]\n    is_all_binary = (child_card == 2) and all(c == 2 for c in parent_cards)\n\n    if mode_upper in {\"OR\", \"AND\"}:\n        if not is_all_binary:\n            raise ValueError(f\"Mode '{mode_upper}' requires binary variables.\")\n        n_parents = len(evidence)\n        activation_magnitude = rng.uniform(0.05, 0.95, size=n_parents)\n        leak = rng.uniform(0.0, 0.1) if isLeaky else None\n\n        return BinaryInfluenceModel(\n            variable=variable,\n            evidence=evidence,\n            activation_magnitude=activation_magnitude,\n            mode=mode_upper,\n            leak=leak,\n            isboolean_style=False\n        )\n\n    elif mode_upper in {\"MAX\", \"MIN\"}:\n\n        influence_tables = {}\n        state_names = {}\n        state_names[variable] = list(range(child_card))\n\n        for parent in evidence:\n            parent_card = cardinality[parent]\n            parent_degree = parent_card * child_card\n            parent_states = list(range(parent_card))\n            state_names[parent] = parent_states\n            parent_table = {}\n\n            last_pmf = np.zeros(child_card)\n            last_pmf[0] = 1.0\n\n            pmf_list = []\n\n            current_alphas = (5/parent_card) * np.ones(child_card) / (1 + np.arange(1,child_card+1))\n            current_alphas[0] = 5.0\n\n            for state in parent_states:\n                if state == 0:\n                    current_pmf = last_pmf\n                else:\n                    next_alphas = current_alphas.copy()\n                    for i in range(child_card - 1):\n                        # Shift mass from bin i to bin i+1\n                        delta = next_alphas[i] * mass_shift_rate * (child_card - i) / parent_degree\n                        next_alphas[i] -= delta\n                        next_alphas[i+1] += delta\n\n                    next_alphas[next_alphas &lt; 0.1] = 0.1\n                    current_alphas = next_alphas\n\n                    found_sample = False\n                    for _ in range(max_rejection_tries):\n                        new_pmf = rng.dirichlet(current_alphas)\n                        if _is_stochastically_dominant(new_pmf, last_pmf):\n                            current_pmf = new_pmf\n                            found_sample = True\n                            break\n\n                    if not found_sample:\n                        current_pmf = _sample_dominant_fallback(last_pmf, rng)\n                pmf_list.append(current_pmf)\n                last_pmf = current_pmf\n\n            if mode_upper==\"MAX\":\n                parent_table = dict(zip(parent_states,pmf_list))\n            else: #MIN case\n                pmf_list.reverse()\n                pmf_list_flip = map(np.flip, pmf_list)\n                parent_table = dict(zip(parent_states,pmf_list_flip))\n            influence_tables[parent] = parent_table\n\n\n        leak_probs = None\n        if isLeaky:\n            alphas = np.ones(child_card)\n            leak_probs = rng.dirichlet(alphas)\n\n        return MultilevelInfluenceModel(\n            variable=variable,\n            evidence=evidence,\n            influence_tables=influence_tables,\n            levels=child_card,\n            leak=leak_probs,\n            mode=mode_upper,\n            state_names=state_names,\n        )\n    else:\n        raise ValueError(\n            f\"Unknown CI mode: '{mode}'. Must be one of 'OR', 'AND', 'MAX', 'MIN'.\"\n        )\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_DAG","title":"<code>get_random_DAG(n_nodes=5, edge_prob=0.5, node_names=None, latents=False, seed=None, method='erdos', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Returns a randomly generated DAG using different generation strategies.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>@staticmethod\ndef get_random_DAG(\n        n_nodes: int = 5,\n        edge_prob: float = 0.5,\n        node_names: Optional[list[Hashable]] = None,\n        latents: bool = False,\n        seed: Optional[int] = None,\n        method: str = \"erdos\",\n        **kwargs,\n    ) -&gt; \"DAG\":\n        \"\"\"\n        Returns a randomly generated DAG using different generation strategies.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        if node_names is None:\n            node_names = [f\"X_{i}\" for i in range(n_nodes)]\n\n        # ---- ERD\u0150S\u2013R\u00c9NYI STYLE DAG ----\n        if method == \"erdos\":\n            adj = rng.choice([0, 1], size=(n_nodes, n_nodes), p=[1 - edge_prob, edge_prob])\n            adj = np.triu(adj, k=1)  # ensure acyclicity\n            adj_pd = pd.DataFrame(adj, columns=node_names, index=node_names)\n            nx_dag = nx.from_pandas_adjacency(adj_pd, create_using=nx.DiGraph)\n\n        # ---- CONNECTED DAG VIA SPANNING TREE + NOISE ----\n        elif method == \"spanning_tree\":\n            order = rng.permutation(node_names)\n            adj = np.zeros((n_nodes, n_nodes), dtype=int)\n\n            # First, a directed spanning tree (acyclic)\n            for i in range(1, n_nodes):\n                parent = rng.integers(0, i)\n                adj[parent, i] = 1  # edge along order\n\n            # Add extra edges probabilistically (preserve DAG)\n            extra = rng.random((n_nodes, n_nodes)) &lt; edge_prob\n            adj = np.triu(np.logical_or(adj, extra), 1).astype(int)\n\n            adj_pd = pd.DataFrame(adj, columns=order, index=order)\n            nx_dag = nx.from_pandas_adjacency(adj_pd, create_using=nx.DiGraph)\n\n        # ---- DIRECTED PREFERENTIAL ATTACHMENT ----\n        elif method == \"preferential\":\n            m = kwargs.get(\"m\", 2)\n            G = nx.DiGraph()\n            G.add_nodes_from(node_names)\n            for new_idx in range(1, n_nodes):\n                existing = list(range(new_idx))\n                probs = np.array([G.in_degree(node_names[i]) + 1 for i in existing])\n                probs = probs / probs.sum()\n                n_parents = min(m, new_idx)\n                parents = rng.choice(existing, size=n_parents, replace=False, p=probs)\n                for p in parents:\n                    G.add_edge(node_names[p], node_names[new_idx])\n            nx_dag = G\n\n        # ---- LAYERED DAG ----\n        elif method == \"layered\":\n            n_layers = kwargs.get(\"n_layers\", int(np.sqrt(n_nodes)))\n            layer_conn_prob = kwargs.get(\"layer_conn_prob\", edge_prob)\n\n            layers = [[] for _ in range(n_layers)]\n            for node in node_names:\n                l = rng.integers(0, n_layers)\n                layers[l].append(node)\n\n            G = nx.DiGraph()\n            G.add_nodes_from(node_names)\n\n            for i in range(n_layers - 1):\n                src = layers[i]\n                for j in range(i + 1, n_layers):\n                    tgt = layers[j]\n                    for u in src:\n                        for v in tgt:\n                            if rng.random() &lt; layer_conn_prob:\n                                G.add_edge(u, v)\n            nx_dag = G\n\n        else:\n            raise ValueError(f\"Unknown DAG generation method '{method}'\")\n\n        # --- FIX: Force all nodes to be standard Python strings ---\n        # This prevents numpy.str_ types from causing \"Variable not in model\" errors\n        mapping = {n: str(n) for n in nx_dag.nodes()}\n        nx_dag = nx.relabel_nodes(nx_dag, mapping)\n        # --------------------------------------------------------\n\n        # ---- Add latent nodes optionally ----\n        dag = DAG(nx_dag)\n        if latents:\n            n_latents = rng.integers(low=1, high=max(2, len(dag.nodes())))\n            dag.latents = set(rng.choice(list(dag.nodes()), n_latents, replace=False))\n\n        return dag\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_DBN","title":"<code>get_random_DBN(n_nodes=5, edge_prob=0.5, node_names=None, n_states=None, latents=False, graph_seed=None, seed=None, method='erdos', n_round=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Returns a randomly generated Bayesian Network on <code>n_nodes</code> variables with edge probabiliy of <code>edge_prob</code> between variables.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_DBN--parameters","title":"Parameters","text":"<p>n_nodes: int     The number of nodes in the randomly generated DAG.</p> float <p>The probability of edge between any two nodes in the topologically sorted DAG.</p> list (default: None) <p>A list of variables names to use in the random graph. If None, the node names are integer values starting from 0.</p> int or dict (default: None) <p>The number of states of each variable in the form {variable: no_of_states}. If a single value is provided, all nodes will have the same number of states. When None randomly generates the number of states.</p> bool (default: False) <p>If True, also creates latent variables.</p> int (default: None) <p>The seed value for random number generators.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_DBN--returns","title":"Returns","text":"<p>Random DAG: pgmpy.base.DAG     The randomly generated DAG.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_random_DBN--examples","title":"Examples","text":"<p>from pgmpy.models import DiscreteBayesianNetwork model = DiscreteBayesianNetwork.get_random(n_nodes=5) model.nodes() NodeView((0, 1, 3, 4, 2)) model.edges() OutEdgeView([(0, 1), (0, 3), (1, 3), (1, 4), (3, 4), (2, 3)]) model.cpds [,  ,  ,  ,  ] Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>@staticmethod\ndef get_random_DBN(\n        n_nodes: int = 5,\n        edge_prob: float = 0.5,\n        node_names: Optional[List[Hashable]] = None,\n        n_states: Optional[Union[int, list[int], Dict[Hashable, int]]] = None,\n        latents: bool = False,\n        graph_seed: Optional[int] = None,\n        seed: Optional[int] = None,\n        method: str = \"erdos\",\n        n_round: Optional[int] = None,\n        **kwargs,\n    ) -&gt; \"DiscreteBayesianNetwork\":\n        \"\"\"\n        Returns a randomly generated Bayesian Network on `n_nodes` variables\n        with edge probabiliy of `edge_prob` between variables.\n\n        Parameters\n        ----------\n        n_nodes: int\n            The number of nodes in the randomly generated DAG.\n\n        edge_prob: float\n            The probability of edge between any two nodes in the topologically\n            sorted DAG.\n\n        node_names: list (default: None)\n            A list of variables names to use in the random graph.\n            If None, the node names are integer values starting from 0.\n\n        n_states: int or dict (default: None)\n            The number of states of each variable in the form\n            {variable: no_of_states}. If a single value is provided,\n            all nodes will have the same number of states. When None\n            randomly generates the number of states.\n\n        latents: bool (default: False)\n            If True, also creates latent variables.\n\n        seed: int (default: None)\n            The seed value for random number generators.\n\n        Returns\n        -------\n        Random DAG: pgmpy.base.DAG\n            The randomly generated DAG.\n\n        Examples\n        --------\n        &gt;&gt;&gt; from pgmpy.models import DiscreteBayesianNetwork\n        &gt;&gt;&gt; model = DiscreteBayesianNetwork.get_random(n_nodes=5)\n        &gt;&gt;&gt; model.nodes()\n        NodeView((0, 1, 3, 4, 2))\n        &gt;&gt;&gt; model.edges()\n        OutEdgeView([(0, 1), (0, 3), (1, 3), (1, 4), (3, 4), (2, 3)])\n        &gt;&gt;&gt; model.cpds\n        [&lt;TabularCPD representing P(0:0) at 0x7f97e16eabe0&gt;,\n         &lt;TabularCPD representing P(1:1 | 0:0) at 0x7f97e16ea670&gt;,\n         &lt;TabularCPD representing P(3:3 | 0:0, 1:1, 2:2) at 0x7f97e16820d0&gt;,\n         &lt;TabularCPD representing P(4:4 | 1:1, 3:3) at 0x7f97e16eae80&gt;,\n         &lt;TabularCPD representing P(2:2) at 0x7f97e1682c40&gt;]\n        \"\"\"\n        gen = np.random.default_rng(seed=graph_seed)\n        if node_names is None:\n            node_names = list([f\"X_{i}\" for i in range(n_nodes)])\n\n        if n_states is None:\n            n_states = gen.integers(low=1, high=5, size=n_nodes)\n            n_states_dict = {node_names[i]: n_states[i] for i in range(n_nodes)}\n\n        elif isinstance(n_states, int):\n            n_states = np.array([n_states] * n_nodes)\n            n_states_dict = {node_names[i]: n_states[i] for i in range(n_nodes)}\n\n        elif isinstance(n_states, list):\n            n_states = gen.choice(n_states, n_nodes)\n            n_states_dict = {node_names[i]: n_states[i] for i in range(n_nodes)}\n\n        elif isinstance(n_states, dict):\n            n_states_dict = n_states\n\n        dag = DAG.get_random(\n            n_nodes=n_nodes,\n            edge_prob=edge_prob,\n            node_names=node_names,\n            latents=latents,\n            seed=graph_seed,\n            method = method,\n            **kwargs,\n        )\n        bn_model = DiscreteBayesianNetwork(dag.edges(), latents=dag.latents)\n        bn_model.add_nodes_from(dag.nodes())\n\n        cpds = []\n        for node in bn_model.nodes():\n            parents = list(bn_model.predecessors(node))\n            cpd = TabularCPD.get_random(\n                    variable=node,\n                    evidence=parents,\n                    cardinality=n_states_dict,\n                    seed=seed,\n                )\n            if n_round != None:\n                cpd.round(n_round)\n            cpds.append(cpd)\n\n        bn_model.add_cpds(*cpds)\n        return bn_model\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.get_robust_elimination_order","title":"<code>get_robust_elimination_order(model, variables, evidence)</code>","text":"<p>Stable, manual implementation of MinFill to avoid pgmpy internals.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def get_robust_elimination_order(model, variables, evidence):\n    \"\"\"\n    Stable, manual implementation of MinFill to avoid pgmpy internals.\n    \"\"\"\n    all_nodes = set(model.nodes())\n    keep_nodes = set(variables) | set(evidence.keys())\n    to_eliminate = all_nodes - keep_nodes\n    if not to_eliminate: return []\n\n    moral_graph = nx.Graph()\n    moral_graph.add_nodes_from(all_nodes)\n    moral_graph.add_edges_from(model.to_undirected().edges())\n\n    for node in model.nodes():\n        parents = list(model.predecessors(node))\n        for i in range(len(parents)):\n            for j in range(i + 1, len(parents)):\n                moral_graph.add_edge(parents[i], parents[j])\n\n    ordering = []\n    working_graph = moral_graph.copy()\n    remaining_nodes = list(to_eliminate)\n\n    while remaining_nodes:\n        best_node = None\n        min_fill = float('inf')\n\n        for node in remaining_nodes:\n            neighbors = list(working_graph.neighbors(node))\n            fill_edges = 0\n            n_neighbors = len(neighbors)\n            for i in range(n_neighbors):\n                for j in range(i + 1, n_neighbors):\n                    if not working_graph.has_edge(neighbors[i], neighbors[j]):\n                        fill_edges += 1\n            if fill_edges &lt; min_fill:\n                min_fill = fill_edges\n                best_node = node\n                if min_fill == 0: break\n\n        ordering.append(best_node)\n        remaining_nodes.remove(best_node)\n\n        neighbors = list(working_graph.neighbors(best_node))\n        for i in range(len(neighbors)):\n            for j in range(i + 1, len(neighbors)):\n                working_graph.add_edge(neighbors[i], neighbors[j])\n        working_graph.remove_node(best_node)\n\n    return ordering\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.js_divergence","title":"<code>js_divergence(d1, d2)</code>","text":"<p>Compute the Jensen-Shannon divergence between two discrete probability distributions.</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def js_divergence(d1, d2):\n    \"\"\"\n    Compute the Jensen-Shannon divergence between two discrete probability distributions.\n    \"\"\"\n    keys = set(d1.keys()).union(set(d2.keys()))\n    if not keys:\n        return 0.0\n\n    p = [d1.get(k, 0.0) for k in keys]\n    q = [d2.get(k, 0.0) for k in keys]\n\n    p_sum = sum(p)\n    q_sum = sum(q)\n    if p_sum &gt; 0: p = [v / p_sum for v in p]\n    if q_sum &gt; 0: q = [v / q_sum for v in q]\n\n    m = [(p[i] + q[i]) / 2 for i in range(len(keys))]\n\n    def kl_divergence(a, b):\n        return sum(a_i * log(a_i / b_i, 2) for a_i, b_i in zip(a, b) if a_i &gt; 0 and b_i &gt; 0)\n\n    js = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n    return js\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.js_reward","title":"<code>js_reward(dg, dt, power=128)</code>","text":"<p>reward of guessing dg where the true distribution is dt</p> Source code in <code>reasoning_core/tasks/causal_reasoning.py</code> <pre><code>def js_reward(dg, dt, power=128):\n    \"\"\"reward of guessing dg where the true distribution is dt\"\"\"\n    js = js_divergence(dg, dt)\n    return (1 - js / log(2)) ** power\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.mim_round","title":"<code>mim_round(self, n_round)</code>","text":"<p>In-place rounding for Multilevel Influence Models with shape correction.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def mim_round(self, n_round: int):\n    \"\"\"In-place rounding for Multilevel Influence Models with shape correction.\"\"\"\n    for parent in self.influence_tables:\n        for val in self.influence_tables[parent]:\n            self.influence_tables[parent][val] = _precise_round_series(\n                self.influence_tables[parent][val], n_round\n            )\n\n    if self.leak is not None:\n        self.leak = _precise_round_series(self.leak, n_round)\n\n    self.cumulative_tables = {\n        p: {v: np.cumsum(probs) for v, probs in table.items()}\n        for p, table in self.influence_tables.items()\n    }\n    if self.leak is not None:\n        self.cumulative_leak = np.cumsum(self.leak)\n\n    parent_states = [self.full_state_names[p] for p in self.evidence]\n    cols = [self._evaluate(dict(zip(self.evidence, c))) for c in product(*parent_states)]\n\n    # Create 2D array first\n    val_2d = np.vstack(cols).T\n    # Reshape to N-dimensional shape\n    self.values = val_2d.reshape([self.variable_card] + self.cardinality[1:].tolist())\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_BIM","title":"<code>to_nl_BIM(self, n_round=4, verbose=False)</code>","text":"<p>Binary Influence Models to Natural Language method.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_BIM--parameters","title":"Parameters","text":"Name Type Default Description 'n_round' int 4 How many digits will be print 'verbose' bool False Verbose version or very minimalistic and probabilistic description Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def to_nl_BIM(self, n_round: int = 4, verbose = False) -&gt; List[str]:\n    \"\"\"Binary Influence Models to Natural Language method.\n\n        ## Parameters\n\n        | Name | Type | Default | Description |\n        |----|----|----|----|\n        | 'n_round' | int | 4 | How many digits will be print |\n        | 'verbose' | bool | False | Verbose version or very minimalistic and probabilistic description |\n        \"\"\"\n    if self.mode.upper() not in {'OR', 'AND'}:\n        return ValueError(f\"\"\"The mode of the interaction must be 'OR' or 'AND' but is instead {self.mode.upper()} \"\"\")\n    if verbose:\n        descriptions = [] \n        if self.mode.upper() == 'OR':\n            model_name = \"Noisy-OR\"\n            action_verb = \"activate\"\n            base_action = \"activation\"\n        else:\n            model_name = \"Noisy-AND\"\n            action_verb = \"inhibit\"\n            base_action = \"inhibition\"\n\n        active_state = repr(self.state_names[self.variable][1]) \n\n        base_desc = (\n            f\"The variable {self.variable} is \"\n            f\"influenced by its parents: {', '.join(self.evidence)}.\"\n        )\n        descriptions.append(base_desc)\n\n        concept_desc = (\n            f\"Each parent may activate independently {self.variable}, but they globally {action_verb} it.\"\n        )\n        descriptions.append(concept_desc)\n        descriptions.append(\"The specific contributions are:\")\n\n        for parent, prob in zip(self.evidence, self.activation_magnitude): \n            parent_active = repr(self.state_names[parent][1]) \n            prob_rounded = round(prob, n_round) \n\n            descriptions.append( \n                f\"  - When {parent} is active (is equal to {parent_active}), it has a \"\n                f\"{prob_rounded} probability of successfully causing \"\n                f\"the activation of {self.variable}.\"\n            ) \n\n        if self.isleaky: \n            leak_prob = round(self.leak[0], n_round) \n            leak_desc_human = (\n                f\"  - There is also a base-line probability (or 'leak'). \"\n                f\"If all parents are inactive, the probability of {self.variable} \"\n                f\"being '{active_state}' is {leak_prob}.\"\n            )\n            descriptions.append(leak_desc_human)\n\n        return descriptions\n    else: #minimalist setting\n        active_state = self.state_names[self.variable][-1]\n        op = self.mode.upper()\n\n        weights = {}\n        for parent, mag in zip(self.evidence, self.activation_magnitude):\n            p_active = self.state_names[parent][-1]\n            weights[f\"{parent}\"] = round(float(mag), n_round)\n\n        leak_val = round(float(self.leak[0]), n_round) if self.isleaky else 0.0\n        return [f\"{self.variable} ~ Noisy-{op}(leak={leak_val}, weights={weights})\"]\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_CPD","title":"<code>to_nl_CPD(self, n_round=4, verbose=True)</code>","text":"<p>Converts TabularCPD to a natural language description.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_CPD--parameters","title":"Parameters","text":"Name Type Default Description 'n_round' int 4 How many digits will be print 'verbose' bool False Verbose version or very minimalistic and probabilistic description Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def to_nl_CPD(self, n_round: int = 4, verbose = True) -&gt; List[str]:\n        \"\"\"Converts TabularCPD to a natural language description.\n\n        ## Parameters\n\n        | Name | Type | Default | Description |\n        |----|----|----|----|\n        | 'n_round' | int | 4 | How many digits will be print |\n        | 'verbose' | bool | False | Verbose version or very minimalistic and probabilistic description |\n        \"\"\"\n        if verbose:\n            descriptions = []\n            self.evidence = self.variables[1:]\n            if len(self.evidence) == 0:\n                self.evidence = None\n\n            if self.evidence:\n                parent_vars = self.evidence\n                parent_states = [self.state_names[parent] for parent in parent_vars]\n                parent_combos = list(product(*parent_states))\n            else:\n                parent_vars = []  \n                parent_combos = [()]\n\n            child_states = self.state_names[self.variable]\n            probs_table = self.get_values()\n\n            for i, combo in enumerate(parent_combos):\n\n                cond_parts = [\n                    f\"{var} = {repr(val)}\" for var, val in zip(parent_vars, combo)\n                ]\n                cond_desc = \" and \".join(cond_parts)\n\n                probs_col = probs_table[:, i]\n                prob_list = []\n                for j, state in enumerate(child_states):\n                    prob = round(probs_col[j], n_round)\n                    prob_list.append((state, prob))\n\n                prob_text = \" and \".join(\n                    f\"The probability of {self.variable} = {repr(val)} is {prob}\"\n                    for val, prob in prob_list\n                )\n\n                if cond_desc:\n                    descriptions.append(f\"If {cond_desc}, then {prob_text}.\")\n                else:\n                    descriptions.append(f\"{prob_text}.\")\n\n            return descriptions\n\n\n        else: #not verbose case, fully conditional probabilistic version\n            self.evidence = self.variables[1:] or None\n            child_states = self.state_names[self.variable]\n            probs_table = self.get_values()\n\n            if not self.evidence:\n                probs = {repr(s): float(round(probs_table[j, 0], n_round)) for j, s in enumerate(child_states)}\n                return [f\"P({self.variable}) = {probs}\"]\n\n            lines = []\n            parent_combos = list(product(*[self.state_names[p] for p in self.evidence]))\n            for i, combo in enumerate(parent_combos):\n                cond = \", \".join(f\"{p}={repr(v)}\" for p, v in zip(self.evidence, combo))\n                probs = {repr(s): float(round(probs_table[j, i], n_round)) for j, s in enumerate(child_states)}\n                lines.append(f\"P({self.variable}|{cond}) = {probs}\")\n            return lines\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_DBN","title":"<code>to_nl_DBN(self, n_round=4, verbose=True)</code>","text":"<p>Converts the entire Bayesian Network into a Natural Language description.</p> <p>It iterates over each node and calls its CPD's .to_NL() method.</p> Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def to_nl_DBN(\n        self, n_round: int = 4, verbose: bool = True\n    ) -&gt; str:\n        \"\"\"\n        Converts the entire Bayesian Network into a Natural Language description.\n\n        It iterates over each node and calls its CPD's .to_NL() method.\n        \"\"\"\n        full_description = []\n\n        for cpd in self.get_cpds():\n            node_nl_lines = cpd.to_nl(\n                n_round=n_round,\n                verbose = verbose,\n            )\n            full_description.extend(node_nl_lines)\n\n        return \" \\n\".join(full_description)\n</code></pre>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_MIM","title":"<code>to_nl_MIM(self, n_round=4, verbose=False)</code>","text":"<p>Multivariate Influence Models to Natural Language method.</p>"},{"location":"tasks/causal_reasoning/#reasoning_core.tasks.causal_reasoning.to_nl_MIM--parameters","title":"Parameters","text":"Name Type Default Description 'n_round' int 4 How many digits will be print 'verbose' bool False Verbose version or very minimalistic and probabilistic description Source code in <code>reasoning_core/tasks/_causal_utils.py</code> <pre><code>def to_nl_MIM(self, n_round: int = 4, verbose = False) -&gt; List[str]:\n    \"\"\"Multivariate Influence Models to Natural Language method.\n\n        ## Parameters\n\n        | Name | Type | Default | Description |\n        |----|----|----|----|\n        | 'n_round' | int | 4 | How many digits will be print |\n        | 'verbose' | bool | False | Verbose version or very minimalistic and probabilistic description |\n        \"\"\"\n    if verbose:\n        descriptions = []\n        model_type = f\"Noisy-{self.mode.upper()}\"\n        child_states = self.state_names[self.variable]\n\n        base_desc = (\n            f\"The variable {self.variable} is the result of a {model_type} interaction due to its parents \"\n            f\"({', '.join(self.evidence)}).\"\n        )\n        descriptions.append(base_desc)\n\n        for parent in self.evidence:\n            parent_states = self.state_names[parent]\n            for state in parent_states:\n                probs = self.influence_tables[parent][state]                    \n                prob_parts = []\n                for i, prob in enumerate(probs):\n                    if prob &gt; 1e-6:\n                        prob_rounded = round(prob, n_round)\n                        prob_parts.append(f\"P({self.variable}={repr(child_states[i])})={prob_rounded}\")\n\n                prob_desc = \", \".join(prob_parts)\n                descriptions.append(\n                    f\"  - The influence of {parent} = {repr(state)} (when others are inactive) is: [{prob_desc}].\"\n                )\n\n        if self.isleaky:\n            prob_parts = []\n            for i, prob in enumerate(self.leak):\n                if prob &gt; 1e-6:\n                    prob_rounded = round(prob, n_round)\n                    prob_parts.append(f\"P({self.variable}={repr(child_states[i])})={prob_rounded}\")\n            prob_desc = \", \".join(prob_parts)\n            descriptions.append(\n                f\"  - The leak distribution (when all parents are inactive) is: [{prob_desc}].\"\n            )\n\n        return descriptions\n    else: #unverbose one\n        op = self.mode.upper()\n\n        if self.isleaky:\n            leak_dist = [round(float(x), n_round) for x in self.leak]\n        else:\n            leak_dist = \"None\"\n\n        influences = {}\n\n        for parent in self.evidence:\n            parent_states = self.state_names[parent]\n            parent_effects = {}\n            for state in parent_states[1:]:\n                dist = self.influence_tables[parent][state]\n                dist_fmt = [round(float(x), n_round) for x in dist]\n                parent_effects[repr(state)] = dist_fmt\n\n            influences[parent] = parent_effects\n\n        return [f\"{self.variable} ~ Noisy-{op}(leak={leak_dist}, influences={influences})\"]\n</code></pre>"},{"location":"tasks/coding/","title":"Coding","text":""},{"location":"tasks/coding/#reasoning_core.tasks.coding","title":"<code>reasoning_core.tasks.coding</code>","text":""},{"location":"tasks/coding/#reasoning_core.tasks.coding-classes","title":"Classes","text":""},{"location":"tasks/coding/#reasoning_core.tasks.coding-functions","title":"Functions","text":""},{"location":"tasks/coding/#reasoning_core.tasks.coding.get_git_diff","title":"<code>get_git_diff(src_lines, tgt_lines)</code>","text":"<p>Generates a standard Git-style unified diff without file headers.</p> Source code in <code>reasoning_core/tasks/coding.py</code> <pre><code>def get_git_diff(src_lines, tgt_lines):\n    \"\"\"Generates a standard Git-style unified diff without file headers.\"\"\"\n    diff = difflib.unified_diff(src_lines, tgt_lines, lineterm='')\n    # Strip the first two lines (--- and +++) to leave only chunks\n    return \"\\n\".join(list(diff)[2:])\n</code></pre>"},{"location":"tasks/coding/#reasoning_core.tasks.coding.get_short_hash","title":"<code>get_short_hash()</code>","text":"<p>Generates a git-style short hash (7 chars).</p> Source code in <code>reasoning_core/tasks/coding.py</code> <pre><code>def get_short_hash():\n    \"\"\"Generates a git-style short hash (7 chars).\"\"\"\n    r = str(random.random()).encode('utf-8')\n    return hashlib.sha1(r).hexdigest()[:7]\n</code></pre>"},{"location":"tasks/coding/#reasoning_core.tasks.coding.mutate_lines","title":"<code>mutate_lines(lines, vocab, rate)</code>","text":"<p>Evolves a list of lines (sentences).</p> Source code in <code>reasoning_core/tasks/coding.py</code> <pre><code>def mutate_lines(lines, vocab, rate):\n    \"\"\"Evolves a list of lines (sentences).\"\"\"\n    new_lines = []\n\n    for line in lines:\n        r = random.random()\n        if r &lt; rate / 5:       # Delete entire line\n            continue\n        elif r &lt; rate:         # Modify line\n            new_lines.append(mutate_words_in_line(line, vocab, rate)) \n        elif r &lt; rate * 1.2:   # Insert new line\n            new_lines.append(\" \".join(fake.words(nb=5)))\n            new_lines.append(line)\n        else:\n            new_lines.append(line)\n\n    if not new_lines:\n        new_lines.append(\" \".join(fake.words(nb=5)))\n\n    return new_lines\n</code></pre>"},{"location":"tasks/coding/#reasoning_core.tasks.coding.mutate_words_in_line","title":"<code>mutate_words_in_line(line, vocab, rate)</code>","text":"<p>Mutates words within a single string (line).</p> Source code in <code>reasoning_core/tasks/coding.py</code> <pre><code>def mutate_words_in_line(line, vocab, rate):\n    \"\"\"Mutates words within a single string (line).\"\"\"\n    words = line.split()\n    if not words: return line\n\n    if random.random() &gt; rate:\n        return line\n\n    new_words = []\n    for word in words:\n        r = random.random()\n        if r &lt; 0.05:   # Delete word\n            continue\n        elif r &lt; 0.15: # Substitute word\n            new_words.append(random.choice(vocab))\n        else:\n            new_words.append(word)\n\n    if not new_words and words:\n        new_words = [random.choice(vocab)]\n\n    return \" \".join(new_words)\n</code></pre>"},{"location":"tasks/equation_system/","title":"Equation System","text":""},{"location":"tasks/equation_system/#reasoning_core.tasks.equation_system","title":"<code>reasoning_core.tasks.equation_system</code>","text":""},{"location":"tasks/equation_system/#reasoning_core.tasks.equation_system-classes","title":"Classes","text":""},{"location":"tasks/equation_system/#reasoning_core.tasks.equation_system.EquationSystem","title":"<code>EquationSystem</code>","text":"<p>               Bases: <code>Task</code></p> Source code in <code>reasoning_core/tasks/equation_system.py</code> <pre><code>class EquationSystem(Task):\n    def __init__(self, config=EquationSystemCfg()):\n        super().__init__(config=config)\n\n    def _generate_base_system(self) -&gt; Tuple[List[sp.Eq], List[sp.Symbol], Dict[sp.Symbol, int]]:\n        \"\"\"Generates a unique system by construction.\"\"\"\n        cfg = self.config\n        # Capture dimension once to keep all arrays/loops in sync\n        n = int(cfg.num_vars)\n        if n &lt; 2:\n            return [], [], {}\n\n        variables = list(sp.symbols(f'X1:{n + 1}'))\n        sol_map = {v: randint_nonzero(-cfg.sol_magnitude, cfg.sol_magnitude) for v in variables}\n        base_exprs = [v - sol_map[v] for v in variables]\n\n        C = [[int(i == j) for j in range(n)] for i in range(n)]\n        for _ in range(n * cfg.obfuscation_steps):\n            i, j = random.sample(range(n), 2)\n            k = randint_nonzero(-cfg.coeff_magnitude // 2, cfg.coeff_magnitude // 2)\n            for col in range(n):\n                C[i][col] += k * C[j][col]\n\n        if random.random() &lt; cfg.p_shortcut:\n            row_to_simplify = random.randrange(n)\n            col_to_keep = random.randrange(n)\n            C[row_to_simplify] = [int(j == col_to_keep) for j in range(n)]\n\n        mixed_exprs = [sp.expand(sum(C[i][j] * base_exprs[j] for j in range(n))) for i in range(n)]\n        return [sp.Eq(expr, 0) for expr in mixed_exprs], variables, sol_map\n\n    def generate(self) -&gt; Problem:\n        for _ in range(self.config.max_generation_attempts):\n            eqs, variables, sol_map = self._generate_base_system()\n            if not eqs: continue\n\n            # Probabilistically modify the base system\n            rand_val = random.random()\n            was_modified = False\n            if rand_val &lt; self.config.p_inconsistent:\n                i, j = random.sample(range(len(eqs)), 2)\n                eqs[j] = sp.Eq(eqs[i].lhs, eqs[i].rhs + randint_nonzero(-10, 10))\n                was_modified = True\n            elif rand_val &lt; self.config.p_inconsistent + self.config.p_underdetermined:\n                eqs.pop(random.randrange(len(eqs)))\n                was_modified = True\n\n            verification = _verify_system(eqs, variables)\n            case = verification['kind']\n            if case == 'error': continue\n\n            query_var = random.choice(variables)\n            answer = None\n\n            if case == 'unique':\n                if was_modified: continue\n                answer = sol_map[query_var]\n            elif case == 'inconsistent':\n                answer = \"No solution\"\n            elif case == 'underdetermined':\n                var_idx = variables.index(query_var)\n                sol_expr = next(iter(verification['solutions']))[var_idx]\n                if not sol_expr.free_symbols:\n                    answer = sp.N(sol_expr)\n                    case = \"underdetermined_but_unique_var\"\n                else:\n                    answer = \"Multiple solutions\"\n\n            if answer is None: continue\n\n            metadata = {\n                \"equations\": [f\"{eq.lhs} = {eq.rhs}\" for eq in eqs],\n                \"query_variable\": str(query_var),\n                \"full_solution_map\": {str(k): int(v) for k, v in sol_map.items()} if not was_modified else None,\n                \"case\": case,\n                \"cot\": self.get_cot(eqs, variables)\n            }\n            a = str(answer)\n            if \".\" in a:\n                a = a.rstrip(\"0\").rstrip(\".\")\n            return Problem(metadata=metadata, answer=a)\n\n        raise RuntimeError(f\"Failed to generate a valid problem. Config: {self.config}\")\n\n    def prompt(self, metadata: dict) -&gt; str:\n        eq_block = \"\\n\".join([f\"  {eq_str}\" for eq_str in metadata['equations']])\n        return (f\"Solve the following system of equations for the variable '{metadata['query_variable']}'.\\n\\n\"\n                f\"System:\\n{eq_block}\\n\\n\"\n                f\"Return the numerical value for {metadata['query_variable']}. If a unique numerical solution does not exist, \"\n                \"return either 'No solution' or 'Multiple solutions'.\")\n\n\n    def score_answer(self, answer, entry) -&gt; float:\n        normalize = lambda text: str(text).split('=')[-1].lower().strip().replace('_', ' ').replace('-', ' ')\n        a = normalize(answer)\n        if \"solution\" in a:\n            return float(a==normalize(entry.answer))\n        if \"solution\" in entry.answer.lower():\n            return 0.0\n        return score_scalar(answer, entry)\n\n\n    def get_cot(self, eqs, variables):\n            try:\n                A, b = sp.linear_eq_to_matrix(eqs, variables)\n            except ValueError: return None\n\n            M = A.row_join(b)\n            log = [\"1. Forward:\"]\n            piv = 0\n\n            # Phase 1: Forward\n            for c in range(len(variables)):\n                if piv &gt;= M.rows: break\n                if M[piv, c] == 0:\n                    if (swap := next((r for r in range(piv+1, M.rows) if M[r, c]), None)) is None: continue\n                    M.row_swap(piv, swap); log.append(f\"Swap R{piv+1}, R{swap+1}\")\n\n                for r in range(piv+1, M.rows):\n                    if k := M[r, c] / M[piv, c]:\n                        M.row_op(r, lambda x, i: x - k * M[piv, i])\n                        log.append(f\"R{r+1} -= {float(k):g}*R{piv+1}\")\n                piv += 1\n\n            # Phase 2: Backward &amp; Check\n            log.append(\"\\n2. Backward:\")\n            sol = {}\n            for i in range(M.rows-1, -1, -1):\n                row = M.row(i)\n                # Check 0=k contradiction\n                if all(x==0 for x in row[:-1]):\n                    if abs(row[-1]) &gt; 1e-9: return f\"Contradiction: 0 != {float(row[-1]):g}\"\n                    continue\n\n                # Identify pivot variable for this row\n                p_idx = next(j for j, x in enumerate(row[:-1]) if x)\n                var = variables[p_idx]\n\n                # Check for skipped variables (free)\n                # (If the previous variable we solved was p_idx+2, then p_idx+1 is free)\n                # For simplicity in concise code, we just rely on 'sol' defaults.\n\n                val = (row[-1] - sum(row[j]*sol.get(variables[j],0) for j in range(p_idx+1, len(variables)))) / row[p_idx]\n                sol[var] = val\n                log.append(f\"{var} = {float(val):g}\")\n\n            return \"\\n\".join(log)\n</code></pre>"},{"location":"tasks/formal_maths/","title":"Formal Maths","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths","title":"<code>reasoning_core.tasks.formal_maths</code>","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths-classes","title":"Classes","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.ConjectureEntailment","title":"<code>ConjectureEntailment</code>","text":"<p>               Bases: <code>Task</code></p> <p>A task that generates problems to determine if a set of hypotheses proves a given conjecture.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>class ConjectureEntailment(Task):\n    \"\"\"\n    A task that generates problems to determine if a set of hypotheses\n    proves a given conjecture.\n    \"\"\"\n    def __init__(self, config=EntailConfig()):\n        super().__init__(config)\n        # Initialize prover session at task init (pulls docker image if needed)\n        # This ensures docker setup happens before any generation timing\n        from reasoning_core.utils.udocker_process import initialize_prover_session\n        initialize_prover_session()\n\n    def _initialize_graph(self):    \n        for _ in range(100):\n            axiom_file_path, axiom_file_name = get_random_tptp_axioms(prefixes=self.config.domains)\n\n            if axiom_file_path:\n                self.axiom_set = axiom_file_name\n            self.graph = generate_derivation_graph( \n                    axiom_file = axiom_file_path, \n                    save_output=False, \n                    ranking=True, \n                    e_limit=2\n                )\n            if os.path.exists(axiom_file_path):\n                os.remove(axiom_file_path)\n\n\n            self.all_formulas = [data['data'].clause_formula for _, data in self.graph.nodes(data=True)]\n            self.interesting_thm = []\n\n            for i in self.graph.nodes() : \n                if self.graph.nodes[i]['data'].interesting_score &gt; self.config.min_interesting_score and self.graph.in_degree(i) &gt; 1 :\n                    self.interesting_thm.append(i)\n            if len(self.interesting_thm) &gt;= 5 :\n                break\n\n    def generate(self):\n        self._initialize_graph()\n\n        while True :\n\n            theorem_node_id = random.choice(list(self.interesting_thm))\n            correct_hypotheses, theorem = extract_problem_from_graph(self.graph, theorem_node_id, self.config.proof_depth)\n            useful_axioms = extract_useful_axioms(self.graph, theorem_node_id)\n            useful_axioms_formula = [self.graph.nodes[node]['data'].full_cnf_clause for node in useful_axioms]\n            if random.random() &lt; self.config.positive_problem_ratio:\n                hypotheses = correct_hypotheses\n                if prove_conjecture(hypotheses, theorem) is not True:\n                    continue\n                answer = True \n            else:\n                distraction_pool = list(set(self.all_formulas) - {theorem})\n                hypotheses = perturb_list(correct_hypotheses, distraction_pool ,self.config.perturbation)\n                try:\n                    answer = prove_conjecture(hypotheses, theorem)\n                except TimeoutError:\n                    continue\n\n            if isinstance(answer, bool):\n                metadata = edict({'hypotheses': hypotheses,\n                            'conjecture': theorem,\n                            'correct_hypotheses': correct_hypotheses ,\n                            'proof_depth' : self.config.proof_depth,\n                            'perturbation' : self.config.perturbation ,\n                            'useful_axioms' : useful_axioms_formula,\n                            'axiom_set' : self.axiom_set})\n                return Problem(metadata, str(answer))\n\n    def prompt(self, metadata):\n\n        hypotheses_text = \"\\n\".join([f\"- {h}\" for h in metadata['hypotheses']])\n        domain_name = DOMAIN_MAP.get(metadata['axiom_set'][:3], metadata['axiom_set'])\n\n        return (\n            f\"Decide if the given premises entail the conjecture (i.e., the conjecture is provable) \"\n            f\"using Superposition/Resolution/Paramodulation.\\n\\n\"\n            f\"Domain: {domain_name}\\n\\n\"\n            f\"Premises:\\n{hypotheses_text}\\n\\n\"\n            f\"Conjecture: `{metadata['conjecture']}`\\n\\n\"\n            f\"Output only `True` (provable) or `False` (not provable).\"\n        )\n\n    def score_answer(self, answer, entry):\n        ref = entry.answer.lower()\n        pred = str(answer).lower().strip().strip('\"').strip(\"'\")\n        return float(ref==pred)\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.ProofReconstruction","title":"<code>ProofReconstruction</code>","text":"<p>               Bases: <code>Task</code></p> <p>A task that generates problems where one must reconstruct the derivation graph from a numbered list of shuffled clauses.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>class ProofReconstruction(Task):\n    \"\"\"\n    A task that generates problems where one must reconstruct the derivation\n    graph from a numbered list of shuffled clauses.\n    \"\"\"\n    def __init__(self, config=ReconstructionConfig()):\n        super().__init__(config)\n        # Initialize prover session at task init\n        from reasoning_core.utils.udocker_process import initialize_prover_session\n        initialize_prover_session()\n\n    _initialize_graph = ConjectureEntailment._initialize_graph\n\n\n    def generate(self):\n\n        self._initialize_graph()\n        useless_axioms = {n for n, d in self.graph.in_degree() if d == 0}\n\n        redundant_children = set()\n        for ax_id in useless_axioms:\n            if self.graph.out_degree(ax_id) == 1:\n                child_id = list(self.graph.successors(ax_id))[0]\n                if self.graph.nodes[ax_id]['data'].clause_formula == self.graph.nodes[child_id]['data'].clause_formula:\n                    redundant_children.add(child_id)\n        nodes_to_remove = useless_axioms.union(redundant_children)\n\n        self.graph.remove_nodes_from(nodes_to_remove)\n\n        all_axioms = {node for node, in_degree in self.graph.in_degree() if in_degree == 0}\n\n        interesting_theorems = self.interesting_thm\n\n        valid_paths = []\n        for theorem_id in interesting_theorems:\n            ancestor_axioms = nx.ancestors(self.graph, theorem_id) &amp; all_axioms\n\n            for axiom_id in ancestor_axioms:\n                path_length = nx.shortest_path_length(self.graph, source=axiom_id, target=theorem_id)\n\n                if 0 &lt; path_length &lt;= self.config.proof_depth:\n\n                    proof_nodes = nx.ancestors(self.graph, theorem_id)\n                    proof_nodes.add(theorem_id)\n                    num_nodes = len(proof_nodes)\n                    min_size = 2**(self.config.proof_depth) - 1\n                    max_size = 2**(self.config.proof_depth+1) - 1\n\n                    if min_size &lt; num_nodes &lt;= max_size:\n\n                        is_binary = all(\n                            self.graph.in_degree(n) in (0, 2) for n in proof_nodes\n                        )\n\n                        if is_binary:\n                            valid_paths.append((axiom_id, theorem_id))\n                            break \n\n        if not valid_paths:\n            return None\n\n        axiom_id, theorem_node_id = random.choice(valid_paths)\n\n        proof_nodes = nx.ancestors(self.graph, theorem_node_id)\n        proof_nodes.add(theorem_node_id)\n        proof_graph = self.graph.subgraph(proof_nodes)\n\n        all_clauses_in_proof = [data['data'].clause_formula for _, data in proof_graph.nodes(data=True)]\n        random.shuffle(all_clauses_in_proof)\n        theorem_formula = self.graph.nodes[theorem_node_id]['data'].clause_formula\n\n        proof_structure_indices = []\n\n        for node_id in proof_graph.nodes():\n            parents = list(proof_graph.predecessors(node_id))\n            if parents:  \n                child_formula = proof_graph.nodes[node_id]['data'].clause_formula\n                parent_formulas = [proof_graph.nodes(data=True)[p]['data'].clause_formula for p in parents]\n\n                child_idx = all_clauses_in_proof.index(child_formula) + 1\n                parent_indices = sorted([all_clauses_in_proof.index(p) + 1 for p in parent_formulas])\n\n                proof_structure_indices.append(f\"{child_idx} &lt;- {', '.join(map(str, parent_indices))}\")\n\n        proof_structure_ids = [f\"{node} &lt;- {', '.join(sorted(list(proof_graph.predecessors(node))))}\" for node in proof_graph.nodes() if proof_graph.in_degree(node) &gt; 0]\n\n\n        f_map = {normalize_formula(c): i+1 for i, c in enumerate(all_clauses_in_proof)}\n        cot = make_cot(proof_graph, theorem_node_id, f_map)\n\n        metadata = edict({\n            'numbered_clauses': all_clauses_in_proof, \n            'conjecture': theorem_formula,\n            'cot': cot,\n            'correct_proof_structure_indices' : proof_structure_indices,\n            'correct_proof_structure_ids': sorted(proof_structure_ids),\n            'correct_proof_graph' : str(proof_graph),\n            'proof_depth' : self.config.proof_depth,\n            'axiom_set': self.axiom_set\n        })\n\n        answer = '\\n'.join(str(element) for element in sorted(proof_structure_indices))\n        return Problem(metadata, answer)\n\n    def prompt(self, metadata):\n        clauses_text = \"\\n\".join([f\"{i+1}. {c}\" for i, c in enumerate(metadata['numbered_clauses'])])\n        domain_name = DOMAIN_MAP.get(metadata['axiom_set'][:3], metadata['axiom_set'])\n\n        return (\n            f\"Reconstruct the proof dependency graph.\\n\"\n            f\"Domain: {domain_name}\\n\"\n            f\"Theorem: {metadata['conjecture']}\\n\\n\"\n            f\"Rules:\\n\"\n            f\"- Some clauses are axioms (no parents); do NOT list them\\n\"\n            f\"- All other clauses derive from exactly 2 parents\\n\"\n            f\"- Clauses can be reused as parents\\n\\n\"\n            f\"Shuffled clauses:\\n{clauses_text}\\n\\n\"\n            f\"Output derivations for derived clauses only, one per line: CHILD &lt;- PARENT_1, PARENT_2\\n\"\n            f\"Example: 5 &lt;- 2, 4\\n\"\n        )\n\n    def score_answer(self, answer, entry):\n        \"\"\"F1 of valid derivation edges against ground truth (lenient parsing).\"\"\"\n        gold = entry.metadata.get('correct_proof_structure_indices') or []\n        n = len(entry.metadata.get('numbered_clauses', []))\n        if not n or not gold:\n            return 0.0\n\n        pat = re.compile(r'^\\s*(\\d+)\\s*&lt;-\\s*(\\d+)\\s*,\\s*(\\d+)\\s*$')\n        derivations, seen = [], set()\n\n        for line in str(answer).strip().splitlines():\n            m = pat.fullmatch(line.strip())\n            if not m:\n                continue  # skip malformed / axiom lines\n            child, p1, p2 = map(int, m.groups())\n            if not (1 &lt;= child &lt;= n and 1 &lt;= p1 &lt;= n and 1 &lt;= p2 &lt;= n):\n                continue\n            if p1 == p2 or child in (p1, p2) or child in seen:\n                continue\n            seen.add(child)\n            derivations.append((child, *sorted((p1, p2))))\n\n        if not derivations:\n            return 0.0\n\n        pred_set = {f\"{c} &lt;- {p1}, {p2}\" for c, p1, p2 in derivations}\n        gold_set = set(gold)\n        tp = len(pred_set &amp; gold_set)\n        prec = tp / len(pred_set) if pred_set else 0.0\n        rec  = tp / len(gold_set)\n        return 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.ProofReconstruction-functions","title":"Functions","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.ProofReconstruction.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>F1 of valid derivation edges against ground truth (lenient parsing).</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"F1 of valid derivation edges against ground truth (lenient parsing).\"\"\"\n    gold = entry.metadata.get('correct_proof_structure_indices') or []\n    n = len(entry.metadata.get('numbered_clauses', []))\n    if not n or not gold:\n        return 0.0\n\n    pat = re.compile(r'^\\s*(\\d+)\\s*&lt;-\\s*(\\d+)\\s*,\\s*(\\d+)\\s*$')\n    derivations, seen = [], set()\n\n    for line in str(answer).strip().splitlines():\n        m = pat.fullmatch(line.strip())\n        if not m:\n            continue  # skip malformed / axiom lines\n        child, p1, p2 = map(int, m.groups())\n        if not (1 &lt;= child &lt;= n and 1 &lt;= p1 &lt;= n and 1 &lt;= p2 &lt;= n):\n            continue\n        if p1 == p2 or child in (p1, p2) or child in seen:\n            continue\n        seen.add(child)\n        derivations.append((child, *sorted((p1, p2))))\n\n    if not derivations:\n        return 0.0\n\n    pred_set = {f\"{c} &lt;- {p1}, {p2}\" for c, p1, p2 in derivations}\n    gold_set = set(gold)\n    tp = len(pred_set &amp; gold_set)\n    prec = tp / len(pred_set) if pred_set else 0.0\n    rec  = tp / len(gold_set)\n    return 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.TheoremPremiseSelection","title":"<code>TheoremPremiseSelection</code>","text":"<p>               Bases: <code>DevTask</code></p> <p>A task that generates problems where one must select the essential hypotheses required to prove a given conjecture from a larger pool of axioms. And a minimality check to ensure the ground truth is correct.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>class TheoremPremiseSelection(DevTask):\n    \"\"\"\n    A task that generates problems where one must select the essential hypotheses\n    required to prove a given conjecture from a larger pool of axioms.\n    And a minimality check to ensure the ground truth is correct.\n    \"\"\"\n    def __init__(self, config=SelectionConfig()):\n        super().__init__(config, timeout=60)\n        # Initialize prover session at task init\n        from reasoning_core.utils.udocker_process import initialize_prover_session\n        initialize_prover_session()\n\n    _initialize_graph = ConjectureEntailment._initialize_graph\n\n    def _reprove_with_minimal(self, hypotheses: list) -&gt; nx.DiGraph:\n            \"\"\"\n            Run E-prover on ONLY the minimal set as AXIOMS. \n            No conjecture is passed; we rely on derivation to find the theorem node.\n            \"\"\"\n            # Change delete=True to delete=False\n            with tempfile.NamedTemporaryFile(mode='w+', suffix='.p', delete=False) as tf:\n                for i, h in enumerate(hypotheses):\n                    tf.write(f\"cnf(h_{i}, axiom, {h}).\\n\")\n                # No need to flush if we close immediately, but good practice\n                tf.flush()\n\n            # File is now closed and safe for subprocesses to read\n            try:\n                return generate_derivation_graph(tf.name, save_output=False, e_limit=2, ranking=False)\n            finally:\n                # Clean up manually\n                if os.path.exists(tf.name):\n                    os.remove(tf.name)\n\n    def find_minimal_hypotheses(self, initial_hypotheses: list[str], conjecture: str) -&gt; list[str]:\n        \"\"\"\n        Prunes an initial set of hypotheses down to a minimal subset that is\n        still sufficient to prove the conjecture.\n        \"\"\"\n        essential_hypotheses = set(initial_hypotheses)\n\n        for h in initial_hypotheses:\n\n            temp_set = essential_hypotheses.copy()\n            if h in temp_set:\n                temp_set.remove(h)\n            else:\n                continue \n\n            is_provable = prove_conjecture(list(temp_set), conjecture)\n\n            if is_provable is True:\n                essential_hypotheses.remove(h)\n\n        return list(essential_hypotheses)\n\n    def generate(self):\n        self._initialize_graph()\n\n        for _ in range(50):\n            if not self.interesting_thm:\n                self._initialize_graph()\n                if not self.interesting_thm: continue\n\n            theorem_node_id = random.choice(self.interesting_thm)\n\n            # 1. Extract Superset &amp; Minimize\n            superset, theorem = extract_problem_from_graph(\n                self.graph, theorem_node_id, self.config.proof_depth\n            )\n            if len(superset)&gt;20:\n                continue\n\n            try:\n                # Verify superset (optimization)\n                if prove_conjecture(superset, theorem) is not True: continue\n\n                minimal = self.find_minimal_hypotheses(superset, theorem)\n\n                # Verify minimal (safety)\n                if not minimal or prove_conjecture(minimal, theorem) is not True: continue\n            except TimeoutException:\n                raise TimeoutException\n            except Exception:\n                continue\n            # 2. RE-PROVE for Clean CoT (Forward Derivation)\n            clean_graph = self._reprove_with_minimal(minimal)\n\n            # Locate theorem node in new graph\n            target_node = None\n            clean_theorem_str = normalize_formula(theorem)\n\n            for n, d in clean_graph.nodes(data=True):\n                if normalize_formula(d['data'].clause_formula) == clean_theorem_str:\n                    target_node = n\n                    break\n\n            if not target_node: continue \n\n            # 3. Create Distractors &amp; Pool\n            distractor_pool = list(set(self.all_formulas) - set(minimal) - {theorem})\n            if len(distractor_pool) &lt; self.config.num_distractors: continue \n\n            distractors = random.sample(distractor_pool, self.config.num_distractors)\n            pool = minimal + distractors\n            random.shuffle(pool)\n\n            # 4. Generate CoT\n            # Map ONLY minimal premises to their pool indices.\n            # This ensures distractors (if derived coincidentally) aren't labeled as premises.\n            f_map = {normalize_formula(h): pool.index(h)+1 for h in minimal}\n            f_map[clean_theorem_str] = \"THEOREM\"\n\n            cot = make_cot(clean_graph, target_node, f_map)\n\n            # 5. Metadata &amp; Context Filtering\n            pool_norm = set(normalize_formula(h) for h in pool)\n            useful_axioms_norm = []\n            orig_useful_ids = extract_useful_axioms(self.graph, theorem_node_id)\n\n            for uid in orig_useful_ids:\n                u_cnf = self.graph.nodes[uid]['data'].full_cnf_clause\n                if normalize_formula(self.graph.nodes[uid]['data'].clause_formula) not in pool_norm:\n                    useful_axioms_norm.append(u_cnf)\n\n            metadata = edict({\n                'hypotheses_pool': pool, \n                'theorem': theorem,\n                'cot': cot,\n                'len_superset': len(superset),\n                'correct_indices': sorted([pool.index(h) + 1 for h in minimal]),\n                'correct_minimal_hypotheses': minimal, \n                'useful_axioms': useful_axioms_norm, \n                'axiom_set': self.axiom_set\n            })\n\n            return Problem(metadata, str(metadata.correct_indices))\n\n    def prompt(self, metadata):\n\n        axiom_text = \"\\n\".join([f\"- {h}\" for h in metadata['useful_axioms']])\n        hypotheses_text = \"\\n\".join(\n            [f\"{i+1}. {h}\" for i, h in enumerate(metadata['hypotheses_pool'])]\n        )\n        domain_name = DOMAIN_MAP.get(metadata['axiom_set'][:3],metadata['axiom_set'])\n\n\n        return (\n            f\"You are a mathematical logic assistant. Your task is to identify a minimal set of premises sufficient for a proof.\\n\\n\"\n            f\"By using the **Superposition Calculus** (which includes rules like Resolution and Paramodulation).\\n\"\n            f\"## General Context\\n\"\n            f\"The problem is set in the domain of: **{domain_name}**.\\n\"\n            f\"The following are the fundamental axioms of this domain. They provide general context. **Do not use them in the proof itself.**\\n\"\n            f\"Fundamental Axioms:\\n\"\n            f\"{axiom_text}\\n\\n\"\n            f\"## Task\\n\"\n            f\"Your goal is to prove the following theorem:\\n\"\n            f\"**Theorem:**\\n\"\n            f\"`{metadata['theorem']}`\\n\\n\"\n            f\"Below is a numbered pool of potential premises. Your task is to identify the **minimal subset** of numbers from this pool whose corresponding statements are **sufficient on their own** to prove the theorem.\\n\"\n            f\"**Pool of Premises:**\\n\"\n            f\"{hypotheses_text}\\n\\n\"\n            f\"### Question\\n\"\n            f\"Which is the smallest set of numbered premises from the pool that is sufficient to prove the theorem, without using the fundamental axioms from the context?\\n\\n\"\n            f\"### Response Format\\n\"\n            f\"Your answer must be **only** a list of numbers, sorted in increasing order. For example: `[2, 5, 8]`.\"\n        )\n\n\n    def score_answer(self, answer, entry):\n        \"\"\"\n        Scores the answer using the Jaccard Index .\n        \"\"\"\n        metadata = entry.metadata\n        hypotheses_pool = metadata.get('hypotheses_pool')\n        if not hypotheses_pool:\n            return 0.0\n\n\n        truth_indices = set(ast.literal_eval(entry.answer))\n        pred_indices = set(map(int, re.findall(r'\\d+', str(answer))))\n\n\n        intersection = len(truth_indices.intersection(pred_indices))\n        union = len(truth_indices.union(pred_indices))\n\n        if union == 0:\n            return 1.0  \n\n        return intersection / union\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.TheoremPremiseSelection-functions","title":"Functions","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.TheoremPremiseSelection.find_minimal_hypotheses","title":"<code>find_minimal_hypotheses(initial_hypotheses, conjecture)</code>","text":"<p>Prunes an initial set of hypotheses down to a minimal subset that is still sufficient to prove the conjecture.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def find_minimal_hypotheses(self, initial_hypotheses: list[str], conjecture: str) -&gt; list[str]:\n    \"\"\"\n    Prunes an initial set of hypotheses down to a minimal subset that is\n    still sufficient to prove the conjecture.\n    \"\"\"\n    essential_hypotheses = set(initial_hypotheses)\n\n    for h in initial_hypotheses:\n\n        temp_set = essential_hypotheses.copy()\n        if h in temp_set:\n            temp_set.remove(h)\n        else:\n            continue \n\n        is_provable = prove_conjecture(list(temp_set), conjecture)\n\n        if is_provable is True:\n            essential_hypotheses.remove(h)\n\n    return list(essential_hypotheses)\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.TheoremPremiseSelection.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>Scores the answer using the Jaccard Index .</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"\n    Scores the answer using the Jaccard Index .\n    \"\"\"\n    metadata = entry.metadata\n    hypotheses_pool = metadata.get('hypotheses_pool')\n    if not hypotheses_pool:\n        return 0.0\n\n\n    truth_indices = set(ast.literal_eval(entry.answer))\n    pred_indices = set(map(int, re.findall(r'\\d+', str(answer))))\n\n\n    intersection = len(truth_indices.intersection(pred_indices))\n    union = len(truth_indices.union(pred_indices))\n\n    if union == 0:\n        return 1.0  \n\n    return intersection / union\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths-functions","title":"Functions","text":""},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.normalize_formula","title":"<code>normalize_formula(f)</code>","text":"<p>Canonicalize formula: remove whitespace and anonymize variables.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def normalize_formula(f: str) -&gt; str:\n    \"\"\"Canonicalize formula: remove whitespace and anonymize variables.\"\"\"\n    if not f: return \"\"\n    # Remove whitespace\n    f = re.sub(r\"\\s+\", \"\", f)\n    # Replace variables (e.g., X123) with generic V to handle alpha-equivalence\n    f = re.sub(r\"X\\d+\", \"V\", f)\n    return f\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.perturb_list","title":"<code>perturb_list(input_l, base_domain, n_perturbations=1)</code>","text":"<p>Applies cumulative perturbations to a list.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def perturb_list(input_l: list, base_domain: list, n_perturbations: int = 1) -&gt; list:\n    \"\"\"Applies cumulative perturbations to a list.\"\"\"\n    lst = list(input_l) \n    base_set = set(base_domain)\n\n    for _ in range(n_perturbations):\n        complementary = base_set - set(lst)\n\n        possible_ops = []\n        if complementary:\n            possible_ops.append('add')\n            if lst: \n                possible_ops.append('replace')\n        if len(lst) &gt; 1:\n            possible_ops.append('remove')\n        if not possible_ops:\n            break\n\n        op_type = random.choice(possible_ops)\n\n        if op_type == 'add':\n            lst.insert(random.randint(0, len(lst)), random.choice(list(complementary)))\n        elif op_type == 'remove':\n            lst.pop(random.randint(0, len(lst) - 1))\n        elif op_type == 'replace':\n            index_to_replace = random.randint(0, len(lst) - 1)\n            lst[index_to_replace] = random.choice(list(complementary))\n\n    return lst\n</code></pre>"},{"location":"tasks/formal_maths/#reasoning_core.tasks.formal_maths.prove_conjecture","title":"<code>prove_conjecture(axioms, conjecture, time_limit_seconds='30', verb=False)</code>","text":"<p>Uses Vampire to prove or disprove a conjecture given a set of axioms. Returns True (provable), False (disprovable/countersatisfiable), or an error string.</p> Source code in <code>reasoning_core/tasks/formal_maths.py</code> <pre><code>def prove_conjecture(axioms: list[str], conjecture: str,\n                        time_limit_seconds: str =\"30\", verb: bool = False):\n    \"\"\"\n    Uses Vampire to prove or disprove a conjecture given a set of axioms.\n    Returns True (provable), False (disprovable/countersatisfiable), or an error string.\n    \"\"\"\n\n    with tempfile.NamedTemporaryFile(mode='w+', delete=True, suffix='.p') as temp_f:\n        for i, axiom in enumerate(axioms, 1):\n            temp_f.write(f\"cnf(axiom_{i}, axiom, {axiom}).\\n\")\n        temp_f.write(f\"fof(conjecture_1, conjecture, {conjecture}).\\n\")\n        temp_f.flush()\n\n        if verb == True:\n            print(f\"---- proof file :-------------------------\")\n            temp_f.seek(0)  \n            print(temp_f.read()) \n            print(\"-------------------------------------------------\")\n\n\n        vampire_command_proove = [ \"-t\", str(time_limit_seconds)]\n\n        vampire_command_disproove = [\"-t\", str(time_limit_seconds),\"-sa\", \"fmb\"]\n\n        result_proove = get_prover_session().run_prover('vampire',vampire_command_proove,temp_f.name)\n\n        if verb == True:\n            print(f\"output proove vampire :  {result_proove.stdout} \")\n\n        if \"% SZS status Theorem\" in result_proove.stdout :\n            return True\n        if \"% SZS status CounterSatisfiable\" in result_proove.stdout :\n            return False\n\n        result_disproove = get_prover_session().run_prover('vampire',vampire_command_disproove,temp_f.name)\n\n        if verb == True:\n            print(f\"output disproove vampire :  {result_disproove.stdout} \")\n\n        if \"% Finite Model Found!\" in result_disproove.stdout :\n            return False \n        if \"% Time limit reached!\" in result_proove.stdout and \"% Time limit reached!\" in result_disproove.stdout  :\n            return f\"ERROR : TIME LIMIT in both tentative to proove AND to disproove\"\n        else :\n            return f\"ERROR : {result_proove.stderr}{result_disproove.stderr}\"\n</code></pre>"},{"location":"tasks/grammar/","title":"Grammar","text":""},{"location":"tasks/grammar/#reasoning_core.tasks.grammar","title":"<code>reasoning_core.tasks.grammar</code>","text":""},{"location":"tasks/grammar/#reasoning_core.tasks.grammar-classes","title":"Classes","text":""},{"location":"tasks/grammar/#reasoning_core.tasks.grammar.Continuation","title":"<code>Continuation</code>","text":"<p>               Bases: <code>Task</code></p> <p>Grammar continuation task using proper CFG parsing.</p> Source code in <code>reasoning_core/tasks/grammar.py</code> <pre><code>class Continuation(Task):\n    \"\"\"Grammar continuation task using proper CFG parsing.\"\"\"\n\n    def __init__(self, config: GrammarConfig = GrammarConfig()):\n        super().__init__(config=config)\n        self.balancing_key_ratio = 0.1\n\n    def generate(self):\n        for _ in range(100):\n            g = sample_cfg(self.config)\n\n            try:\n                sentences = list(islice(nltk_generate(g, depth=self.config.max_depth), 50))\n                if not sentences:\n                    continue\n                sentence = random.choice(sentences)\n            except (RecursionError, ValueError):\n                continue\n\n            if len(sentence) &lt; 2:\n                continue\n\n            max_prefix = min(len(sentence) - 1, 5)\n            min_prefix = min(2, max_prefix)\n            if min_prefix &gt; max_prefix:\n                continue\n            prefix_len = random.randint(min_prefix, max_prefix)\n            prefix = list(sentence[:prefix_len])\n\n            try:\n                tokens, can_stop, justifications = get_valid_next_tokens(g, prefix)\n            except Exception:\n                continue\n\n            if not tokens and not can_stop:\n                continue\n\n            answer = '|'.join(sorted(tokens))\n            if can_stop:\n                answer = (answer + '|STOP') if answer else 'STOP'\n\n            cot = _build_cot(tokens, can_stop, justifications)\n\n            return Problem(\n                edict(g=\"\\n\".join(str(p) for p in g.productions()), \n                      prefix=prefix, depth=len(prefix), cot=cot),\n                answer\n            )\n        raise ValueError(\"Failed to generate continuation after 100 attempts\")\n\n    def prompt(self, meta):\n        pfx = ' '.join(meta.prefix) if meta.prefix else '&lt;empty&gt;'\n        return (f\"List all valid next tokens for this prefix. \"\n                f\"Answer sorted alphabetically separated by |, with STOP at the end if complete.\\n\"\n                f\"(GRAMMAR)\\n{meta.g}\\n(PREFIX)\\n{pfx}\")\n\n    def score_answer(self, answer, entry):\n        if not answer: return 0.0\n        ref, ans = set(entry['answer'].split('|')), set(answer.strip().split('|'))\n        return len(ref &amp; ans) / max(len(ref | ans), 1)\n</code></pre>"},{"location":"tasks/grammar/#reasoning_core.tasks.grammar-functions","title":"Functions","text":""},{"location":"tasks/grammar/#reasoning_core.tasks.grammar.get_valid_next_tokens","title":"<code>get_valid_next_tokens(grammar, prefix)</code>","text":"<p>Given a CFG and a prefix (list of tokens), return: - set of valid next terminals - whether STOP is valid (prefix is a complete sentence) - dict mapping each token to its justification from the chart edge</p> <p>Uses EarleyChartParser to consider ALL possible parse interpretations.</p> Source code in <code>reasoning_core/tasks/grammar.py</code> <pre><code>def get_valid_next_tokens(grammar, prefix):\n    \"\"\"\n    Given a CFG and a prefix (list of tokens), return:\n    - set of valid next terminals\n    - whether STOP is valid (prefix is a complete sentence)\n    - dict mapping each token to its justification from the chart edge\n\n    Uses EarleyChartParser to consider ALL possible parse interpretations.\n    \"\"\"\n    from functools import lru_cache\n\n    parser = EarleyChartParser(grammar)\n\n    @lru_cache(maxsize=None)\n    def first_with_path(symbol, depth=0):\n        \"\"\"Return dict mapping terminals to derivation paths from symbol (max 2 levels)\"\"\"\n        if depth &gt; 2:\n            return {}\n        if isinstance(symbol, str):\n            return {symbol: symbol}\n        result = {}\n        for prod in grammar.productions(lhs=symbol):\n            if not prod.rhs():\n                continue\n            first_sym = prod.rhs()[0]\n            if isinstance(first_sym, str):\n                result[first_sym] = f\"{symbol}\u2192{first_sym}\"\n            else:\n                for tok, path in first_with_path(first_sym, depth+1).items():\n                    if tok not in result:\n                        # Show one level of derivation instead of \u2192..\u2192\n                        result[tok] = f\"{symbol}\u2192{first_sym}\u2192{tok}\"\n        return result\n\n    chart = parser.chart_parse(prefix)\n\n    valid_tokens = set()\n    justifications = {}\n    can_stop = False\n    n = len(prefix)\n\n    # Use chart.select for efficiency - only look at boundary edges\n    for edge in chart.select(end=n):\n        if edge.is_complete():\n            if edge.start() == 0 and edge.lhs() == grammar.start():\n                can_stop = True\n                justifications['STOP'] = f\"{edge.lhs()}\u2022\"\n        else:\n            nextsym = edge.nextsym()\n            if nextsym:\n                # Format edge as \"A\u2192\u03b1\u2022\u03b2\" style\n                lhs = edge.lhs()\n                rhs = edge.rhs()\n                dot_pos = edge.dot()\n                before = ' '.join(str(s) for s in rhs[:dot_pos])\n                after = ' '.join(str(s) for s in rhs[dot_pos:])\n                edge_str = f\"{lhs}\u2192{before}\u2022{after}\" if before else f\"{lhs}\u2192\u2022{after}\"\n\n                if isinstance(nextsym, str):\n                    valid_tokens.add(nextsym)\n                    if nextsym not in justifications:\n                        justifications[nextsym] = edge_str\n                else:\n                    for tok, path in first_with_path(nextsym).items():\n                        valid_tokens.add(tok)\n                        if tok not in justifications:\n                            justifications[tok] = f\"{edge_str}, {path}\"\n\n    return valid_tokens, can_stop, justifications\n</code></pre>"},{"location":"tasks/graph_operations/","title":"Graph Operations","text":""},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations","title":"<code>reasoning_core.tasks.graph_operations</code>","text":""},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations-classes","title":"Classes","text":""},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.BaseGraphTask","title":"<code>BaseGraphTask</code>","text":"<p>Handles shared, flexible graph generation and rendering.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>class BaseGraphTask:\n    \"\"\"Handles shared, flexible graph generation and rendering.\"\"\"\n    def __init__(self, config=GraphReasoningConfig()):\n        super().__init__(config)\n\n    def _generate_graph(self):\n        \"\"\"Randomly selects a topology from the list and generates a graph.\"\"\"\n        num_nodes = self.config.num_nodes\n\n        for _ in range(10): # Try a few times to get a valid graph\n            gen_func, params_ranges = random.choice(_GRAPH_GENERATORS)\n            params = {'n': num_nodes}\n            try:\n                for p_name, p_range in params_ranges.items():\n                    if isinstance(p_range[0], float):\n                        params[p_name] = random.uniform(*p_range)\n                    else:\n                        params[p_name] = random.randint(*p_range)\n\n                G = gen_func(**params)\n                # Ensure it's connected and has nodes for most tasks\n                if G.number_of_nodes() &gt; 0 and nx.is_connected(G):\n                    return G\n            except (nx.NetworkXError, ValueError) as e:\n                continue # Some generators can fail with certain params, just retry\n\n        return nx.fast_gnp_random_graph(num_nodes, 0.5) # Fallback\n\n    def _render_graph(self, G):\n        \"\"\"Randomly selects a method to describe the graph in text.\"\"\"\n        def r_adjacency_list(g):\n            return \"\\n\".join(\n                f\"Node {n} is connected to: {', '.join(map(str, sorted(g.neighbors(n))))}.\"\n                for n in sorted(g.nodes())\n            )\n\n        def r_edge_list(g):\n            edges_str = \", \".join(map(str, sorted(list(g.edges()))))\n            return f\"Nodes {sorted(list(g.nodes()))} and edges: {edges_str}.\"\n\n        def r_adj_dict(g):\n            return str({n: sorted(list(g.neighbors(n))) for n in sorted(g.nodes())})\n\n        def r_edge_pairs(g):\n            edges = [f\"{u}-{v}\" for u, v in sorted(g.edges())]\n            return f\"Edges: {', '.join(edges)}\"\n\n        def r_adjacency_matrix(g):\n            nodes = sorted(g.nodes())\n            matrix = [[1 if g.has_edge(i, j) else 0 for j in nodes] for i in nodes]\n            return f\"Nodes: {nodes}\\nMatrix:\\n\" + \"\\n\".join(map(str, matrix))\n\n        def r_dot_notation(g):\n            edges = \"; \".join(f\"{u}--{v}\" for u, v in sorted(g.edges()))\n            return f\"graph {{ {edges} }}\"\n\n        def r_prose(g):\n            return \" \".join(\n                f\"Node {n} connects to {', '.join(map(str, sorted(g.neighbors(n))))}.\" \n                if g.degree(n) &gt; 0 else f\"Node {n} is isolated.\"\n                for n in sorted(g.nodes()))\n\n        def r_incidence(g):\n            \"\"\"Each node lists its edges\"\"\"\n            return \"; \".join(\n                f\"{n}: {' '.join(f'{n}-{nb}' for nb in sorted(g.neighbors(n)))}\"\n                for n in sorted(g.nodes()))\n        renderers = [r_adjacency_list, r_edge_list, r_adj_dict, r_edge_pairs, r_adjacency_matrix, r_dot_notation, r_prose, r_incidence]\n        renderer = random.choice(renderers)\n        return renderer(G)\n</code></pre>"},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphCycleDetection","title":"<code>GraphCycleDetection</code>","text":"<p>               Bases: <code>BaseGraphTask</code>, <code>Task</code></p> <p>Task to identify the specific nodes that form a cycle in a graph.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>class GraphCycleDetection(BaseGraphTask, Task):\n    \"\"\"Task to identify the specific nodes that form a cycle in a graph.\"\"\"\n\n    def generate(self):\n        # Create a graph with exactly one cycle.\n        # Start with a path graph (guaranteed acyclic), then add one edge.\n        G = nx.path_graph(self.config.num_nodes)\n\n        # Add one edge between non-adjacent nodes to create a single cycle.\n        possible_edges = list(nx.non_edges(G))\n        if not possible_edges: # Should not happen for n &gt; 2\n            return self.generate() # Retry\n        u, v = random.choice(possible_edges)\n        G.add_edge(u, v)\n\n        # The answer is the set of nodes forming this unique cycle.\n        cycle_edges = nx.find_cycle(G)\n        answer_nodes = sorted(list(set(node for edge in cycle_edges for node in edge)))\n\n        metadata = {\"graph_description\": self._render_graph(G)}\n        return Problem(metadata=metadata, answer=str(answer_nodes))\n\n    def prompt(self, metadata):\n        return (\n            f\"Consider the graph below, which contains exactly one cycle.\\n\\n\"\n            f\"{metadata['graph_description']}\\n\\n\"\n            \"Identify all the nodes that form the cycle.\\n\"\n            \"Your answer must be a Python list of node integers, sorted in increasing order. \"\n            \"Example: `[2, 5, 7, 8]`.\"\n        )\n\n    def score_answer(self, answer, entry):\n        \"\"\"Scores based on whether the predicted set of nodes matches the true cycle.\"\"\"\n        try:\n            pred_nodes = literal_eval(answer)\n            true_nodes = literal_eval(entry.answer)\n            # Use sets for order-agnostic comparison, then check if sorted.\n            is_correct_set = (set(pred_nodes) == set(true_nodes))\n            is_sorted = (pred_nodes == sorted(pred_nodes))\n            return 1.0 if is_correct_set and is_sorted else 0.0\n        except:\n            return 0.0\n</code></pre>"},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphCycleDetection-functions","title":"Functions","text":""},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphCycleDetection.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>Scores based on whether the predicted set of nodes matches the true cycle.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"Scores based on whether the predicted set of nodes matches the true cycle.\"\"\"\n    try:\n        pred_nodes = literal_eval(answer)\n        true_nodes = literal_eval(entry.answer)\n        # Use sets for order-agnostic comparison, then check if sorted.\n        is_correct_set = (set(pred_nodes) == set(true_nodes))\n        is_sorted = (pred_nodes == sorted(pred_nodes))\n        return 1.0 if is_correct_set and is_sorted else 0.0\n    except:\n        return 0.0\n</code></pre>"},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphIsomorphism","title":"<code>GraphIsomorphism</code>","text":"<p>               Bases: <code>BaseGraphTask</code>, <code>Task</code></p> <p>Task to determine if two graphs have the exact same structure.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>class GraphIsomorphism(BaseGraphTask, Task): \n    \"\"\"Task to determine if two graphs have the exact same structure.\"\"\"\n\n    def generate(self):\n        G1 = self._generate_graph()\n\n        # We want False ~70% of the time.\n        if random.random() &lt; 0.3:\n            # TRUE Case: Create a structurally identical graph by relabeling nodes.\n            nodes = list(G1.nodes())\n            mapping = dict(zip(nodes, random.sample(nodes, len(nodes))))\n            G2 = nx.relabel_nodes(G1, mapping)\n            answer = True\n        else:\n            G2 = G1.copy()\n            success = False\n\n            for _ in range(10):\n                swaps = max(1, G2.number_of_edges() // 5)\n                try:\n                    nx.double_edge_swap(G2, nswap=swaps, max_tries=100)\n                    if not nx.is_isomorphic(G1, G2):\n                        success = True\n                        break\n                except nx.NetworkXError:  # Can fail on certain graph types\n                    continue\n\n            # Fallback: generate a completely different graph\n            if not success:\n                for _ in range(50):  # Prevent infinite loop\n                    G2 = self._generate_graph()\n                    if (G2.number_of_nodes() == G1.number_of_nodes() and \n                        not nx.is_isomorphic(G1, G2)):\n                        break            \n            answer = False  # \u2190 Now INSIDE the else block\n\n        metadata = {\n            \"graph1_description\": self._render_graph(G1),\n            \"graph2_description\": self._render_graph(G2),\n        }\n        return Problem(metadata=metadata, answer=str(answer))\n\n    def prompt(self, metadata):\n        return (\n            f\"Consider two graphs described below.\\n\\nGraph A:\\n{metadata['graph1_description']}\\n\\n\"\n            f\"Graph B:\\n{metadata['graph2_description']}\\n\\n\"\n            \"Do Graph A and Graph B have the exact same structure, just with different node labels? \"\n            \"(In other words, are they isomorphic?)\\n\"\n            \"Answer with only `True` or `False`.\"\n        )\n\n    def score_answer(self, answer, entry):\n        return 1.0 if str(answer).strip().lower() == entry.answer.lower() else 0.0\n</code></pre>"},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphNodeCentrality","title":"<code>GraphNodeCentrality</code>","text":"<p>               Bases: <code>BaseGraphTask</code>, <code>Task</code></p> <p>Task to find all nodes with the highest centrality in a graph.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>class GraphNodeCentrality(BaseGraphTask, Task):\n    \"\"\"Task to find all nodes with the highest centrality in a graph.\"\"\"\n\n    def generate(self):\n        G = self._generate_graph()\n\n        # Degree centrality is simple and intuitive: node with most connections.\n        centrality = nx.degree_centrality(G)\n        if not centrality: # Handle empty graph case\n            return self.generate()\n\n        # Find the maximum centrality value.\n        max_value = max(centrality.values())\n\n        # Find all nodes that share this maximum value.\n        most_central_nodes = sorted([\n            node for node, value in centrality.items() if value == max_value\n        ])\n\n        metadata = {\"graph_description\": self._render_graph(G)}\n        return Problem(metadata=metadata, answer=str(most_central_nodes))\n\n    def prompt(self, metadata):\n        return (\n            f\"Consider the following social network graph:\\n\\n{metadata['graph_description']}\\n\\n\"\n            \"Based on the number of connections, identify all nodes that are the most central \"\n            \"(i.e., have the highest degree centrality). There may be more than one.\\n\"\n            \"Your answer must be a Python list of node integers, sorted in increasing order. \"\n            \"Example: `[3, 8]`.\"\n        )\n\n    def score_answer(self, answer, entry):\n        \"\"\"Scores based on whether the predicted list of nodes is exactly correct.\"\"\"\n        try:\n            # Safely evaluate the string representations of the lists.\n            pred_list = literal_eval(answer)\n            true_list = literal_eval(entry.answer)\n            # The lists must be identical (which also enforces the sorting rule).\n            return 1.0 if pred_list == true_list else 0.0\n        except:\n            return 0.0\n</code></pre>"},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphNodeCentrality-functions","title":"Functions","text":""},{"location":"tasks/graph_operations/#reasoning_core.tasks.graph_operations.GraphNodeCentrality.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>Scores based on whether the predicted list of nodes is exactly correct.</p> Source code in <code>reasoning_core/tasks/graph_operations.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"Scores based on whether the predicted list of nodes is exactly correct.\"\"\"\n    try:\n        # Safely evaluate the string representations of the lists.\n        pred_list = literal_eval(answer)\n        true_list = literal_eval(entry.answer)\n        # The lists must be identical (which also enforces the sorting rule).\n        return 1.0 if pred_list == true_list else 0.0\n    except:\n        return 0.0\n</code></pre>"},{"location":"tasks/logic/","title":"Logic","text":""},{"location":"tasks/logic/#reasoning_core.tasks.logic","title":"<code>reasoning_core.tasks.logic</code>","text":""},{"location":"tasks/logic/#reasoning_core.tasks.logic-classes","title":"Classes","text":""},{"location":"tasks/logic/#reasoning_core.tasks.logic-functions","title":"Functions","text":""},{"location":"tasks/planning/","title":"Planning","text":""},{"location":"tasks/planning/#reasoning_core.tasks.planning","title":"<code>reasoning_core.tasks.planning</code>","text":""},{"location":"tasks/planning/#reasoning_core.tasks.planning-classes","title":"Classes","text":""},{"location":"tasks/planning/#reasoning_core.tasks.planning-functions","title":"Functions","text":""},{"location":"tasks/planning/#reasoning_core.tasks.planning.parse_jsonl_plan","title":"<code>parse_jsonl_plan(jsonl_string)</code>","text":"<p>Parses a JSONL string of tool calls into a PDDL-like plan string.</p> Source code in <code>reasoning_core/tasks/planning.py</code> <pre><code>def parse_jsonl_plan(jsonl_string: str) -&gt; str:\n    \"\"\"\n    Parses a JSONL string of tool calls into a PDDL-like plan string.\n    \"\"\"\n    actions = []\n    for line in jsonl_string.strip().splitlines():\n        try:\n            # Parse the JSON from the line\n            call = json.loads(line)\n            tool_name = call.get(\"tool_name\")\n            args = call.get(\"arguments\", {})\n\n            if not tool_name or not isinstance(args, dict):\n                continue # Skip malformed lines\n\n            # Format into PDDL-like action: (action_name arg1 arg2)\n            arg_values = \" \".join(args.values())\n            actions.append(f\"({tool_name} {arg_values})\".strip())\n        except (json.JSONDecodeError, AttributeError):\n            # Ignore lines that are not valid JSON or don't have the expected structure\n            continue\n\n    return \"\\n\".join(actions)\n</code></pre>"},{"location":"tasks/regex_following/","title":"Regex Following","text":""},{"location":"tasks/regex_following/#reasoning_core.tasks.regex_following","title":"<code>reasoning_core.tasks.regex_following</code>","text":""},{"location":"tasks/regex_following/#reasoning_core.tasks.regex_following-classes","title":"Classes","text":""},{"location":"tasks/regex_following/#reasoning_core.tasks.regex_following-functions","title":"Functions","text":""},{"location":"tasks/regex_following/#reasoning_core.tasks.regex_following.sample_instance","title":"<code>sample_instance(r_str, max_tries=100)</code>","text":"<p>Generates a non-empty string that is verified by re.fullmatch().</p> Source code in <code>reasoning_core/tasks/regex_following.py</code> <pre><code>@shutup\ndef sample_instance(r_str, max_tries=100):\n    \"\"\"Generates a non-empty string that is verified by re.fullmatch().\"\"\"\n    try:\n        #p = re.compile(r_str)\n        p = regex.compile(r_str)\n\n    except re.error:\n        raise ValueError(f\"Could not compile invalid regex: {r_str}\")\n\n    for _ in range(max_tries):\n        s = exrex.getone(r_str, 5)\n        # Verify the generated string is a non-empty full match\n        if s and p.fullmatch(s, timeout=5):\n            return s\n    raise ValueError(f\"Could not generate a verified string for regex: {r_str}\")\n</code></pre>"},{"location":"tasks/regex_following/#reasoning_core.tasks.regex_following.strip_anchors_safe","title":"<code>strip_anchors_safe(text)</code>","text":"<p>Strips optional ^ and non-escaped $ from a regex string.</p> Source code in <code>reasoning_core/tasks/regex_following.py</code> <pre><code>def strip_anchors_safe(text: str) -&gt; str:\n    \"\"\"Strips optional ^ and non-escaped $ from a regex string.\"\"\"\n    # This is the robust one-liner\n    m = regex.match(r\"^\\^?(.*?)(?&lt;!\\\\)\\$?$\", text)\n    return m.group(1) if m else text\n</code></pre>"},{"location":"tasks/rung3_reasoning/","title":"Rung3 Reasoning","text":""},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning","title":"<code>reasoning_core.tasks.rung3_reasoning</code>","text":""},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning-classes","title":"Classes","text":""},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning.BaseSCMGraph","title":"<code>BaseSCMGraph</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for Structural Causal Models (Rung 3). Handles DAG generation, Counterfactual logic, and NL formatting.</p> Source code in <code>reasoning_core/tasks/rung3_reasoning.py</code> <pre><code>class BaseSCMGraph(ABC):\n    \"\"\"\n    Abstract Base Class for Structural Causal Models (Rung 3).\n    Handles DAG generation, Counterfactual logic, and NL formatting.\n    \"\"\"\n    def __init__(self):\n        self.scm = None\n        self.dag = None\n        self.equations_nl = {} \n        self.observed_values = {}\n        self.noise_values = {}   \n        self.counterfactual_query = {} \n        self.target_node = None\n        self.truth_value = 0.0\n        self.precision = 2\n        self.model_name = \"Structural Causal Model\"\n        self.model_description = \"Variables are continuous.\"\n        self.noise_separation = 2.5\n        self.noise_scale = 1.0\n\n    def _generate_dag_structure(self, n, edge_prob, seed):\n        \"\"\"Generates a random DAG and ensures topological order is respected.\"\"\"\n        rng = np.random.default_rng(seed)\n        G = nx.gnp_random_graph(n, edge_prob, directed=True, seed=seed)\n        # Enforce Acyclicity (u &lt; v)\n        self.dag = nx.DiGraph([(u, v) for (u, v) in G.edges() if u &lt; v])\n        self.dag.add_nodes_from(range(n))\n\n        # Return sorted nodes for safe processing\n        try:\n            return list(nx.topological_sort(self.dag))\n        except:\n            return sorted(list(self.dag.nodes()))\n\n    @abstractmethod\n    def generate_random_scm(self, n, edge_prob, precision, seed):\n        \"\"\"Define equations and build SCM.\"\"\"\n        pass\n\n    @abstractmethod\n    def _manual_forward_pass(self, noise_dict, intervention_dict=None):\n        \"\"\"Calculate variable values from noise + parents.\"\"\"\n        pass\n\n    def generate_counterfactual_problem(self,):\n        \"\"\"\n        The Universal Algorithm for Counterfactuals (Rung 3).\n        1. Generate Noise (N).\n        2. Calculate Fact (X) from N.\n        3. Define Intervention (do(X_i)).\n        4. Calculate Counterfactual (X') from N + do(X_i).\n        \"\"\"\n        rng = np.random.default_rng()\n\n        # 1. Generate Hidden Noise\n        self.noise_values = {}\n        for node in self.dag.nodes():\n            raw_noise = float(rng.standard_normal())\n            self.noise_values[f\"N_{node}\"] = round(raw_noise, self.precision)\n\n        # 2. Forward Pass (The Fact)\n        raw_facts = self._manual_forward_pass(self.noise_values)\n        self.observed_values = {f\"X_{k}\": v for k, v in raw_facts.items()}\n\n        # 3. Define Intervention\n        # Pick a target (sink) and an intervention node (upstream)\n        try:\n            ordered_nodes = list(nx.topological_sort(self.dag))\n        except:\n            ordered_nodes = sorted(list(self.dag.nodes()))\n\n        self.target_node = f\"X_{ordered_nodes[-1]}\"\n        upstream = ordered_nodes[:-1]\n\n        if not upstream: return # Degenerate case\n\n        intervene_node = random.choice(upstream)\n        intervene_var = f\"X_{intervene_node}\"\n\n        # Choose a value distinct from the observed fact\n        curr_val = self.observed_values[intervene_var]\n\n        chi_sample = np.sqrt(rng.chisquare(df=3))\n        sign = rng.choice([-1, 1])\n\n        shift = float(sign * chi_sample)\n        new_val = round(curr_val + shift, self.precision)\n\n        self.counterfactual_query = {intervene_var: new_val}\n\n        # 4. Prediction (The Counterfactual)\n        # Critical: We reuse self.noise_values (The Abduction step is implicit here)\n        cf_values = self._manual_forward_pass(self.noise_values, self.counterfactual_query)\n        self.truth_value = cf_values[int(self.target_node.split(\"_\")[1])]\n\n    def to_NL_description(self):\n        lines = [f\"### {self.model_name}\"]\n        lines.append(self.model_description)\n        lines.append(\"Equations:\")\n        for node in sorted(self.dag.nodes()):\n            lines.append(f\"- {self.equations_nl[node]}\")\n        return \"\\n\".join(lines)\n\n    def to_NL_scenario(self):\n        facts = \", \".join([f\"{k}={v}\" for k, v in self.observed_values.items()])\n        q_var = list(self.counterfactual_query.keys())[0]\n        q_val = list(self.counterfactual_query.values())[0]\n        return (\n            f\"1. OBSERVED DATA: {facts}\\n\"\n            f\"2. QUERY: What if {q_var} had been {q_val}?\"\n        )\n</code></pre>"},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning.BaseSCMGraph-functions","title":"Functions","text":""},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning.BaseSCMGraph.generate_counterfactual_problem","title":"<code>generate_counterfactual_problem()</code>","text":"<p>The Universal Algorithm for Counterfactuals (Rung 3). 1. Generate Noise (N). 2. Calculate Fact (X) from N. 3. Define Intervention (do(X_i)). 4. Calculate Counterfactual (X') from N + do(X_i).</p> Source code in <code>reasoning_core/tasks/rung3_reasoning.py</code> <pre><code>def generate_counterfactual_problem(self,):\n    \"\"\"\n    The Universal Algorithm for Counterfactuals (Rung 3).\n    1. Generate Noise (N).\n    2. Calculate Fact (X) from N.\n    3. Define Intervention (do(X_i)).\n    4. Calculate Counterfactual (X') from N + do(X_i).\n    \"\"\"\n    rng = np.random.default_rng()\n\n    # 1. Generate Hidden Noise\n    self.noise_values = {}\n    for node in self.dag.nodes():\n        raw_noise = float(rng.standard_normal())\n        self.noise_values[f\"N_{node}\"] = round(raw_noise, self.precision)\n\n    # 2. Forward Pass (The Fact)\n    raw_facts = self._manual_forward_pass(self.noise_values)\n    self.observed_values = {f\"X_{k}\": v for k, v in raw_facts.items()}\n\n    # 3. Define Intervention\n    # Pick a target (sink) and an intervention node (upstream)\n    try:\n        ordered_nodes = list(nx.topological_sort(self.dag))\n    except:\n        ordered_nodes = sorted(list(self.dag.nodes()))\n\n    self.target_node = f\"X_{ordered_nodes[-1]}\"\n    upstream = ordered_nodes[:-1]\n\n    if not upstream: return # Degenerate case\n\n    intervene_node = random.choice(upstream)\n    intervene_var = f\"X_{intervene_node}\"\n\n    # Choose a value distinct from the observed fact\n    curr_val = self.observed_values[intervene_var]\n\n    chi_sample = np.sqrt(rng.chisquare(df=3))\n    sign = rng.choice([-1, 1])\n\n    shift = float(sign * chi_sample)\n    new_val = round(curr_val + shift, self.precision)\n\n    self.counterfactual_query = {intervene_var: new_val}\n\n    # 4. Prediction (The Counterfactual)\n    # Critical: We reuse self.noise_values (The Abduction step is implicit here)\n    cf_values = self._manual_forward_pass(self.noise_values, self.counterfactual_query)\n    self.truth_value = cf_values[int(self.target_node.split(\"_\")[1])]\n</code></pre>"},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning.BaseSCMGraph.generate_random_scm","title":"<code>generate_random_scm(n, edge_prob, precision, seed)</code>  <code>abstractmethod</code>","text":"<p>Define equations and build SCM.</p> Source code in <code>reasoning_core/tasks/rung3_reasoning.py</code> <pre><code>@abstractmethod\ndef generate_random_scm(self, n, edge_prob, precision, seed):\n    \"\"\"Define equations and build SCM.\"\"\"\n    pass\n</code></pre>"},{"location":"tasks/rung3_reasoning/#reasoning_core.tasks.rung3_reasoning.Rung3FloatConfig","title":"<code>Rung3FloatConfig</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Config</code></p> <p>Configuration for Continuous Counterfactual tasks.</p> Source code in <code>reasoning_core/tasks/rung3_reasoning.py</code> <pre><code>@dataclass\nclass Rung3FloatConfig(Config):\n    \"\"\"\n    Configuration for Continuous Counterfactual tasks.\n    \"\"\"\n    n_nodes: int = 3\n    edge_prob: float = 0.4\n    # We don't need domain_size anymore.\n    # We add precision to keep numbers clean.\n    precision: int = 1 \n    graph_generation_mode: str = \"erdos\"\n\n    def update(self, c):\n        self.n_nodes += c\n</code></pre>"},{"location":"tasks/sequential_induction/","title":"Sequential Induction","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction","title":"<code>reasoning_core.tasks.sequential_induction</code>","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction-classes","title":"Classes","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence","title":"<code>Sequence</code>","text":"Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>class Sequence:\n    def __init__(self, formula : str, initial_elem : list = None):\n        self.rec_formula = formula\n        self.degree = self.compute_degree()\n        if initial_elem == None:\n            self.set_random_first_sample()\n        elif (self.degree &lt;= len(initial_elem)):\n            self.first_elem = initial_elem[:self.degree]\n        else:\n            raise ValueError(f\"The degree of the recursive formula is {self.degree}, but there is only {len(initial_elem)} initial terms, so the sequence cannot be properly defined.\")\n\n    def __repr__(self):\n        return f\"The formula of the sequence is {self.rec_formula}, of degree of recursion {self.degree}, and initial terms {self.first_elem}\"\n\n    def compute_degree(self) -&gt; int: \n        ret = 0\n        for i in range(100):\n            U_i = f'U[n - {i}]'\n            U_i_bis = f'U[n-{i}]' #case of LLM not respecting format\n            if U_i in str(self.rec_formula) or U_i_bis in str(self.rec_formula):\n                ret = i\n        return ret\n\n    def instantiate(self, previous_terms : list, rank : int) -&gt; str:\n        \"\"\"instantiate the U[n - i] and n within the recursive formula of the Sequence, used in the function U_n in order to compute the following element\"\"\"\n        d = self.degree\n        if d &gt; len(previous_terms) or d &gt; rank:\n            raise ValueError(f\"\"\"Cannot instantiate, as the recursive degree of the formula ( {d} )\n             is superior to the rank ( {rank} ) or to the number of the previous provided terms ( {len(previous_terms)} )\"\"\")\n        ret = str(self.rec_formula)\n        for i,value in enumerate(previous_terms[-d:]): # replace each U[n - i] by its value\n            index = d-i\n            to_replace = f'U[n - {index}]'\n            to_replace_bis = f'U[n-{index}]' # case of llm not respecting format\n            ret = ret.replace(to_replace, str(value))\n            ret = ret.replace(to_replace_bis, str(value))\n        ret = re.sub(r'(?&lt;!sign)\\bn\\b', str(rank), ret) # replace \"n\" by the rank, except the case when \"n\" is in \"sign\"\n        return ret\n\n    def set_random_first_sample(self):\n        self.first_elem = [np.random.randint(-9,10) for _ in range(self.degree)]\n\n    def U_n(self, predecessors : list, rank : int) -&gt; int :\n        \"\"\"Given the rank and the predecessors, compute the following element of the sequence\"\"\"\n        formula = str(self.rec_formula)\n        d = self.degree\n        formula_instance = self.instantiate(predecessors, rank = rank)\n        relu = lambda x : max(0,x)  # define those functions for the eval\n        Mod = lambda k,n : k%n \n        sign = lambda n : 1 if n &gt; 0 else -1 if n &lt; 0 else 0\n        return eval(formula_instance.replace('/', '//')) # in order to use integer division\n\n    def n_first_elem(self, n: int, max_terms_len: int = 12) -&gt; list:\n        \"\"\"\n        Generates the first n elements of the sequence.\n\n        Args:\n            n (int): The number of elements to generate.\n            max_terms_len (int, optional): The maximum string length allowed for any single term.\n                                    If a term exceeds this length, the generation stops,\n                                    and the list of terms generated so far is returned.\n\n        Returns:\n            list: A list of the sequence elements. The list will have a length of 'n'\n                if the generation completed, or a length less than 'n' if it was\n                stopped early by the max_terms_len check.\n        \"\"\"\n        if n &lt; self.degree:\n            raise ValueError(\"The number of sequence elements requested is less than its degree of recurrence.\")\n\n        ret = self.first_elem[:]\n        rank = len(ret)\n        while len(ret) &lt; n:\n            next_term = self.U_n(predecessors=ret, rank=rank)\n            if max_terms_len is not None and len(str(next_term)) &gt; max_terms_len:\n                # Explosion detected! Stop the process gracefully.\n                break\n            ret.append(next_term)\n            rank += 1\n\n        return ret\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence-functions","title":"Functions","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence.U_n","title":"<code>U_n(predecessors, rank)</code>","text":"<p>Given the rank and the predecessors, compute the following element of the sequence</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def U_n(self, predecessors : list, rank : int) -&gt; int :\n    \"\"\"Given the rank and the predecessors, compute the following element of the sequence\"\"\"\n    formula = str(self.rec_formula)\n    d = self.degree\n    formula_instance = self.instantiate(predecessors, rank = rank)\n    relu = lambda x : max(0,x)  # define those functions for the eval\n    Mod = lambda k,n : k%n \n    sign = lambda n : 1 if n &gt; 0 else -1 if n &lt; 0 else 0\n    return eval(formula_instance.replace('/', '//')) # in order to use integer division\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence.instantiate","title":"<code>instantiate(previous_terms, rank)</code>","text":"<p>instantiate the U[n - i] and n within the recursive formula of the Sequence, used in the function U_n in order to compute the following element</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def instantiate(self, previous_terms : list, rank : int) -&gt; str:\n    \"\"\"instantiate the U[n - i] and n within the recursive formula of the Sequence, used in the function U_n in order to compute the following element\"\"\"\n    d = self.degree\n    if d &gt; len(previous_terms) or d &gt; rank:\n        raise ValueError(f\"\"\"Cannot instantiate, as the recursive degree of the formula ( {d} )\n         is superior to the rank ( {rank} ) or to the number of the previous provided terms ( {len(previous_terms)} )\"\"\")\n    ret = str(self.rec_formula)\n    for i,value in enumerate(previous_terms[-d:]): # replace each U[n - i] by its value\n        index = d-i\n        to_replace = f'U[n - {index}]'\n        to_replace_bis = f'U[n-{index}]' # case of llm not respecting format\n        ret = ret.replace(to_replace, str(value))\n        ret = ret.replace(to_replace_bis, str(value))\n    ret = re.sub(r'(?&lt;!sign)\\bn\\b', str(rank), ret) # replace \"n\" by the rank, except the case when \"n\" is in \"sign\"\n    return ret\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence.n_first_elem","title":"<code>n_first_elem(n, max_terms_len=12)</code>","text":"<p>Generates the first n elements of the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of elements to generate.</p> required <code>max_terms_len</code> <code>int</code> <p>The maximum string length allowed for any single term.                     If a term exceeds this length, the generation stops,                     and the list of terms generated so far is returned.</p> <code>12</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of the sequence elements. The list will have a length of 'n' if the generation completed, or a length less than 'n' if it was stopped early by the max_terms_len check.</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def n_first_elem(self, n: int, max_terms_len: int = 12) -&gt; list:\n    \"\"\"\n    Generates the first n elements of the sequence.\n\n    Args:\n        n (int): The number of elements to generate.\n        max_terms_len (int, optional): The maximum string length allowed for any single term.\n                                If a term exceeds this length, the generation stops,\n                                and the list of terms generated so far is returned.\n\n    Returns:\n        list: A list of the sequence elements. The list will have a length of 'n'\n            if the generation completed, or a length less than 'n' if it was\n            stopped early by the max_terms_len check.\n    \"\"\"\n    if n &lt; self.degree:\n        raise ValueError(\"The number of sequence elements requested is less than its degree of recurrence.\")\n\n    ret = self.first_elem[:]\n    rank = len(ret)\n    while len(ret) &lt; n:\n        next_term = self.U_n(predecessors=ret, rank=rank)\n        if max_terms_len is not None and len(str(next_term)) &gt; max_terms_len:\n            # Explosion detected! Stop the process gracefully.\n            break\n        ret.append(next_term)\n        rank += 1\n\n    return ret\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction","title":"<code>SequentialInduction</code>","text":"<p>               Bases: <code>Task</code></p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>class SequentialInduction(Task):\n    def __init__(self, config=SequenceConfig()):\n        super().__init__(config=config)\n        # Now, self.filters will contain references to picklable instance methods\n        self.filters = [\n            self._filter_2_outof,\n            self._filter_max_terms_len\n        ]\n\n    def _filter_2_outof(self, s):\n        return filter_2_outof(s, length_check=self.config.n_visible_terms)\n\n    def _filter_max_terms_len(self, s):\n        return filter_max_terms_len(\n            s,\n            max_terms_len=self.config.max_terms_len,\n            length_check=self.config.n_visible_terms + 1\n        )\n\n    def one_shot_sympy_generate(self):\n        \"\"\"generate a formula (instance of the defined cfg) and simplify it with sympy\"\"\"\n        rule = Sequence_cfg(self.config.mode, self.config.recurrence_depth)\n        prod = generate(rule, depth=self.config.max_depth_grammar, min_depth=self.config.min_depth_grammar)@'eq' \n        return convert_to_sympy(prod, self.config.recurrence_depth)\n\n\n    def generate(self) -&gt; Problem:\n        formula = self.one_shot_sympy_generate()\n        S = Sequence(formula)\n        while not all([ self.filters[i](S) for i in range(len(self.filters)) ]): #keep generating until the sequence fulfill all requirements (filters)\n            formula = self.one_shot_sympy_generate()\n            S = Sequence(formula)\n        data = {\"first elements\" : S.n_first_elem(self.config.n_visible_terms), \"degree of recursion\" : S.degree, \"initial terms\" : S.first_elem}\n        answer = str(formula)\n        return Problem(metadata = data, answer = answer)\n\n    def verify(self, y_pred, y_truth, initial_element = None) -&gt; bool:\n        \"\"\" Check if the guessed formula match with the true one (for the n_visible term)\"\"\"\n        S_true = Sequence(y_truth, initial_elem = initial_element)\n        degree = S_true.degree\n        elem_true = S_true.n_first_elem(self.config.n_visible_terms)\n        try:\n            S_pred = Sequence(y_pred, initial_elem= initial_element)\n            elem_pred = S_pred.n_first_elem(self.config.n_visible_terms)\n            return elem_true[degree:] == elem_pred[degree:]\n        except Exception as e:\n                    return False\n\n\n    def score_answer(self, answer, entry):\n        \"\"\"\n        Score the predicted recursive formula (y_pred) against the true one (y_truth).\n\n        Returns:\n            float: A score between 0 and 1 based on correctness, simplicity, and efficiency.\n        \"\"\"\n        initial_terms = entry.metadata[\"initial terms\"]\n        n_visible = len(entry.metadata['first elements'])\n        formula_true = str(entry.answer)        \n        S_true = Sequence(formula_true, initial_elem = initial_terms)\n        degree_true = S_true.degree\n        terms_true = S_true.n_first_elem(n_visible)\n\n        try:\n            formula_pred = str(answer)\n            S_pred = Sequence(formula_pred, initial_elem= initial_terms)\n            terms_pred = S_pred.n_first_elem(n_visible)\n        except Exception as e:\n            return 0.0\n\n        base_score = sum(a == b for a, b in zip(terms_pred[degree_true:], terms_true[degree_true:]))/len(terms_true[degree_true:])\n\n        if base_score &lt; 0.5: #if half of the sample are not predicted well, then consider it as not predicted\n            return 0\n\n        degree_score = (1 + degree_true) / (1 + S_pred.degree)\n\n        # Efficiency penalty from operator usage\n        ops_pred = parse_recursive_formula(formula_pred)\n        ops_true = parse_recursive_formula(formula_true)\n\n        total_ops_true = sum(ops_true.get(op, 0) for op in ops_true)\n        total_ops_pred = sum(ops_pred.get(op, 0) for op in ops_pred)\n\n        ops_conciseness_score = (1 + total_ops_true) / (1 + total_ops_pred)\n\n        final_score = base_score * min(1.0, degree_score * ops_conciseness_score)\n\n        return final_score\n\n\n    def prompt(self, metadata) -&gt; str:\n        \"\"\"Build a concise prompt for inferring a recurrence from first terms.\"\"\"\n        n_vis = self.config.n_visible_terms\n        d = metadata[\"degree of recursion\"]\n\n        lines = [\n            f\"Infer a recurrence for a sequence indexed from 0: [U0, U1, ..., U{n_vis - 1}].\",\n            f\"Max recurrence degree: {d}.\",\n            \"\",\n            \"Allowed binary ops: +, -, *, **\",\n        ]\n\n        if self.config.mode == \"full\":\n            lines.append(\"- Also allowed: / (Euclidean division), Mod(a,b), relu(x), sign(x)\")\n\n        lines += [\n            f\"- Previous terms must be referenced exactly as: U[n - 1] ... U[n - {d}]\",\n            '- You may use \"n\" (current index).',\n            '- Output ONLY the right-hand side (do not write \"U[n] =\").',\n            f\"- Your recurrence degree must be &lt;= {d}.\",\n            \"\",\n            f\"Sequence: {metadata['first elements']}\",\n            f\"Degree of recurrence: {d}\",\n            f\"Initial terms: {metadata['initial terms']}\",\n            \"\",\n            \"Answer must hold for all n &gt;= d and be as simple as possible.\",\n        ]\n\n        return \"\\n\".join(lines)\n\n\n    def deduplication_key(self, problem):\n        \"\"\"\n        The couple (formula,initial_terms) is sufficient/necessary in order to characterize the sequence\n        \"\"\"\n        return problem.answer , problem.metadata['first elements']\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction-functions","title":"Functions","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction.deduplication_key","title":"<code>deduplication_key(problem)</code>","text":"<p>The couple (formula,initial_terms) is sufficient/necessary in order to characterize the sequence</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def deduplication_key(self, problem):\n    \"\"\"\n    The couple (formula,initial_terms) is sufficient/necessary in order to characterize the sequence\n    \"\"\"\n    return problem.answer , problem.metadata['first elements']\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction.one_shot_sympy_generate","title":"<code>one_shot_sympy_generate()</code>","text":"<p>generate a formula (instance of the defined cfg) and simplify it with sympy</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def one_shot_sympy_generate(self):\n    \"\"\"generate a formula (instance of the defined cfg) and simplify it with sympy\"\"\"\n    rule = Sequence_cfg(self.config.mode, self.config.recurrence_depth)\n    prod = generate(rule, depth=self.config.max_depth_grammar, min_depth=self.config.min_depth_grammar)@'eq' \n    return convert_to_sympy(prod, self.config.recurrence_depth)\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction.prompt","title":"<code>prompt(metadata)</code>","text":"<p>Build a concise prompt for inferring a recurrence from first terms.</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def prompt(self, metadata) -&gt; str:\n    \"\"\"Build a concise prompt for inferring a recurrence from first terms.\"\"\"\n    n_vis = self.config.n_visible_terms\n    d = metadata[\"degree of recursion\"]\n\n    lines = [\n        f\"Infer a recurrence for a sequence indexed from 0: [U0, U1, ..., U{n_vis - 1}].\",\n        f\"Max recurrence degree: {d}.\",\n        \"\",\n        \"Allowed binary ops: +, -, *, **\",\n    ]\n\n    if self.config.mode == \"full\":\n        lines.append(\"- Also allowed: / (Euclidean division), Mod(a,b), relu(x), sign(x)\")\n\n    lines += [\n        f\"- Previous terms must be referenced exactly as: U[n - 1] ... U[n - {d}]\",\n        '- You may use \"n\" (current index).',\n        '- Output ONLY the right-hand side (do not write \"U[n] =\").',\n        f\"- Your recurrence degree must be &lt;= {d}.\",\n        \"\",\n        f\"Sequence: {metadata['first elements']}\",\n        f\"Degree of recurrence: {d}\",\n        f\"Initial terms: {metadata['initial terms']}\",\n        \"\",\n        \"Answer must hold for all n &gt;= d and be as simple as possible.\",\n    ]\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction.score_answer","title":"<code>score_answer(answer, entry)</code>","text":"<p>Score the predicted recursive formula (y_pred) against the true one (y_truth).</p> <p>Returns:</p> Name Type Description <code>float</code> <p>A score between 0 and 1 based on correctness, simplicity, and efficiency.</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def score_answer(self, answer, entry):\n    \"\"\"\n    Score the predicted recursive formula (y_pred) against the true one (y_truth).\n\n    Returns:\n        float: A score between 0 and 1 based on correctness, simplicity, and efficiency.\n    \"\"\"\n    initial_terms = entry.metadata[\"initial terms\"]\n    n_visible = len(entry.metadata['first elements'])\n    formula_true = str(entry.answer)        \n    S_true = Sequence(formula_true, initial_elem = initial_terms)\n    degree_true = S_true.degree\n    terms_true = S_true.n_first_elem(n_visible)\n\n    try:\n        formula_pred = str(answer)\n        S_pred = Sequence(formula_pred, initial_elem= initial_terms)\n        terms_pred = S_pred.n_first_elem(n_visible)\n    except Exception as e:\n        return 0.0\n\n    base_score = sum(a == b for a, b in zip(terms_pred[degree_true:], terms_true[degree_true:]))/len(terms_true[degree_true:])\n\n    if base_score &lt; 0.5: #if half of the sample are not predicted well, then consider it as not predicted\n        return 0\n\n    degree_score = (1 + degree_true) / (1 + S_pred.degree)\n\n    # Efficiency penalty from operator usage\n    ops_pred = parse_recursive_formula(formula_pred)\n    ops_true = parse_recursive_formula(formula_true)\n\n    total_ops_true = sum(ops_true.get(op, 0) for op in ops_true)\n    total_ops_pred = sum(ops_pred.get(op, 0) for op in ops_pred)\n\n    ops_conciseness_score = (1 + total_ops_true) / (1 + total_ops_pred)\n\n    final_score = base_score * min(1.0, degree_score * ops_conciseness_score)\n\n    return final_score\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.SequentialInduction.verify","title":"<code>verify(y_pred, y_truth, initial_element=None)</code>","text":"<p>Check if the guessed formula match with the true one (for the n_visible term)</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def verify(self, y_pred, y_truth, initial_element = None) -&gt; bool:\n    \"\"\" Check if the guessed formula match with the true one (for the n_visible term)\"\"\"\n    S_true = Sequence(y_truth, initial_elem = initial_element)\n    degree = S_true.degree\n    elem_true = S_true.n_first_elem(self.config.n_visible_terms)\n    try:\n        S_pred = Sequence(y_pred, initial_elem= initial_element)\n        elem_pred = S_pred.n_first_elem(self.config.n_visible_terms)\n        return elem_true[degree:] == elem_pred[degree:]\n    except Exception as e:\n                return False\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction-functions","title":"Functions","text":""},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.Sequence_cfg","title":"<code>Sequence_cfg(mode, recurrence_depth)</code>","text":"<p>Generate a grammar of recursive sequences of a given depth.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>simple or full</code> <p>type of sequence</p> required <code>recurrence_depth</code> <code>int</code> <p>the maximum recursion depth</p> required <p>Returns:</p> Type Description <p>Rule subclass: a unigram grammar object for recursive sequences</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def Sequence_cfg(mode: str, recurrence_depth: int): #chatgpt one\n    \"\"\" Generate a grammar of recursive sequences of a given depth.\n\n    Args:\n        mode (\"simple\" or \"full\"): type of sequence\n        recurrence_depth (int): the maximum recursion depth\n\n    Returns:\n        Rule subclass: a unigram grammar object for recursive sequences\n    \"\"\"\n    R = init_grammar(['eq'], name=f\"sequence_of_depth_{recurrence_depth}\", preprocess_template=lambda s:s)\n\n    # --- Common rules ---\n    R('start(exp)', '{0}')            # top-level expansion\n    R('exp', 'n')                     # U[n] = n\n\n    # U[n] = U[n - d]\n    R('exp(Ui)', '{0}')\n    for i in range(1, recurrence_depth + 1):\n        R('Ui', f'U{i}')\n\n    # Constant terms\n    R('exp(c_z)', '{0}')\n    for i in range(-9, 10):\n        R('c_z', str(i))\n\n    # Binary operations\n    R('exp(exp,bop,exp)', '({0} {1} {2})')\n    for op in ['+', '*', '-']:\n        R('bop', op)\n\n    # --- Full-mode extensions ---\n    if mode == 'full':\n        # Unary operations like sign() or relu()\n        R('exp(uop,exp)', '{0}({1})')\n        for u in ['sign', 'relu']:\n            R('uop', u)\n\n        # Safe binary ops with natural-number constants\n        R('exp(exp,safe_bop,c_n)', '({0} {1} {2})')\n        for i in range(1, 10):\n            R('c_n', str(i))\n        for op in ['/', '%']:\n            R('safe_bop', op)\n\n    return R\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.convert_to_sympy","title":"<code>convert_to_sympy(tokens, recurrence_depth=3)</code>","text":"<p>Convert a list of tokens to a SymPy expression with special handling</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def convert_to_sympy(tokens, recurrence_depth = 3):\n    \"\"\"Convert a list of tokens to a SymPy expression with special handling\"\"\"\n\n    expr_str = ''.join(tokens)\n    n = sp.symbols('n', integer=True, nonnegative=True)\n    U = sp.IndexedBase('U')   \n\n    # Build a dictionary of the u_i variables dynamically.\n    U_vars = {f'U{i}': U[n - i] for i in range(1, recurrence_depth+1)}\n\n    # Local dictionary with safe operations and the dynamically generated u variables.\n    local_dict = {\n            \"relu\": Relu,  \n            \"mod\": Mod,\n            \"sign\": Sign,\n            '/': safe_div,\n                    }\n    local_dict.update(U_vars)\n    expr = sp.sympify(expr_str, locals=local_dict)\n    return expr\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.filter_2_outof","title":"<code>filter_2_outof(S, length_check=10)</code>","text":"<p>Filter constant sequences</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def filter_2_outof(S :Sequence, length_check : int = 10) -&gt; bool:\n    \"\"\"Filter constant sequences\"\"\"\n    first_terms = S.n_first_elem(length_check)[S.degree:]\n    return len(set(first_terms)) &gt; 1\n</code></pre>"},{"location":"tasks/sequential_induction/#reasoning_core.tasks.sequential_induction.filter_max_terms_len","title":"<code>filter_max_terms_len(S, max_terms_len=12, length_check=12)</code>","text":"<p>Check if, for the length_check first terms, the length of the numbers of the sequence doesn't exceed max_terms_len (prevent explosions)</p> Source code in <code>reasoning_core/tasks/sequential_induction.py</code> <pre><code>def filter_max_terms_len(S :Sequence, max_terms_len : int = 12, length_check : int = 12) -&gt; bool:\n    \"\"\"Check if, for the length_check first terms, the length of the numbers of the sequence doesn't exceed max_terms_len (prevent explosions)\"\"\"\n    first_terms = S.n_first_elem(length_check, max_terms_len= max_terms_len) # might not compute a safe version of \"n_first_elem\"\n    return max([ len(str(elem)) for elem in first_terms ]) &lt;= max_terms_len and len(first_terms) == length_check\n</code></pre>"},{"location":"tasks/set_operations/","title":"Set Operations","text":""},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations","title":"<code>reasoning_core.tasks.set_operations</code>","text":""},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations-classes","title":"Classes","text":""},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations-functions","title":"Functions","text":""},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations.create_intension","title":"<code>create_intension(domain, length)</code>","text":"<p>Returns a contiguous subdomain (of domain) of size length.</p> Source code in <code>reasoning_core/tasks/set_operations.py</code> <pre><code>def create_intension(domain : list, length : int):\n        \"\"\"Returns a contiguous subdomain (of domain) of size length.\"\"\"\n        n = len(domain)\n        i = np.random.randint(n-length)\n        return domain[i:i+length]\n</code></pre>"},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations.random_subdomain","title":"<code>random_subdomain(domain, size=None)</code>","text":"<p>Domain must be a collection of element convertible in list, and frac must be a float between 0 and 1. </p> <p>return a random fraction of the domain.</p> Source code in <code>reasoning_core/tasks/set_operations.py</code> <pre><code>def random_subdomain(domain, size=None):\n    \"\"\"Domain must be a collection of element convertible in list, and frac must be a float between 0 and 1. \\n\n    return a random fraction of the domain.\"\"\"\n    domain = list(domain)\n    subset = random.sample(domain, size)\n    return return_shuffle(subset)\n</code></pre>"},{"location":"tasks/set_operations/#reasoning_core.tasks.set_operations.return_shuffle","title":"<code>return_shuffle(domain)</code>","text":"<p>Domain must be a collection of element convertible in list</p> Source code in <code>reasoning_core/tasks/set_operations.py</code> <pre><code>def return_shuffle(domain):\n    \"\"\"Domain must be a collection of element convertible in list\"\"\"\n    # Cast to SetList so it prints with curly braces but behaves like a list\n    domain = SetList(domain)\n    random.shuffle(domain)\n    return domain\n</code></pre>"},{"location":"tasks/table_qa/","title":"Table Qa","text":""},{"location":"tasks/table_qa/#reasoning_core.tasks.table_qa","title":"<code>reasoning_core.tasks.table_qa</code>","text":""},{"location":"tasks/table_qa/#reasoning_core.tasks.table_qa-classes","title":"Classes","text":""}]}